@article{bakerBioAcousticaFreeOpen2015,
  title = {{{BioAcoustica}}: A Free and Open Repository and Analysis Platform for Bioacoustics},
  shorttitle = {{{BioAcoustica}}},
  author = {Baker, Edward and Price, Ben and Rycroft, Simon and Hill, Jon and Smith, Vincent S.},
  date = {2015-01-01},
  journaltitle = {Database},
  shortjournal = {Database},
  volume = {2015},
  pages = {bav054},
  issn = {1758-0463},
  doi = {10.1093/database/bav054},
  url = {https://doi.org/10.1093/database/bav054},
  urldate = {2024-04-28},
  abstract = {We describe an online open repository and analysis platform, BioAcoustica ( http://bio.acousti.ca ), for recordings of wildlife sounds. Recordings can be annotated using a crowdsourced approach, allowing voice introductions and sections with extraneous noise to be removed from analyses. This system is based on the Scratchpads virtual research environment, the BioVeL portal and the Taverna workflow management tool, which allows for analysis of recordings using a grid computing service. At present the analyses include spectrograms, oscillograms and dominant frequency analysis. Further analyses can be integrated to meet the needs of specific researchers or projects. Researchers can upload and annotate their recordings to supplement traditional publication. Database URL: http://bio.acousti.ca}
}

@article{bakerGlobalCicadaSound2015,
  title = {Global {{Cicada Sound Collection I}}: {{Recordings}} from {{South Africa}} and {{Malawi}} by {{B}}. {{W}}. {{Price}} \&amp; {{M}}. {{H}}. {{Villet}} and Harvesting of {{BioAcoustica}} Data by {{GBIF}}},
  shorttitle = {Global {{Cicada Sound Collection I}}},
  author = {Baker, Edward and Price, Ben and Rycroft, Simon and Villet, Martin},
  date = {2015-09-02},
  journaltitle = {Biodiversity Data Journal},
  volume = {3},
  pages = {e5792},
  publisher = {Pensoft Publishers},
  issn = {1314-2828},
  doi = {10.3897/BDJ.3.e5792},
  url = {https://bdj.pensoft.net/article/5792/},
  urldate = {2024-04-28},
  abstract = {This collection of sounds includes 219 recordings of 133 voucher specimens, comprising 42 taxa (25 identified to species, all identified to genus) from South Africa and Malawi. The recordings have been used to underpin work on the species limits of cicadas in southern Africa, including Price et al. (2007) and Price et al. (2010). The specimens are deposited in the Albany Museum, Grahamstown, South Africa (AMGS). The harvesting of acoustic data as occurrence records by GBIF has been implemented by the Scratchpads Team at the Natural History Museum, London. This link increases the value of individual recordings and the BioAcoustica platform within the global infrastructure of biodiversity informatics by making specimen/occurence records from BioAcoustica available to a wider audience, and allowing their integration with other occurence datasets that also contribute to GBIF.},
  langid = {english}
}

@book{brondizioGlobalAssessmentReport2019,
  title = {The Global Assessment Report of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services},
  editor = {Brondízio, Eduardo Sonnewend and Settele, Josef and Díaz, Sandra and Ngo, Hien Thu},
  date = {2019},
  publisher = {{Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES)}},
  location = {Bonn},
  isbn = {978-3-947851-20-1},
  langid = {english},
  annotation = {OCLC: 1336011247}
}

@incollection{capineraOrthoptera2008,
  title = {Orthoptera},
  booktitle = {Encyclopedia of {{Entomology}}},
  editor = {Capinera, John L.},
  date = {2008},
  pages = {2695--2695},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-1-4020-6359-6_1892},
  url = {https://doi.org/10.1007/978-1-4020-6359-6_1892},
  urldate = {2024-04-28},
  isbn = {978-1-4020-6359-6},
  langid = {english}
}

@article{cardinaleBiodiversityLossIts2012,
  title = {Biodiversity Loss and Its Impact on Humanity},
  author = {Cardinale, Bradley J. and Duffy, J. Emmett and Gonzalez, Andrew and Hooper, David U. and Perrings, Charles and Venail, Patrick and Narwani, Anita and Mace, Georgina M. and Tilman, David and Wardle, David A. and Kinzig, Ann P. and Daily, Gretchen C. and Loreau, Michel and Grace, James B. and Larigauderie, Anne and Srivastava, Diane S. and Naeem, Shahid},
  date = {2012-06},
  journaltitle = {Nature},
  volume = {486},
  number = {7401},
  pages = {59--67},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature11148},
  url = {https://www.nature.com/articles/nature11148},
  urldate = {2024-04-18},
  abstract = {Two decades ago the first Earth Summit raised the question of how biological diversity loss alters ecosystem functioning and affects humanity; this Review looks at the progress made towards answering this question.},
  langid = {english},
  keywords = {Biodiversity,Ecosystem services}
}

@article{cardinaleBiodiversityLossIts2012a,
  title = {Biodiversity Loss and Its Impact on Humanity},
  author = {Cardinale, Bradley J. and Duffy, J. Emmett and Gonzalez, Andrew and Hooper, David U. and Perrings, Charles and Venail, Patrick and Narwani, Anita and Mace, Georgina M. and Tilman, David and Wardle, David A. and Kinzig, Ann P. and Daily, Gretchen C. and Loreau, Michel and Grace, James B. and Larigauderie, Anne and Srivastava, Diane S. and Naeem, Shahid},
  date = {2012-06},
  journaltitle = {Nature},
  volume = {486},
  number = {7401},
  pages = {59--67},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature11148},
  url = {https://www.nature.com/articles/nature11148},
  urldate = {2024-06-30},
  abstract = {Two decades ago the first Earth Summit raised the question of how biological diversity loss alters ecosystem functioning and affects humanity; this Review looks at the progress made towards answering this question.},
  langid = {english},
  keywords = {Biodiversity,Ecosystem services}
}

@article{dengHarnessingPowerSound2023,
  title = {Harnessing the {{Power}} of {{Sound}} and {{AI}} to Track {{Global Biodiversity Framework}} ({{GBF}}) {{Targets}}},
  author = {Deng, Iris},
  date = {2023},
  langid = {english}
}

@article{faissAdaptiveRepresentationsSound2023,
  title = {Adaptive Representations of Sound for Automatic Insect Recognition},
  author = {Faiss, Marius and Stowell, Dan},
  editor = {Martinez-Garcia, Ricardo},
  date = {2023-10-04},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {19},
  number = {10},
  pages = {e1011541},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1011541},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1011541},
  urldate = {2024-04-18},
  abstract = {Insect population numbers and biodiversity have been rapidly declining with time, and monitoring these trends has become increasingly important for conservation measures to be effectively implemented. But monitoring methods are often invasive, time and resource intense, and prone to various biases. Many insect species produce characteristic sounds that can easily be detected and recorded without large cost or effort. Using deep learning methods, insect sounds from field recordings could be automatically detected and classified to monitor biodiversity and species distribution ranges. We implement this using recently published datasets of insect sounds (up to 66 species of Orthoptera and Cicadidae) and machine learning methods and evaluate their potential for acoustic insect monitoring. We compare the performance of the conventional spectrogram-based audio representation against LEAF, a new adaptive and waveform-based frontend. LEAF achieved better classification performance than the mel-spectrogram frontend by adapting its feature extraction parameters during training. This result is encouraging for future implementations of deep learning technology for automatic insect sound recognition, especially as larger datasets become available.},
  langid = {english}
}

@dataset{faissInsectSet32DatasetAutomatic2022,
  title = {{{InsectSet32}}: {{Dataset}} for Automatic Acoustic Identification of Insects ({{Orthoptera}} and {{Cicadidae}})},
  shorttitle = {{{InsectSet32}}},
  author = {Faiss, Marius},
  date = {2022-09-12},
  publisher = {Zenodo},
  doi = {10.5281/zenodo.7072196},
  url = {https://zenodo.org/records/7072196},
  urldate = {2024-04-18},
  abstract = {This dataset contains recordings of 32 sound producing insect species with~a total~335 files and a length of 57 minutes. The dataset was~compiled for training neural networks to automatically identify insect species while comparing adaptive, waveform-based frontends to conventional mel-spectrogram frontends for audio feature extraction. This work will be submitted for publication in the future and this dataset can be used to replicate the results, as well as other uses. A preprint of the paper is publicly available. The scripts for audio processing and the machine learning implementations~will be published on~Github. The recordings are split into two datasets.~Roughly half of the~recordings (147) are of nine species belonging to~the order Orthoptera. These recordings stem from a dataset that was~originally compiled by Baudewijn Odé~(unpublished).~ The remaining recordings~(188)~are of 23 species in the family Cicadidae.~These recordings were selected~from the Global Cicada Sound Collection hosted on~Bioacoustica~(doi.org/10.1093/database/bav054), including recordings published in~doi.org/10.3897/BDJ.3.e5792~\&~doi.org/10.11646/zootaxa.4340.1.~Many recordings from this collection included speech annotations in the beginning of the recordings, therefore the last ten seconds of audio were extracted and used in this dataset.~ All files were manually inspected and files with strong noise interference or with sounds of multiple species were removed. Between species, the number of files ranges from four to 22 files and the length from 40 seconds to almost nine minutes of audio material for a single species. The files range in length from less than one second to several minutes. All original files were available with sample rates of at least~44.1 kHz or higher but were resampled to 44.1 kHz mono WAV~files for consistency. The annotation files contain information for each recording, including the file name, species name and identifier, as well as the data subset they were included in for training the neural network (training, test, validation).},
  langid = {english},
  version = {0.1},
  keywords = {bioacoustics,cicadidae,ecology,insecta,machine listening,orthoptera,remote monitoring}
}

@book{Goodfellow-et-al-201,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Benigo, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  url = {http://www.deeplearningbook.org}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  url = {https://ieeexplore.ieee.org/document/7780459},
  urldate = {2024-07-03},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization}
}

@misc{hornMachineLearningPattern2021,
  title = {Machine {{Learning}} \& {{Pattern Recognition}}},
  author = {Horn, Claus},
  date = {2021-10-12},
  langid = {english}
}

@article{kahlBirdNETDeepLearning2021,
  title = {{{BirdNET}}: {{A}} Deep Learning Solution for Avian Diversity Monitoring},
  shorttitle = {{{BirdNET}}},
  author = {Kahl, Stefan and Wood, Connor and Eibl, Maximilian and Klinck, Holger},
  date = {2021-01-27},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {61},
  pages = {101236},
  doi = {10.1016/j.ecoinf.2021.101236},
  abstract = {Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286 h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670 h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4 years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation.}
}

@article{poppleRevisionMyopsaltaCrucifera2017,
  title = {A Revision of the {{Myopsalta}} Crucifera ({{Ashton}}) Species Group ({{Hemiptera}}: {{Cicadidae}}: {{Cicadettini}}) with 14 New Species from Mainland {{Australia}}},
  shorttitle = {Vol. 4340 {{No}}. 1},
  author = {Popple, Lindseay W.},
  date = {2017-10-27},
  journaltitle = {Zootaxa},
  doi = {10.11646/zootaxa.4340.1},
  url = {https://doi.org/10.11646/zootaxa.4340.1},
  urldate = {2024-04-28},
  issue = {Vol. 4340 No. 1},
  langid = {american}
}

@incollection{sanbornCicadasHemipteraCicadoidea2008,
  title = {Cicadas ({{Hemiptera}}: {{Cicadoidea}})},
  shorttitle = {Cicadas ({{Hemiptera}}},
  booktitle = {Encyclopedia of {{Entomology}}},
  author = {Sanborn, Allen},
  editor = {Capinera, John L.},
  date = {2008},
  pages = {874--877},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-1-4020-6359-6_666},
  url = {https://doi.org/10.1007/978-1-4020-6359-6_666},
  urldate = {2024-04-28},
  isbn = {978-1-4020-6359-6},
  langid = {english}
}

@article{scarpelliMultiIndexEcoacousticsAnalysis2021,
  title = {Multi-{{Index Ecoacoustics Analysis}} for {{Terrestrial Soundscapes}}: {{A New Semi-Automated Approach Using Time-Series Motif Discovery}} and {{Random Forest Classification}}},
  shorttitle = {Multi-{{Index Ecoacoustics Analysis}} for {{Terrestrial Soundscapes}}},
  author = {Scarpelli, Marina D. A. and Liquet, Benoit and Tucker, David and Fuller, Susan and Roe, Paul},
  date = {2021-12-17},
  journaltitle = {Frontiers in Ecology and Evolution},
  shortjournal = {Front. Ecol. Evol.},
  volume = {9},
  pages = {738537},
  issn = {2296-701X},
  doi = {10.3389/fevo.2021.738537},
  url = {https://www.frontiersin.org/articles/10.3389/fevo.2021.738537/full},
  urldate = {2024-05-06},
  abstract = {High rates of biodiversity loss caused by human-induced changes in the environment require new methods for large scale fauna monitoring and data analysis. While ecoacoustic monitoring is increasingly being used and shows promise, analysis and interpretation of the big data produced remains a challenge. Computer-generated acoustic indices potentially provide a biologically meaningful summary of sound, however, temporal autocorrelation, difficulties in statistical analysis of multi-index data and lack of consistency or transferability in different terrestrial environments have hindered the application of those indices in different contexts. To address these issues we investigate the use of time-series motif discovery and random forest classification of multi-indices through two case studies. We use a semi-automated workflow combining time-series motif discovery and random forest classification of multi-index (acoustic complexity, temporal entropy, and events per second) data to categorize sounds in unfiltered recordings according to the main source of sound present (birds, insects, geophony). Our approach showed more than 70\% accuracy in label assignment in both datasets. The categories assigned were broad, but we believe this is a great improvement on traditional single index analysis of environmental recordings as we can now give ecological meaning to recordings in a semi-automated way that does not require expert knowledge and manual validation is only necessary for a small subset of the data. Furthermore, temporal autocorrelation, which is largely ignored by researchers, has been effectively eliminated through the time-series motif discovery technique applied here for the first time to ecoacoustic data. We expect that our approach will greatly assist researchers in the future as it will allow large datasets to be rapidly processed and labeled, enabling the screening of recordings for undesired sounds, such as wind, or target biophony (insects and birds) for biodiversity monitoring or bioacoustics research.},
  langid = {english}
}

@article{stowellAutomaticAcousticDetection2019,
  title = {Automatic Acoustic Detection of Birds through Deep Learning: The First {{Bird Audio Detection}} Challenge},
  shorttitle = {Automatic Acoustic Detection of Birds through Deep Learning},
  author = {Stowell, Dan and Stylianou, Yannis and Wood, Mike and Pamuła, Hanna and Glotin, Hervé},
  date = {2019-03},
  journaltitle = {Methods in Ecology and Evolution},
  shortjournal = {Methods Ecol Evol},
  volume = {10},
  number = {3},
  eprint = {1807.05812},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  pages = {368--380},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.13103},
  url = {http://arxiv.org/abs/1807.05812},
  urldate = {2024-06-30},
  abstract = {Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here we report outcomes from a collaborative data challenge showing that with modern machine learning including deep learning, general-purpose acoustic bird detection can achieve very high retrieval rates in remote monitoring data --- with no manual recalibration, and no pre-training of the detector for the target species or the acoustic conditions in the target environment. Multiple methods were able to attain performance of around 88\% AUC (area under the ROC curve), much higher performance than previous general-purpose methods. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects.},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{stowellComputationalBioacousticsDeep2022,
  title = {Computational Bioacoustics with Deep Learning: A Review and Roadmap},
  shorttitle = {Computational Bioacoustics with Deep Learning},
  author = {Stowell, Dan},
  date = {2022-03-21},
  journaltitle = {PeerJ},
  volume = {10},
  eprint = {2112.06725},
  eprinttype = {arXiv},
  eprintclass = {cs, eess, q-bio},
  pages = {e13152},
  issn = {2167-8359},
  doi = {10.7717/peerj.13152},
  url = {http://arxiv.org/abs/2112.06725},
  urldate = {2024-03-05},
  abstract = {Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider field of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Quantitative Biology - Quantitative Methods}
}

@online{zotero-314,
  url = {https://arxiv.org/pdf/1807.05812},
  urldate = {2024-06-30}
}
