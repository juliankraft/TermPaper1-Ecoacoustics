@article{cardinaleBiodiversityLossIts2012,
  title = {Biodiversity Loss and Its Impact on Humanity},
  author = {Cardinale, Bradley J. and Duffy, J. Emmett and Gonzalez, Andrew and Hooper, David U. and Perrings, Charles and Venail, Patrick and Narwani, Anita and Mace, Georgina M. and Tilman, David and Wardle, David A. and Kinzig, Ann P. and Daily, Gretchen C. and Loreau, Michel and Grace, James B. and Larigauderie, Anne and Srivastava, Diane S. and Naeem, Shahid},
  date = {2012-06},
  journaltitle = {Nature},
  volume = {486},
  number = {7401},
  pages = {59--67},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature11148},
  url = {https://www.nature.com/articles/nature11148},
  urldate = {2024-04-18},
  abstract = {Two decades ago the first Earth Summit raised the question of how biological diversity loss alters ecosystem functioning and affects humanity; this Review looks at the progress made towards answering this question.},
  langid = {english},
  keywords = {Biodiversity,Ecosystem services},
  file = {C:\Users\kraft\Zotero\storage\JSHVVUMS\Cardinale et al. - 2012 - Biodiversity loss and its impact on humanity.pdf}
}

@article{faissAdaptiveRepresentationsSound2023,
  title = {Adaptive Representations of Sound for Automatic Insect Recognition},
  author = {Faiß, Marius and Stowell, Dan},
  editor = {Martinez-Garcia, Ricardo},
  date = {2023-10-04},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {19},
  number = {10},
  pages = {e1011541},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1011541},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1011541},
  urldate = {2024-04-18},
  abstract = {Insect population numbers and biodiversity have been rapidly declining with time, and monitoring these trends has become increasingly important for conservation measures to be effectively implemented. But monitoring methods are often invasive, time and resource intense, and prone to various biases. Many insect species produce characteristic sounds that can easily be detected and recorded without large cost or effort. Using deep learning methods, insect sounds from field recordings could be automatically detected and classified to monitor biodiversity and species distribution ranges. We implement this using recently published datasets of insect sounds (up to 66 species of Orthoptera and Cicadidae) and machine learning methods and evaluate their potential for acoustic insect monitoring. We compare the performance of the conventional spectrogram-based audio representation against LEAF, a new adaptive and waveform-based frontend. LEAF achieved better classification performance than the mel-spectrogram frontend by adapting its feature extraction parameters during training. This result is encouraging for future implementations of deep learning technology for automatic insect sound recognition, especially as larger datasets become available.},
  langid = {english},
  file = {C:\Users\kraft\Zotero\storage\HN9JVTQT\Faiß and Stowell - 2023 - Adaptive representations of sound for automatic in.pdf}
}

@dataset{faissInsectSet32DatasetAutomatic2022,
  title = {{{InsectSet32}}: {{Dataset}} for Automatic Acoustic Identification of Insects ({{Orthoptera}} and {{Cicadidae}})},
  shorttitle = {{{InsectSet32}}},
  author = {Faiß, Marius},
  date = {2022-09-12},
  publisher = {Zenodo},
  doi = {10.5281/zenodo.7072196},
  url = {https://zenodo.org/records/7072196},
  urldate = {2024-04-18},
  abstract = {This dataset contains recordings of 32 sound producing insect species with~a total~335 files and a length of 57 minutes. The dataset was~compiled for training neural networks to automatically identify insect species while comparing adaptive, waveform-based frontends to conventional mel-spectrogram frontends for audio feature extraction. This work will be submitted for publication in the future and this dataset can be used to replicate the results, as well as other uses. A preprint of the paper is publicly available. The scripts for audio processing and the machine learning implementations~will be published on~Github. The recordings are split into two datasets.~Roughly half of the~recordings (147) are of nine species belonging to~the order Orthoptera. These recordings stem from a dataset that was~originally compiled by Baudewijn Odé~(unpublished).~ The remaining recordings~(188)~are of 23 species in the family Cicadidae.~These recordings were selected~from the Global Cicada Sound Collection hosted on~Bioacoustica~(doi.org/10.1093/database/bav054), including recordings published in~doi.org/10.3897/BDJ.3.e5792~\&~doi.org/10.11646/zootaxa.4340.1.~Many recordings from this collection included speech annotations in the beginning of the recordings, therefore the last ten seconds of audio were extracted and used in this dataset.~ All files were manually inspected and files with strong noise interference or with sounds of multiple species were removed. Between species, the number of files ranges from four to 22 files and the length from 40 seconds to almost nine minutes of audio material for a single species. The files range in length from less than one second to several minutes. All original files were available with sample rates of at least~44.1 kHz or higher but were resampled to 44.1 kHz mono WAV~files for consistency. The annotation files contain information for each recording, including the file name, species name and identifier, as well as the data subset they were included in for training the neural network (training, test, validation).},
  langid = {english},
  version = {0.1},
  keywords = {bioacoustics,cicadidae,ecology,insecta,machine listening,orthoptera,remote monitoring},
  file = {C:\Users\kraft\Zotero\storage\BX5IQPHE\7072196.html}
}

@misc{hornMachineLearningPattern2021,
  title = {Machine {{Learning}} \& {{Pattern Recognition}}},
  author = {Horn, Claus},
  date = {2021-10-12},
  langid = {english},
  file = {C:\Users\kraft\Zotero\storage\S42WHG8X\Horn - 2021 - Machine Learning & Pattern Recognition.pdf}
}

@article{stowellComputationalBioacousticsDeep2022,
  title = {Computational Bioacoustics with Deep Learning: A Review and Roadmap},
  shorttitle = {Computational Bioacoustics with Deep Learning},
  author = {Stowell, Dan},
  date = {2022-03-21},
  journaltitle = {PeerJ},
  volume = {10},
  eprint = {2112.06725},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, q-bio},
  pages = {e13152},
  issn = {2167-8359},
  doi = {10.7717/peerj.13152},
  url = {http://arxiv.org/abs/2112.06725},
  urldate = {2024-03-05},
  abstract = {Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider field of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Quantitative Biology - Quantitative Methods},
  file = {C:\Users\kraft\Zotero\storage\I73IN2J7\Stowell - 2022 - Computational bioacoustics with deep learning a r.pdf}
}
