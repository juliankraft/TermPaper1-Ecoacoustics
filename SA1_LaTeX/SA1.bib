@article{bakerBioAcousticaFreeOpen2015,
  title = {{{BioAcoustica}}: A Free and Open Repository and Analysis Platform for Bioacoustics},
  shorttitle = {{{BioAcoustica}}},
  author = {Baker, Edward and Price, Ben and Rycroft, Simon and Hill, Jon and Smith, Vincent S.},
  date = {2015-01-01},
  journaltitle = {Database},
  shortjournal = {Database},
  volume = {2015},
  pages = {bav054},
  issn = {1758-0463},
  doi = {10.1093/database/bav054},
  url = {https://doi.org/10.1093/database/bav054},
  urldate = {2024-04-28},
  abstract = {We describe an online open repository and analysis platform, BioAcoustica ( http://bio.acousti.ca ), for recordings of wildlife sounds. Recordings can be annotated using a crowdsourced approach, allowing voice introductions and sections with extraneous noise to be removed from analyses. This system is based on the Scratchpads virtual research environment, the BioVeL portal and the Taverna workflow management tool, which allows for analysis of recordings using a grid computing service. At present the analyses include spectrograms, oscillograms and dominant frequency analysis. Further analyses can be integrated to meet the needs of specific researchers or projects. Researchers can upload and annotate their recordings to supplement traditional publication. Database URL: http://bio.acousti.ca},
  file = {C\:\\Users\\kraft\\Zotero\\storage\\J8DMNWRR\\Baker et al. - 2015 - BioAcoustica a free and open repository and analy.pdf;C\:\\Users\\kraft\\Zotero\\storage\\LXWRLSBE\\2433187.html}
}

@article{bakerGlobalCicadaSound2015,
  title = {Global {{Cicada Sound Collection I}}: {{Recordings}} from {{South Africa}} and {{Malawi}} by {{B}}. {{W}}. {{Price}} \&amp; {{M}}. {{H}}. {{Villet}} and Harvesting of {{BioAcoustica}} Data by {{GBIF}}},
  shorttitle = {Global {{Cicada Sound Collection I}}},
  author = {Baker, Edward and Price, Ben and Rycroft, Simon and Villet, Martin},
  date = {2015-09-02},
  journaltitle = {Biodiversity Data Journal},
  volume = {3},
  pages = {e5792},
  publisher = {Pensoft Publishers},
  issn = {1314-2828},
  doi = {10.3897/BDJ.3.e5792},
  url = {https://bdj.pensoft.net/article/5792/},
  urldate = {2024-04-28},
  abstract = {This collection of sounds includes 219 recordings of 133 voucher specimens, comprising 42 taxa (25 identified to species, all identified to genus) from South Africa and Malawi. The recordings have been used to underpin work on the species limits of cicadas in southern Africa, including Price et al. (2007) and Price et al. (2010). The specimens are deposited in the Albany Museum, Grahamstown, South Africa (AMGS). The harvesting of acoustic data as occurrence records by GBIF has been implemented by the Scratchpads Team at the Natural History Museum, London. This link increases the value of individual recordings and the BioAcoustica platform within the global infrastructure of biodiversity informatics by making specimen/occurence records from BioAcoustica available to a wider audience, and allowing their integration with other occurence datasets that also contribute to GBIF.},
  langid = {english},
  file = {C:\Users\kraft\Zotero\storage\G6R99C36\Baker et al. - 2015 - Global Cicada Sound Collection I Recordings from .pdf}
}

@book{brondizioGlobalAssessmentReport2019,
  title = {The Global Assessment Report of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services},
  editor = {Brondízio, Eduardo Sonnewend and Settele, Josef and Díaz, Sandra and Ngo, Hien Thu},
  date = {2019},
  publisher = {{Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES)}},
  location = {Bonn},
  isbn = {978-3-947851-20-1},
  langid = {english},
  annotation = {OCLC: 1336011247},
  file = {C:\Users\kraft\Zotero\storage\W4H4RYU9\Brondízio et al. - 2019 - The global assessment report of the intergovernmen.pdf}
}

@incollection{capineraOrthoptera2008,
  title = {Orthoptera},
  booktitle = {Encyclopedia of {{Entomology}}},
  editor = {Capinera, John L.},
  date = {2008},
  pages = {2695--2695},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-1-4020-6359-6_1892},
  url = {https://doi.org/10.1007/978-1-4020-6359-6_1892},
  urldate = {2024-04-28},
  isbn = {978-1-4020-6359-6},
  langid = {english}
}

@article{cardinaleBiodiversityLossIts2012,
  title = {Biodiversity Loss and Its Impact on Humanity},
  author = {Cardinale, Bradley J. and Duffy, J. Emmett and Gonzalez, Andrew and Hooper, David U. and Perrings, Charles and Venail, Patrick and Narwani, Anita and Mace, Georgina M. and Tilman, David and Wardle, David A. and Kinzig, Ann P. and Daily, Gretchen C. and Loreau, Michel and Grace, James B. and Larigauderie, Anne and Srivastava, Diane S. and Naeem, Shahid},
  date = {2012-06},
  journaltitle = {Nature},
  volume = {486},
  number = {7401},
  pages = {59--67},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature11148},
  url = {https://www.nature.com/articles/nature11148},
  urldate = {2024-04-18},
  abstract = {Two decades ago the first Earth Summit raised the question of how biological diversity loss alters ecosystem functioning and affects humanity; this Review looks at the progress made towards answering this question.},
  langid = {english},
  keywords = {Biodiversity,Ecosystem services},
  file = {C:\Users\kraft\Zotero\storage\JSHVVUMS\Cardinale et al. - 2012 - Biodiversity loss and its impact on humanity.pdf}
}

@article{dengHarnessingPowerSound2023,
  title = {Harnessing the {{Power}} of {{Sound}} and {{AI}} to Track {{Global Biodiversity Framework}} ({{GBF}}) {{Targets}}},
  author = {Deng, Iris},
  date = {2023},
  langid = {english},
  file = {C:\Users\kraft\Zotero\storage\LEW37E64\Deng - 2023 - Harnessing the Power of Sound and AI to track Glob.pdf}
}

@article{faissAdaptiveRepresentationsSound2023,
  title = {Adaptive Representations of Sound for Automatic Insect Recognition},
  author = {Faiss, Marius and Stowell, Dan},
  editor = {Martinez-Garcia, Ricardo},
  date = {2023-10-04},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {19},
  number = {10},
  pages = {e1011541},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1011541},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1011541},
  urldate = {2024-04-18},
  abstract = {Insect population numbers and biodiversity have been rapidly declining with time, and monitoring these trends has become increasingly important for conservation measures to be effectively implemented. But monitoring methods are often invasive, time and resource intense, and prone to various biases. Many insect species produce characteristic sounds that can easily be detected and recorded without large cost or effort. Using deep learning methods, insect sounds from field recordings could be automatically detected and classified to monitor biodiversity and species distribution ranges. We implement this using recently published datasets of insect sounds (up to 66 species of Orthoptera and Cicadidae) and machine learning methods and evaluate their potential for acoustic insect monitoring. We compare the performance of the conventional spectrogram-based audio representation against LEAF, a new adaptive and waveform-based frontend. LEAF achieved better classification performance than the mel-spectrogram frontend by adapting its feature extraction parameters during training. This result is encouraging for future implementations of deep learning technology for automatic insect sound recognition, especially as larger datasets become available.},
  langid = {english},
  file = {C:\Users\kraft\Zotero\storage\HN9JVTQT\Faiß and Stowell - 2023 - Adaptive representations of sound for automatic in.pdf}
}

@dataset{faissInsectSet32DatasetAutomatic2022,
  title = {{{InsectSet32}}: {{Dataset}} for Automatic Acoustic Identification of Insects ({{Orthoptera}} and {{Cicadidae}})},
  shorttitle = {{{InsectSet32}}},
  author = {Faiss, Marius},
  date = {2022-09-12},
  publisher = {Zenodo},
  doi = {10.5281/zenodo.7072196},
  url = {https://zenodo.org/records/7072196},
  urldate = {2024-04-18},
  abstract = {This dataset contains recordings of 32 sound producing insect species with~a total~335 files and a length of 57 minutes. The dataset was~compiled for training neural networks to automatically identify insect species while comparing adaptive, waveform-based frontends to conventional mel-spectrogram frontends for audio feature extraction. This work will be submitted for publication in the future and this dataset can be used to replicate the results, as well as other uses. A preprint of the paper is publicly available. The scripts for audio processing and the machine learning implementations~will be published on~Github. The recordings are split into two datasets.~Roughly half of the~recordings (147) are of nine species belonging to~the order Orthoptera. These recordings stem from a dataset that was~originally compiled by Baudewijn Odé~(unpublished).~ The remaining recordings~(188)~are of 23 species in the family Cicadidae.~These recordings were selected~from the Global Cicada Sound Collection hosted on~Bioacoustica~(doi.org/10.1093/database/bav054), including recordings published in~doi.org/10.3897/BDJ.3.e5792~\&~doi.org/10.11646/zootaxa.4340.1.~Many recordings from this collection included speech annotations in the beginning of the recordings, therefore the last ten seconds of audio were extracted and used in this dataset.~ All files were manually inspected and files with strong noise interference or with sounds of multiple species were removed. Between species, the number of files ranges from four to 22 files and the length from 40 seconds to almost nine minutes of audio material for a single species. The files range in length from less than one second to several minutes. All original files were available with sample rates of at least~44.1 kHz or higher but were resampled to 44.1 kHz mono WAV~files for consistency. The annotation files contain information for each recording, including the file name, species name and identifier, as well as the data subset they were included in for training the neural network (training, test, validation).},
  langid = {english},
  version = {0.1},
  keywords = {bioacoustics,cicadidae,ecology,insecta,machine listening,orthoptera,remote monitoring},
  file = {C:\Users\kraft\Zotero\storage\BX5IQPHE\7072196.html}
}

@book{Goodfellow-et-al-201,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Benigo, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  url = {http://www.deeplearningbook.org}
}

@misc{hornMachineLearningPattern2021,
  title = {Machine {{Learning}} \& {{Pattern Recognition}}},
  author = {Horn, Claus},
  date = {2021-10-12},
  langid = {english},
  file = {C:\Users\kraft\Zotero\storage\S42WHG8X\Horn - 2021 - Machine Learning & Pattern Recognition.pdf}
}

@article{poppleRevisionMyopsaltaCrucifera2017,
  title = {A Revision of the {{Myopsalta}} Crucifera ({{Ashton}}) Species Group ({{Hemiptera}}: {{Cicadidae}}: {{Cicadettini}}) with 14 New Species from Mainland {{Australia}}},
  shorttitle = {Vol. 4340 {{No}}. 1},
  author = {Popple, Lindseay W.},
  date = {2017-10-27},
  journaltitle = {Zootaxa},
  doi = {10.11646/zootaxa.4340.1},
  url = {https://doi.org/10.11646/zootaxa.4340.1},
  urldate = {2024-04-28},
  issue = {Vol. 4340 No. 1},
  langid = {american},
  file = {C:\Users\kraft\Zotero\storage\EN6R9JVQ\Vol. 4340 No. 1 27 Oct. 2017  Zootaxa.pdf}
}

@incollection{sanbornCicadasHemipteraCicadoidea2008,
  title = {Cicadas ({{Hemiptera}}: {{Cicadoidea}})},
  shorttitle = {Cicadas ({{Hemiptera}}},
  booktitle = {Encyclopedia of {{Entomology}}},
  author = {Sanborn, Allen},
  editor = {Capinera, John L.},
  date = {2008},
  pages = {874--877},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-1-4020-6359-6_666},
  url = {https://doi.org/10.1007/978-1-4020-6359-6_666},
  urldate = {2024-04-28},
  isbn = {978-1-4020-6359-6},
  langid = {english}
}

@article{scarpelliMultiIndexEcoacousticsAnalysis2021,
  title = {Multi-{{Index Ecoacoustics Analysis}} for {{Terrestrial Soundscapes}}: {{A New Semi-Automated Approach Using Time-Series Motif Discovery}} and {{Random Forest Classification}}},
  shorttitle = {Multi-{{Index Ecoacoustics Analysis}} for {{Terrestrial Soundscapes}}},
  author = {Scarpelli, Marina D. A. and Liquet, Benoit and Tucker, David and Fuller, Susan and Roe, Paul},
  date = {2021-12-17},
  journaltitle = {Frontiers in Ecology and Evolution},
  shortjournal = {Front. Ecol. Evol.},
  volume = {9},
  pages = {738537},
  issn = {2296-701X},
  doi = {10.3389/fevo.2021.738537},
  url = {https://www.frontiersin.org/articles/10.3389/fevo.2021.738537/full},
  urldate = {2024-05-06},
  abstract = {High rates of biodiversity loss caused by human-induced changes in the environment require new methods for large scale fauna monitoring and data analysis. While ecoacoustic monitoring is increasingly being used and shows promise, analysis and interpretation of the big data produced remains a challenge. Computer-generated acoustic indices potentially provide a biologically meaningful summary of sound, however, temporal autocorrelation, difficulties in statistical analysis of multi-index data and lack of consistency or transferability in different terrestrial environments have hindered the application of those indices in different contexts. To address these issues we investigate the use of time-series motif discovery and random forest classification of multi-indices through two case studies. We use a semi-automated workflow combining time-series motif discovery and random forest classification of multi-index (acoustic complexity, temporal entropy, and events per second) data to categorize sounds in unfiltered recordings according to the main source of sound present (birds, insects, geophony). Our approach showed more than 70\% accuracy in label assignment in both datasets. The categories assigned were broad, but we believe this is a great improvement on traditional single index analysis of environmental recordings as we can now give ecological meaning to recordings in a semi-automated way that does not require expert knowledge and manual validation is only necessary for a small subset of the data. Furthermore, temporal autocorrelation, which is largely ignored by researchers, has been effectively eliminated through the time-series motif discovery technique applied here for the first time to ecoacoustic data. We expect that our approach will greatly assist researchers in the future as it will allow large datasets to be rapidly processed and labeled, enabling the screening of recordings for undesired sounds, such as wind, or target biophony (insects and birds) for biodiversity monitoring or bioacoustics research.},
  langid = {english},
  file = {C:\Users\kraft\Zotero\storage\GKFSP5HY\Scarpelli et al. - 2021 - Multi-Index Ecoacoustics Analysis for Terrestrial .pdf}
}

@article{stowellComputationalBioacousticsDeep2022,
  title = {Computational Bioacoustics with Deep Learning: A Review and Roadmap},
  shorttitle = {Computational Bioacoustics with Deep Learning},
  author = {Stowell, Dan},
  date = {2022-03-21},
  journaltitle = {PeerJ},
  volume = {10},
  eprint = {2112.06725},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, q-bio},
  pages = {e13152},
  issn = {2167-8359},
  doi = {10.7717/peerj.13152},
  url = {http://arxiv.org/abs/2112.06725},
  urldate = {2024-03-05},
  abstract = {Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider field of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Quantitative Biology - Quantitative Methods},
  file = {C:\Users\kraft\Zotero\storage\I73IN2J7\Stowell - 2022 - Computational bioacoustics with deep learning a r.pdf}
}
