{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import scipy.stats as stats\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "import importlib\n",
    "\n",
    "import evaluation\n",
    "importlib.reload(evaluation)\n",
    "from evaluation import RunEval\n",
    "from evaluation import LatexObject\n",
    "\n",
    "\n",
    "run_path = \"../logs/main_run/\"\n",
    "ex = RunEval(run_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Visualization of the Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'compare_spectrogram'\n",
    "object_type = 'figure'\n",
    "caption = 'Visualization of the two transformations of the audio signal.'\n",
    "image_size_cm = [14, 11]\n",
    "\n",
    "def create_object(image_size_cm=image_size_cm):\n",
    "    index = 210 #43\n",
    "    n_fft = 1024\n",
    "    top_db = None\n",
    "\n",
    "    metadata = ex.dataloader.dataset.get_metadata()\n",
    "\n",
    "    waveform, sample_rate = ex.dataloader.dataset.load_sample(metadata['path'][index])\n",
    "\n",
    "    def transform(waveform, sample_rate, n_mels, n_fft, top_db):\n",
    "        if n_mels == -1:\n",
    "            spec = torchaudio.transforms.Spectrogram(\n",
    "                                                    n_fft=n_fft, \n",
    "                                                    hop_length=int(n_fft/2), \n",
    "                                                    win_length=n_fft)(waveform)\n",
    "        else:\n",
    "            spec = torchaudio.transforms.MelSpectrogram(\n",
    "                                                    n_fft=n_fft,\n",
    "                                                    hop_length=int(n_fft/2),\n",
    "                                                    win_length=n_fft,\n",
    "                                                    n_mels=n_mels)(waveform)\n",
    "            \n",
    "        db_spec = torchaudio.transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "\n",
    "        norm_db_spec = (db_spec - db_spec.min()) / (db_spec.max() - db_spec.min())\n",
    "\n",
    "        return norm_db_spec\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(image_size_cm[0]/2.54, image_size_cm[1]/2.54))\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[4, 6], width_ratios=[6.5, 5.5])\n",
    "\n",
    "    # upper plot\n",
    "    ax_upper = fig.add_subplot(gs[0, :])\n",
    "    time = np.arange(0, len(waveform[0])) / sample_rate\n",
    "    ax_upper.plot(time, waveform[0], color='#482273')\n",
    "    ax_upper.set_title(f'Waveform (file length {time.max():.0f}s)')\n",
    "    # ax_upper.set_xlabel('Time [s]')\n",
    "    # ax_upper.set_ylabel('Amplitude')\n",
    "\n",
    "    # Remove y-axis and frame\n",
    "    ax_upper.yaxis.set_visible(False)\n",
    "    ax_upper.xaxis.set_visible(False)\n",
    "    ax_upper.spines[['top', 'right', 'left', 'bottom']].set_visible(False)\n",
    "\n",
    "    # lower plot\n",
    "\n",
    "    for i, (title, n_mels) in enumerate(zip(\n",
    "        ['Spectrogram', 'Mel Spectrogram'],\n",
    "        [-1, 64])):\n",
    "\n",
    "        spec = transform(waveform, sample_rate, n_mels, n_fft, top_db)\n",
    "\n",
    "        # Convert frame indices to seconds\n",
    "        hop_length = int(n_fft / 2)\n",
    "        duration = waveform.shape[-1] / sample_rate\n",
    "        time_bins = np.linspace(0, duration, spec.shape[-1])\n",
    "        \n",
    "        fig.add_subplot(gs[1, i])\n",
    "\n",
    "        if n_mels == -1:\n",
    "            # Frequency bins for the y-axis\n",
    "            freq_bins = np.linspace(0, sample_rate // 2, spec.shape[0])\n",
    "            freq_labels = [i for i in range(0, 20001, 2500)]\n",
    "\n",
    "            plt.imshow(spec.numpy().squeeze(), origin='lower', aspect='auto', extent=[0, duration, 0, sample_rate // 2])\n",
    "            plt.yticks(\n",
    "                ticks=freq_labels,\n",
    "                labels=[f'{freq / 1000:.1f}' for freq in freq_labels])\n",
    "            \n",
    "        elif n_mels == 64:\n",
    "            mel_frequencies = librosa.mel_frequencies(n_mels=n_mels, fmin=0, fmax=sample_rate // 2)\n",
    "            plt.imshow(spec.numpy().squeeze(), origin='lower', aspect='auto', extent=[0, duration, 0, 64])\n",
    "\n",
    "            num_ticks = 10\n",
    "            tick_positions = np.linspace(0, n_mels-1, num_ticks).astype(int)\n",
    "            tick_labels = [f'{mel_frequencies[i] / 1000:.1f}' for i in tick_positions]\n",
    "\n",
    "            plt.yticks(tick_positions, tick_labels)\n",
    "        \n",
    "        plt.ylabel('Frequency (KHz)')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.title(title)\n",
    "\n",
    "        if i == 0:\n",
    "            plt.colorbar()\n",
    "        else:\n",
    "            plt.gca().yaxis.tick_right()\n",
    "            plt.gca().yaxis.set_label_position(\"right\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "compare_spectrogram = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting  Visualizations for best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'confusion_matrix_best'\n",
    "object_type = 'figure'\n",
    "caption = 'Confusion matrix for the predictions of the test set using the best model. The over all accuracy on the test set was ***.'\n",
    "image_size_cm = [12, 12]\n",
    "\n",
    "def calculate_caption_elements():\n",
    "    _, accuracy = ex.get_best_model(sel_metric='accuracy', eval_subset=['test'])\n",
    "    return round(accuracy, 3)\n",
    "\n",
    "def create_object(image_size_cm=image_size_cm):\n",
    "    index_best, _ = ex.get_best_model(eval_subset=['test'])\n",
    "\n",
    "    y_true, y_pred = ex.get_predictions(index_best)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(image_size_cm[0]/2.54, image_size_cm[1]/2.54))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=ax, colorbar=False)\n",
    "\n",
    "    plt.setp(disp.ax_.texts, fontsize=6)\n",
    "\n",
    "    # Label the axes\n",
    "    ax.set_xlabel('Predicted ClassID')\n",
    "    ax.set_ylabel('True ClassID')\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='viridis')\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    cbar.ax.tick_params()\n",
    "\n",
    "    return fig\n",
    "\n",
    "confusion_matrix_best = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object, calculate_caption_elements=calculate_caption_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'loss_acc_best'\n",
    "object_type = 'figure'\n",
    "caption = 'The validation loss and accuracy of the best model. Light version is not smoothed and dark version is smoothed with a window size of 5.'\n",
    "image_size_cm = [14, 7]\n",
    "\n",
    "def create_object(image_size_cm=image_size_cm):\n",
    "    def smooth_data(data, window_size=5):\n",
    "        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    index_best, _ = ex.get_best_model()\n",
    "    metrics = ex.train_log[index_best]\n",
    "\n",
    "    validation_data = metrics[~metrics['val_loss'].isnull()]\n",
    "\n",
    "    # Smooth the data\n",
    "    smoothed_val_loss = smooth_data(validation_data['val_loss'])\n",
    "    smoothed_val_acc = smooth_data(validation_data['val_acc'])\n",
    "\n",
    "    # Adjust the epoch range to match the length of the smoothed data\n",
    "    epochs = validation_data['epoch'][len(validation_data['epoch']) - len(smoothed_val_loss):]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(image_size_cm[0]/2.54, image_size_cm[1]/2.54))\n",
    "\n",
    "    # Plot unsmoothed val_loss on the primary y-axis\n",
    "    ax1.plot(validation_data['epoch'], validation_data['val_loss'], color='#A6CEE3', label='', linewidth=1, alpha=0.5)\n",
    "    # Plot smoothed val_loss on the primary y-axis\n",
    "    ax1.plot(epochs, smoothed_val_loss, color='#1F78B4', label='Validation Loss', linewidth=0.5)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Validation Loss')\n",
    "    ax1.tick_params(axis='y')\n",
    "\n",
    "    # Create a secondary y-axis to plot val_acc\n",
    "    ax2 = ax1.twinx()\n",
    "    # Plot unsmoothed val_acc on the secondary y-axis\n",
    "    ax2.plot(validation_data['epoch'], validation_data['val_acc'], color='#FB9A99', label='', linewidth=1, alpha=0.5)\n",
    "    # Plot smoothed val_acc on the secondary y-axis\n",
    "    ax2.plot(epochs, smoothed_val_acc, color='#E31A1C', label='Validation Accuracy', linewidth=0.5)\n",
    "    ax2.set_ylabel('Validation Accuracy')\n",
    "    ax2.tick_params(axis='y')\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    fig.legend(lines_1 + lines_2, labels_1 + labels_2, loc='lower center', bbox_to_anchor=(0.5, 0.18), ncol=2)\n",
    "\n",
    "    return fig\n",
    "\n",
    "loss_acc_best = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Hyperparameter Tuning of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'hyperparameters_results'\n",
    "object_type = 'table'\n",
    "caption = 'Results of the hyperparameter tuning by descending accuracy.'\n",
    "table_size = 'scriptsize'\n",
    "\n",
    "def create_object():\n",
    "    data = ex.get_summary(eval_subset=['validation'])\n",
    "\n",
    "    selection = ['n_mels', 'n_res_blocks', 'learning_rate', 'kernel_size', 'num_trainable_params', 'trained_epochs', 'accuracy', 'f1']\n",
    "\n",
    "    data_selected = pd.DataFrame()\n",
    "\n",
    "    for col in selection:\n",
    "        if col in ['n_mels']:\n",
    "            data_selected[col] = data[col].apply(lambda x: f\"{x:.0f}\")\n",
    "        elif col in ['learning_rate']:\n",
    "            data_selected[col] = data[col].apply(lambda x: f\"{x:.3g}\")\n",
    "        elif col in ['accuracy', 'f1']:\n",
    "            data_selected[col] = data[col].apply(lambda x: f\"{x:.3f}\")\n",
    "        elif col in ['num_trainable_params']:\n",
    "            data_selected[col] = data[col].apply(lambda x: f\"{int(x):,}\")\n",
    "        else:\n",
    "            data_selected[col] = data[col]\n",
    "\n",
    "    # Convert DataFrame to LaTeX table\n",
    "    latex_table = data_selected.to_latex(\n",
    "        index=False, \n",
    "        column_format=\"rrrrrrrr\"\n",
    "    )\n",
    "\n",
    "    # Manually replace the header row to make it bold\n",
    "    old_header = \"n_mels & n_res_blocks & learning_rate & kernel_size & num_trainable_params & trained_epochs & accuracy & f1 \\\\\"\n",
    "    new_header = \"\\\\textbf{n\\\\_mels} & \\\\textbf{res blocks} & \\\\textbf{learning rate} & \\\\textbf{kernel size}& \\\\textbf{parameters} & \\\\textbf{epochs} & \\\\textbf{accuracy} & \\\\textbf{F1} \\\\\"\n",
    "    latex_table = latex_table.replace(old_header, new_header)\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "hyperparameters_results = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object, table_size=table_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'hyperparameters_boxplot'\n",
    "object_type = 'figure'\n",
    "caption = 'Accuracy of the models for different hyperparameter grouped by the transformation type.'\n",
    "image_size_cm = [14, 6]\n",
    "\n",
    "def create_object(image_size_cm=image_size_cm):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(image_size_cm[0]/2.54, image_size_cm[1]/2.54), sharex=True)\n",
    "\n",
    "    for i, (hp, cmap) in enumerate(zip(\n",
    "            ['learning_rate', 'n_res_blocks', 'kernel_size'],\n",
    "            ['Pastel1', 'Accent', 'Set3'])):\n",
    "        ax = axes[i]\n",
    "        sns.boxplot(\n",
    "            data=ex.get_summary(eval_subset=['validation']),\n",
    "            x='transform',\n",
    "            y='accuracy',\n",
    "            whis=2,\n",
    "            linewidth=0.5,\n",
    "            fliersize=0.5,\n",
    "            palette=cmap,\n",
    "            hue=hp,\n",
    "            ax=ax)\n",
    "        \n",
    "        ax.set_xlabel('')\n",
    "        # sns.move_legend(ax, 'lower left', ncols=3)\n",
    "        # ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3, frameon=False)\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles, labels, title=f'{hp}', loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "\n",
    "\n",
    "        if i > 0:\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_yticklabels([])\n",
    "    \n",
    "    return fig\n",
    "\n",
    "hyperparameters_boxplot = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'hyperparameters_scatterplot'\n",
    "object_type = 'figure'\n",
    "caption = 'Model size compared to the accuracy and F1 Score of the models.'\n",
    "image_size_cm = [14, 8]\n",
    "\n",
    "def create_object(image_size_cm=image_size_cm):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(image_size_cm[0]/2.54, image_size_cm[1]/2.54), sharex=True)\n",
    "\n",
    "    data = ex.get_summary(eval_subset=['validation'])\n",
    "\n",
    "    for i, metric in enumerate(['accuracy', 'f1']):\n",
    "\n",
    "        pearson_corr, p_value = stats.pearsonr(data[metric], data['num_trainable_params'])\n",
    "\n",
    "        ax = axes[i]\n",
    "        sns.scatterplot(\n",
    "            data=data,\n",
    "            x='num_trainable_params',\n",
    "            y=metric,\n",
    "            s=10,\n",
    "            ax=ax)\n",
    "        \n",
    "        if i > 0:\n",
    "            ax.yaxis.tick_right()\n",
    "            ax.yaxis.set_label_position('right')\n",
    "        \n",
    "        ax.text(0.5, -0.3, f'Pearson Test\\nr: {pearson_corr:.2f}, p-value: {p_value:.2f}',\n",
    "                horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    # sns.move_legend(ax, 'right', ncols=3)\n",
    "\n",
    "    return fig\n",
    "\n",
    "hyperparameters_scatterplot = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'f1_per_class'\n",
    "object_type = 'figure'\n",
    "caption = 'F1 Score per class as mean trough all models compared to data distribution.'\n",
    "image_size_cm = [14, 14]\n",
    "\n",
    "def create_object(image_size_cm=image_size_cm):\n",
    "    metadata = ex.dataloader.dataset.get_metadata()\n",
    "    grouped = metadata.groupby(['class_id', 'data_set']).size().unstack(fill_value=0).reset_index()\n",
    "    melted = grouped.melt(id_vars='class_id', var_name='data_set', value_name='count')\n",
    "\n",
    "    f1_per_class = ex.get_metrics(sel_metric='f1_per_class', eval_subset=['validation']).mean()\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(image_size_cm[0]/2.54, image_size_cm[1]/2.54), sharex=True)\n",
    "\n",
    "    # Plot 1: Count of Class ID by Data Set\n",
    "    sns.barplot(\n",
    "        data=melted, \n",
    "        x='class_id', \n",
    "        y='count', \n",
    "        hue='data_set',\n",
    "        palette = 'Accent', \n",
    "        ax=axes[0])\n",
    "    axes[0].set_ylabel('Data Count')\n",
    "\n",
    "    # Plot 2: Mean F1 Score per Class\n",
    "    sns.barplot(\n",
    "        data=f1_per_class,\n",
    "        color = 'skyblue', \n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_xlabel('Class ID')\n",
    "    axes[1].set_ylabel('Mean F1 Score')\n",
    "\n",
    "    # axes[1].xaxis.set_label_position('top')\n",
    "    axes[1].xaxis.tick_top()\n",
    "    axes[1].invert_yaxis()\n",
    "\n",
    "    return fig\n",
    "\n",
    "f1_per_class = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'f1_per_class_to_data_distribution'\n",
    "object_type = 'figure'\n",
    "caption = 'Available training data per classes compared to the F1 Score per classes in a scatter plot and the result of the Pearson correlation test.'\n",
    "image_size_cm = [14, 8]\n",
    "\n",
    "def create_object(image_size_cm=image_size_cm):\n",
    "    metadata = ex.dataloader.dataset.get_metadata()\n",
    "    grouped = metadata.groupby(['class_id', 'data_set']).size().unstack(fill_value=0).reset_index()\n",
    "    melted = grouped.melt(id_vars='class_id', var_name='data_set', value_name='count')\n",
    "\n",
    "\n",
    "    f1_per_class = ex.get_metrics(\n",
    "        sel_metric='f1_per_class', \n",
    "        eval_subset=['validation']\n",
    "        ).mean().reset_index().rename(columns={'index': 'class_id', 0: 'f1'})\n",
    "    f1_per_class\n",
    "\n",
    "    f1_vs_data = melted.merge(f1_per_class, on='class_id', how='outer')\n",
    "\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(image_size_cm[0]/2.54, image_size_cm[1]/2.54), sharey=True)\n",
    "\n",
    "\n",
    "\n",
    "    data = f1_vs_data[f1_vs_data['data_set'] == 'train']\n",
    "\n",
    "    pearson_corr, p_value = stats.pearsonr(data['f1'], data['count'])\n",
    "\n",
    "\n",
    "    ax = axes\n",
    "    sns.scatterplot(\n",
    "        data=data,\n",
    "        x='count',\n",
    "        y='f1',\n",
    "        s=10,\n",
    "        ax=ax)\n",
    "    ax.set_xlabel(f'Train Data Count')\n",
    "    ax.set_ylabel(f'F1 Score')\n",
    "\n",
    "    ax.text(0.5, -0.25, f'Pearson Test\\nr: {pearson_corr:.2f}, p-value: {p_value:.2f}',\n",
    "        horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "    return fig\n",
    "\n",
    "f1_per_class_to_data_distribution = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a table showing the label and names of the insects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'ClassID_legend'\n",
    "object_type = 'table'\n",
    "caption = 'ClassID legend.'\n",
    "table_size = 'scriptsize'\n",
    "\n",
    "def create_object():\n",
    "    metadata = ex.dataloader.dataset.get_metadata()\n",
    "    unique_class_id = metadata[['class_id', 'species']].drop_duplicates(subset=['class_id']).sort_values(by='class_id')\n",
    "\n",
    "    # Split data into two parts\n",
    "    half = len(unique_class_id) // 2 + len(unique_class_id) % 2\n",
    "    first_half = unique_class_id.iloc[:half].reset_index(drop=True)\n",
    "    second_half = unique_class_id.iloc[half:].reset_index(drop=True)\n",
    "\n",
    "    # Concatenate first half and second half side by side\n",
    "    concatenated = pd.concat([first_half, second_half], axis=1)\n",
    "\n",
    "    # Adjust column names for LaTeX output\n",
    "    concatenated.columns = ['ClassID', 'Species', 'ClassID', 'Species']\n",
    "\n",
    "    # Convert to LaTeX\n",
    "    latex_table = concatenated.to_latex(\n",
    "        index=False, \n",
    "        column_format=\"rlrl\",\n",
    "        header=True\n",
    "    )\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "ClassID_legend = LatexObject(object_type=object_type, label=label, caption=caption, create_object=create_object, table_size=table_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LatexObject.export_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
