{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "titanic_raw = pd.read_csv('titanic.csv')\n",
    "\n",
    "#Processing data\n",
    "\n",
    "titanic_processed = pd.DataFrame()\n",
    "\n",
    "titanic_processed['Survived'] = titanic_raw['Survived']\n",
    "titanic_processed['pclass'] = titanic_raw['Pclass']\n",
    "titanic_processed['Sex'] = (titanic_raw['Sex'] == \"male\").astype(int)\n",
    "titanic_processed['SibSp'] = titanic_raw['SibSp']\n",
    "titanic_processed['Parch'] = titanic_raw['Parch']\n",
    "titanic_processed['Age'] = titanic_raw['Age'].fillna(titanic_raw['Age'].mean())\n",
    "\n",
    "# Splitting data\n",
    "titanic_train = titanic_processed.sample(frac=0.8, random_state=200)\n",
    "titanic_test = titanic_processed.drop(titanic_train.index)\n",
    "\n",
    "x_train = titanic_train.drop('Survived', axis=1).values.astype(np.float32)\n",
    "y_train = titanic_train['Survived'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining dataloader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.from_numpy(x_data).float()\n",
    "        self.y_data = torch.from_numpy(y_data).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "# creating the model\n",
    "\n",
    "class Layer(torch.nn.Module):\n",
    "    \"\"\"A simple feedforward layer with an activation function.\"\"\"\n",
    "    def __init__(self, n_feature, n_hidden, activation_function=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(n_feature, n_hidden)\n",
    "\n",
    "        if activation_function is None:\n",
    "            self.activation_function = torch.nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.activation_function(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\"A simple feedforward neural network with two hidden layers.\"\"\"\n",
    "    def __init__(self, n_feature, n_hidden, n_output, activation_function, activation_function_last):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer0 = Layer(n_feature=n_feature, n_hidden=n_hidden, activation_function=activation_function)\n",
    "        self.layer1 = Layer(n_feature=n_hidden, n_hidden=n_hidden, activation_function=activation_function)\n",
    "        self.layer2 = Layer(n_feature=n_hidden, n_hidden=n_output, activation_function=activation_function_last)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b, f) -> (b, h)\n",
    "        out = self.layer0(x)\n",
    "        # (b, h) -> (b, h)\n",
    "        out = self.layer1(out)\n",
    "        # (b, h) -> (b, o)\n",
    "        out = self.layer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating dataloader\n",
    "dataset = MyDataset(x_train, y_train)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Create a feedforward neural network\n",
    "feedforward = FeedForward(n_feature=5, n_hidden=64, n_output=1, activation_function=torch.nn.ReLU(), activation_function_last=torch.nn.Sigmoid()).cuda()\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.AdamW(feedforward.parameters(), lr=0.001, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 1 | Loss: 0.29383862018585205\n",
      "Epoch 1 | Batch 2 | Loss: 0.3605947196483612\n",
      "Epoch 1 | Batch 3 | Loss: 0.16482847929000854\n",
      "Epoch 1 | Batch 4 | Loss: 0.5793150067329407\n",
      "Epoch 1 | Batch 5 | Loss: 0.222622811794281\n",
      "Epoch 1 | Batch 6 | Loss: 0.1884271800518036\n",
      "Epoch 1 | Batch 7 | Loss: 0.5221000909805298\n",
      "Epoch 1 | Batch 8 | Loss: 0.2671268582344055\n",
      "Epoch 1 | Batch 9 | Loss: 0.3048456907272339\n",
      "Epoch 1 | Batch 10 | Loss: 0.5738648176193237\n",
      "Epoch 1 | Batch 11 | Loss: 0.25158417224884033\n",
      "Epoch 1 | Batch 12 | Loss: 0.49690428376197815\n",
      "Epoch 1 | Batch 13 | Loss: 0.11133798211812973\n",
      "Epoch 1 | Batch 14 | Loss: 0.1097010001540184\n",
      "Epoch 1 | Batch 15 | Loss: 0.16911830008029938\n",
      "Epoch 1 | Batch 16 | Loss: 0.20409291982650757\n",
      "Epoch 1 | Batch 17 | Loss: 0.4399993121623993\n",
      "Epoch 1 | Batch 18 | Loss: 0.35786980390548706\n",
      "Epoch 1 | Batch 19 | Loss: 0.2173219472169876\n",
      "Epoch 1 | Batch 20 | Loss: 0.4424092173576355\n",
      "Epoch 1 | Batch 21 | Loss: 0.35384297370910645\n",
      "Epoch 1 | Batch 22 | Loss: 0.43424344062805176\n",
      "Epoch 1 | Batch 23 | Loss: 0.2331264615058899\n",
      "Epoch 1 | Batch 24 | Loss: 0.10162180662155151\n",
      "Epoch 1 | Batch 25 | Loss: 0.2022683322429657\n",
      "Epoch 1 | Batch 26 | Loss: 0.5578904151916504\n",
      "Epoch 1 | Batch 27 | Loss: 0.5677374601364136\n",
      "Epoch 1 | Batch 28 | Loss: 0.5082206726074219\n",
      "Epoch 1 | Batch 29 | Loss: 0.25756022334098816\n",
      "Epoch 1 | Batch 30 | Loss: 0.8178091049194336\n",
      "Epoch 1 | Batch 31 | Loss: 1.0543745756149292\n",
      "Epoch 1 | Batch 32 | Loss: 0.521851122379303\n",
      "Epoch 1 | Batch 33 | Loss: 0.25904902815818787\n",
      "Epoch 1 | Batch 34 | Loss: 0.4127103090286255\n",
      "Epoch 1 | Batch 35 | Loss: 0.13132528960704803\n",
      "Epoch 1 | Batch 36 | Loss: 0.13286490738391876\n",
      "Epoch 1 | Batch 37 | Loss: 0.41680967807769775\n",
      "Epoch 1 | Batch 38 | Loss: 0.23930665850639343\n",
      "Epoch 1 | Batch 39 | Loss: 0.40572553873062134\n",
      "Epoch 1 | Batch 40 | Loss: 0.19423773884773254\n",
      "Epoch 1 | Batch 41 | Loss: 0.2821860909461975\n",
      "Epoch 1 | Batch 42 | Loss: 0.17863845825195312\n",
      "Epoch 1 | Batch 43 | Loss: 0.10275062173604965\n",
      "Epoch 1 | Batch 44 | Loss: 0.3329601287841797\n",
      "Epoch 1 | Batch 45 | Loss: 0.5941047668457031\n",
      "Epoch 1 | Batch 46 | Loss: 0.13710810244083405\n",
      "Epoch 1 | Batch 47 | Loss: 0.3106701672077179\n",
      "Epoch 1 | Batch 48 | Loss: 0.2027636021375656\n",
      "Epoch 1 | Batch 49 | Loss: 0.5106645822525024\n",
      "Epoch 1 | Batch 50 | Loss: 0.47465506196022034\n",
      "Epoch 1 | Batch 51 | Loss: 0.07860296964645386\n",
      "Epoch 1 | Batch 52 | Loss: 0.3033025860786438\n",
      "Epoch 1 | Batch 53 | Loss: 0.3935337960720062\n",
      "Epoch 1 | Batch 54 | Loss: 0.32889848947525024\n",
      "Epoch 1 | Batch 55 | Loss: 0.28683626651763916\n",
      "Epoch 1 | Batch 56 | Loss: 0.4701964557170868\n",
      "Epoch 1 | Batch 57 | Loss: 0.48216694593429565\n",
      "Epoch 1 | Batch 58 | Loss: 0.25115472078323364\n",
      "Epoch 1 | Batch 59 | Loss: 0.8088431358337402\n",
      "Epoch 1 | Batch 60 | Loss: 0.7611007690429688\n",
      "Epoch 1 | Batch 61 | Loss: 0.22537526488304138\n",
      "Epoch 1 | Batch 62 | Loss: 0.6195324659347534\n",
      "Epoch 1 | Batch 63 | Loss: 0.4411780536174774\n",
      "Epoch 1 | Batch 64 | Loss: 0.48130491375923157\n",
      "Epoch 1 | Batch 65 | Loss: 0.46258431673049927\n",
      "Epoch 1 | Batch 66 | Loss: 0.10702820122241974\n",
      "Epoch 1 | Batch 67 | Loss: 0.22503654658794403\n",
      "Epoch 1 | Batch 68 | Loss: 0.39756256341934204\n",
      "Epoch 1 | Batch 69 | Loss: 0.5479593873023987\n",
      "Epoch 1 | Batch 70 | Loss: 0.20904028415679932\n",
      "Epoch 1 | Batch 71 | Loss: 0.5370989441871643\n",
      "Epoch 1 | Batch 72 | Loss: 0.17936545610427856\n",
      "Epoch 1 | Batch 73 | Loss: 0.11881174892187119\n",
      "Epoch 1 | Batch 74 | Loss: 0.3260297477245331\n",
      "Epoch 1 | Batch 75 | Loss: 0.6914187073707581\n",
      "Epoch 1 | Batch 76 | Loss: 0.16774308681488037\n",
      "Epoch 1 | Batch 77 | Loss: 0.8551703691482544\n",
      "Epoch 1 | Batch 78 | Loss: 0.14149414002895355\n",
      "Epoch 1 | Batch 79 | Loss: 0.6281746029853821\n",
      "Epoch 1 | Batch 80 | Loss: 0.23057961463928223\n",
      "Epoch 1 | Batch 81 | Loss: 0.8489684462547302\n",
      "Epoch 1 | Batch 82 | Loss: 0.8677873611450195\n",
      "Epoch 1 | Batch 83 | Loss: 0.13712209463119507\n",
      "Epoch 1 | Batch 84 | Loss: 0.6314379572868347\n",
      "Epoch 1 | Batch 85 | Loss: 0.11389271169900894\n",
      "Epoch 1 | Batch 86 | Loss: 0.37131595611572266\n",
      "Epoch 1 | Batch 87 | Loss: 0.5559731721878052\n",
      "Epoch 1 | Batch 88 | Loss: 0.8291425108909607\n",
      "Epoch 1 | Batch 89 | Loss: 0.7192496657371521\n",
      "Epoch 1 | Batch 90 | Loss: 0.14332929253578186\n",
      "Epoch 2 | Batch 1 | Loss: 0.6714478731155396\n",
      "Epoch 2 | Batch 2 | Loss: 0.219578355550766\n",
      "Epoch 2 | Batch 3 | Loss: 0.5626552700996399\n",
      "Epoch 2 | Batch 4 | Loss: 0.1564820557832718\n",
      "Epoch 2 | Batch 5 | Loss: 0.15011274814605713\n",
      "Epoch 2 | Batch 6 | Loss: 0.17626816034317017\n",
      "Epoch 2 | Batch 7 | Loss: 0.47166216373443604\n",
      "Epoch 2 | Batch 8 | Loss: 0.4606330096721649\n",
      "Epoch 2 | Batch 9 | Loss: 0.4721662998199463\n",
      "Epoch 2 | Batch 10 | Loss: 0.4395676553249359\n",
      "Epoch 2 | Batch 11 | Loss: 0.18075379729270935\n",
      "Epoch 2 | Batch 12 | Loss: 0.7363019585609436\n",
      "Epoch 2 | Batch 13 | Loss: 0.4429433345794678\n",
      "Epoch 2 | Batch 14 | Loss: 0.2263903170824051\n",
      "Epoch 2 | Batch 15 | Loss: 0.15873286128044128\n",
      "Epoch 2 | Batch 16 | Loss: 0.6928874850273132\n",
      "Epoch 2 | Batch 17 | Loss: 0.16737280786037445\n",
      "Epoch 2 | Batch 18 | Loss: 0.29086270928382874\n",
      "Epoch 2 | Batch 19 | Loss: 0.3391410708427429\n",
      "Epoch 2 | Batch 20 | Loss: 0.2791394889354706\n",
      "Epoch 2 | Batch 21 | Loss: 0.30306172370910645\n",
      "Epoch 2 | Batch 22 | Loss: 0.2715649902820587\n",
      "Epoch 2 | Batch 23 | Loss: 0.10841172188520432\n",
      "Epoch 2 | Batch 24 | Loss: 0.9568925499916077\n",
      "Epoch 2 | Batch 25 | Loss: 0.7175399661064148\n",
      "Epoch 2 | Batch 26 | Loss: 0.9296554327011108\n",
      "Epoch 2 | Batch 27 | Loss: 0.11857257038354874\n",
      "Epoch 2 | Batch 28 | Loss: 0.2139633595943451\n",
      "Epoch 2 | Batch 29 | Loss: 0.11268126964569092\n",
      "Epoch 2 | Batch 30 | Loss: 0.4359695613384247\n",
      "Epoch 2 | Batch 31 | Loss: 0.5353443622589111\n",
      "Epoch 2 | Batch 32 | Loss: 0.6742518544197083\n",
      "Epoch 2 | Batch 33 | Loss: 0.7356349229812622\n",
      "Epoch 2 | Batch 34 | Loss: 0.890271782875061\n",
      "Epoch 2 | Batch 35 | Loss: 0.13899394869804382\n",
      "Epoch 2 | Batch 36 | Loss: 0.1719026267528534\n",
      "Epoch 2 | Batch 37 | Loss: 0.26185232400894165\n",
      "Epoch 2 | Batch 38 | Loss: 0.18782630562782288\n",
      "Epoch 2 | Batch 39 | Loss: 0.36865466833114624\n",
      "Epoch 2 | Batch 40 | Loss: 0.46827057003974915\n",
      "Epoch 2 | Batch 41 | Loss: 0.37006503343582153\n",
      "Epoch 2 | Batch 42 | Loss: 0.1409115493297577\n",
      "Epoch 2 | Batch 43 | Loss: 0.3649797737598419\n",
      "Epoch 2 | Batch 44 | Loss: 0.2606915533542633\n",
      "Epoch 2 | Batch 45 | Loss: 0.33489230275154114\n",
      "Epoch 2 | Batch 46 | Loss: 0.19676508009433746\n",
      "Epoch 2 | Batch 47 | Loss: 0.4188159108161926\n",
      "Epoch 2 | Batch 48 | Loss: 0.5116119980812073\n",
      "Epoch 2 | Batch 49 | Loss: 0.38708436489105225\n",
      "Epoch 2 | Batch 50 | Loss: 0.4301633834838867\n",
      "Epoch 2 | Batch 51 | Loss: 0.41581690311431885\n",
      "Epoch 2 | Batch 52 | Loss: 0.157392680644989\n",
      "Epoch 2 | Batch 53 | Loss: 0.35520139336586\n",
      "Epoch 2 | Batch 54 | Loss: 0.5254598259925842\n",
      "Epoch 2 | Batch 55 | Loss: 0.4917786717414856\n",
      "Epoch 2 | Batch 56 | Loss: 0.14253583550453186\n",
      "Epoch 2 | Batch 57 | Loss: 0.7027566432952881\n",
      "Epoch 2 | Batch 58 | Loss: 0.25938093662261963\n",
      "Epoch 2 | Batch 59 | Loss: 0.3453814387321472\n",
      "Epoch 2 | Batch 60 | Loss: 0.26104480028152466\n",
      "Epoch 2 | Batch 61 | Loss: 0.4066503643989563\n",
      "Epoch 2 | Batch 62 | Loss: 0.6113895773887634\n",
      "Epoch 2 | Batch 63 | Loss: 0.2837430238723755\n",
      "Epoch 2 | Batch 64 | Loss: 0.14644771814346313\n",
      "Epoch 2 | Batch 65 | Loss: 0.3563852906227112\n",
      "Epoch 2 | Batch 66 | Loss: 0.2975603938102722\n",
      "Epoch 2 | Batch 67 | Loss: 0.3018569052219391\n",
      "Epoch 2 | Batch 68 | Loss: 0.28811347484588623\n",
      "Epoch 2 | Batch 69 | Loss: 0.3189734220504761\n",
      "Epoch 2 | Batch 70 | Loss: 0.46902671456336975\n",
      "Epoch 2 | Batch 71 | Loss: 0.606922447681427\n",
      "Epoch 2 | Batch 72 | Loss: 0.6265866756439209\n",
      "Epoch 2 | Batch 73 | Loss: 0.4638087749481201\n",
      "Epoch 2 | Batch 74 | Loss: 0.31783556938171387\n",
      "Epoch 2 | Batch 75 | Loss: 0.22168564796447754\n",
      "Epoch 2 | Batch 76 | Loss: 0.5143657922744751\n",
      "Epoch 2 | Batch 77 | Loss: 0.41706451773643494\n",
      "Epoch 2 | Batch 78 | Loss: 0.4471440613269806\n",
      "Epoch 2 | Batch 79 | Loss: 0.2551771402359009\n",
      "Epoch 2 | Batch 80 | Loss: 0.4999646544456482\n",
      "Epoch 2 | Batch 81 | Loss: 0.14183026552200317\n",
      "Epoch 2 | Batch 82 | Loss: 0.29399919509887695\n",
      "Epoch 2 | Batch 83 | Loss: 0.49513858556747437\n",
      "Epoch 2 | Batch 84 | Loss: 0.46561387181282043\n",
      "Epoch 2 | Batch 85 | Loss: 0.46225816011428833\n",
      "Epoch 2 | Batch 86 | Loss: 0.3183422386646271\n",
      "Epoch 2 | Batch 87 | Loss: 0.39983540773391724\n",
      "Epoch 2 | Batch 88 | Loss: 0.4248751401901245\n",
      "Epoch 2 | Batch 89 | Loss: 0.22498473525047302\n",
      "Epoch 2 | Batch 90 | Loss: 0.3634805977344513\n",
      "Epoch 3 | Batch 1 | Loss: 0.2160911113023758\n",
      "Epoch 3 | Batch 2 | Loss: 0.3806576728820801\n",
      "Epoch 3 | Batch 3 | Loss: 0.6006457805633545\n",
      "Epoch 3 | Batch 4 | Loss: 0.3468223810195923\n",
      "Epoch 3 | Batch 5 | Loss: 0.30436837673187256\n",
      "Epoch 3 | Batch 6 | Loss: 0.25079432129859924\n",
      "Epoch 3 | Batch 7 | Loss: 0.32358384132385254\n",
      "Epoch 3 | Batch 8 | Loss: 0.3857461214065552\n",
      "Epoch 3 | Batch 9 | Loss: 1.1797446012496948\n",
      "Epoch 3 | Batch 10 | Loss: 0.3297332525253296\n",
      "Epoch 3 | Batch 11 | Loss: 0.23201408982276917\n",
      "Epoch 3 | Batch 12 | Loss: 0.4811418056488037\n",
      "Epoch 3 | Batch 13 | Loss: 0.23478849232196808\n",
      "Epoch 3 | Batch 14 | Loss: 0.2861664891242981\n",
      "Epoch 3 | Batch 15 | Loss: 0.40780138969421387\n",
      "Epoch 3 | Batch 16 | Loss: 0.11232172697782516\n",
      "Epoch 3 | Batch 17 | Loss: 0.1775209605693817\n",
      "Epoch 3 | Batch 18 | Loss: 0.5543810129165649\n",
      "Epoch 3 | Batch 19 | Loss: 0.1467316448688507\n",
      "Epoch 3 | Batch 20 | Loss: 0.5256012678146362\n",
      "Epoch 3 | Batch 21 | Loss: 0.3032742738723755\n",
      "Epoch 3 | Batch 22 | Loss: 0.5468427538871765\n",
      "Epoch 3 | Batch 23 | Loss: 0.5171230435371399\n",
      "Epoch 3 | Batch 24 | Loss: 0.6183924078941345\n",
      "Epoch 3 | Batch 25 | Loss: 0.24776926636695862\n",
      "Epoch 3 | Batch 26 | Loss: 0.4720022976398468\n",
      "Epoch 3 | Batch 27 | Loss: 0.5072869658470154\n",
      "Epoch 3 | Batch 28 | Loss: 0.18352638185024261\n",
      "Epoch 3 | Batch 29 | Loss: 0.5859370827674866\n",
      "Epoch 3 | Batch 30 | Loss: 0.19084812700748444\n",
      "Epoch 3 | Batch 31 | Loss: 0.1177627444267273\n",
      "Epoch 3 | Batch 32 | Loss: 0.770112156867981\n",
      "Epoch 3 | Batch 33 | Loss: 0.5033490657806396\n",
      "Epoch 3 | Batch 34 | Loss: 0.41391995549201965\n",
      "Epoch 3 | Batch 35 | Loss: 0.34284141659736633\n",
      "Epoch 3 | Batch 36 | Loss: 0.18782852590084076\n",
      "Epoch 3 | Batch 37 | Loss: 0.15645737946033478\n",
      "Epoch 3 | Batch 38 | Loss: 0.26079633831977844\n",
      "Epoch 3 | Batch 39 | Loss: 0.31374168395996094\n",
      "Epoch 3 | Batch 40 | Loss: 0.42528533935546875\n",
      "Epoch 3 | Batch 41 | Loss: 0.2291235774755478\n",
      "Epoch 3 | Batch 42 | Loss: 0.5927625894546509\n",
      "Epoch 3 | Batch 43 | Loss: 0.10167638212442398\n",
      "Epoch 3 | Batch 44 | Loss: 0.17105403542518616\n",
      "Epoch 3 | Batch 45 | Loss: 0.26028579473495483\n",
      "Epoch 3 | Batch 46 | Loss: 0.5496408939361572\n",
      "Epoch 3 | Batch 47 | Loss: 0.22422203421592712\n",
      "Epoch 3 | Batch 48 | Loss: 0.26894402503967285\n",
      "Epoch 3 | Batch 49 | Loss: 0.3567148447036743\n",
      "Epoch 3 | Batch 50 | Loss: 0.1567317247390747\n",
      "Epoch 3 | Batch 51 | Loss: 0.25566521286964417\n",
      "Epoch 3 | Batch 52 | Loss: 0.39307069778442383\n",
      "Epoch 3 | Batch 53 | Loss: 0.6133319735527039\n",
      "Epoch 3 | Batch 54 | Loss: 0.3332808315753937\n",
      "Epoch 3 | Batch 55 | Loss: 0.12601754069328308\n",
      "Epoch 3 | Batch 56 | Loss: 0.6452460289001465\n",
      "Epoch 3 | Batch 57 | Loss: 0.36582428216934204\n",
      "Epoch 3 | Batch 58 | Loss: 0.8959500193595886\n",
      "Epoch 3 | Batch 59 | Loss: 0.5189552903175354\n",
      "Epoch 3 | Batch 60 | Loss: 0.48682960867881775\n",
      "Epoch 3 | Batch 61 | Loss: 0.2999500334262848\n",
      "Epoch 3 | Batch 62 | Loss: 0.18872444331645966\n",
      "Epoch 3 | Batch 63 | Loss: 0.3913934826850891\n",
      "Epoch 3 | Batch 64 | Loss: 0.08254808932542801\n",
      "Epoch 3 | Batch 65 | Loss: 0.7024101614952087\n",
      "Epoch 3 | Batch 66 | Loss: 0.5951157808303833\n",
      "Epoch 3 | Batch 67 | Loss: 0.4834964871406555\n",
      "Epoch 3 | Batch 68 | Loss: 0.1347467005252838\n",
      "Epoch 3 | Batch 69 | Loss: 0.4797341227531433\n",
      "Epoch 3 | Batch 70 | Loss: 0.30957362055778503\n",
      "Epoch 3 | Batch 71 | Loss: 0.666485071182251\n",
      "Epoch 3 | Batch 72 | Loss: 0.22338935732841492\n",
      "Epoch 3 | Batch 73 | Loss: 0.1932356208562851\n",
      "Epoch 3 | Batch 74 | Loss: 0.3762759566307068\n",
      "Epoch 3 | Batch 75 | Loss: 0.7251044511795044\n",
      "Epoch 3 | Batch 76 | Loss: 0.2681085169315338\n",
      "Epoch 3 | Batch 77 | Loss: 0.5221887826919556\n",
      "Epoch 3 | Batch 78 | Loss: 0.5797932147979736\n",
      "Epoch 3 | Batch 79 | Loss: 0.5707967281341553\n",
      "Epoch 3 | Batch 80 | Loss: 0.22581659257411957\n",
      "Epoch 3 | Batch 81 | Loss: 0.6391574144363403\n",
      "Epoch 3 | Batch 82 | Loss: 0.0443122535943985\n",
      "Epoch 3 | Batch 83 | Loss: 0.623317301273346\n",
      "Epoch 3 | Batch 84 | Loss: 0.2574367821216583\n",
      "Epoch 3 | Batch 85 | Loss: 0.4246397614479065\n",
      "Epoch 3 | Batch 86 | Loss: 0.2569045424461365\n",
      "Epoch 3 | Batch 87 | Loss: 0.28229787945747375\n",
      "Epoch 3 | Batch 88 | Loss: 0.24468693137168884\n",
      "Epoch 3 | Batch 89 | Loss: 0.45787313580513\n",
      "Epoch 3 | Batch 90 | Loss: 0.11241552978754044\n",
      "Epoch 4 | Batch 1 | Loss: 0.1251179277896881\n",
      "Epoch 4 | Batch 2 | Loss: 0.3077218234539032\n",
      "Epoch 4 | Batch 3 | Loss: 0.7584881782531738\n",
      "Epoch 4 | Batch 4 | Loss: 0.12322425097227097\n",
      "Epoch 4 | Batch 5 | Loss: 0.2112087607383728\n",
      "Epoch 4 | Batch 6 | Loss: 0.08251048624515533\n",
      "Epoch 4 | Batch 7 | Loss: 0.4721372127532959\n",
      "Epoch 4 | Batch 8 | Loss: 0.8451197147369385\n",
      "Epoch 4 | Batch 9 | Loss: 0.19809043407440186\n",
      "Epoch 4 | Batch 10 | Loss: 0.4802005887031555\n",
      "Epoch 4 | Batch 11 | Loss: 0.4946706295013428\n",
      "Epoch 4 | Batch 12 | Loss: 0.44155722856521606\n",
      "Epoch 4 | Batch 13 | Loss: 0.6013752222061157\n",
      "Epoch 4 | Batch 14 | Loss: 0.44565510749816895\n",
      "Epoch 4 | Batch 15 | Loss: 0.12233474850654602\n",
      "Epoch 4 | Batch 16 | Loss: 0.19647538661956787\n",
      "Epoch 4 | Batch 17 | Loss: 0.27340060472488403\n",
      "Epoch 4 | Batch 18 | Loss: 0.43920060992240906\n",
      "Epoch 4 | Batch 19 | Loss: 0.08922667801380157\n",
      "Epoch 4 | Batch 20 | Loss: 0.5394191741943359\n",
      "Epoch 4 | Batch 21 | Loss: 0.208842933177948\n",
      "Epoch 4 | Batch 22 | Loss: 0.37344932556152344\n",
      "Epoch 4 | Batch 23 | Loss: 0.4281696081161499\n",
      "Epoch 4 | Batch 24 | Loss: 0.4423356354236603\n",
      "Epoch 4 | Batch 25 | Loss: 0.6968291997909546\n",
      "Epoch 4 | Batch 26 | Loss: 0.14088092744350433\n",
      "Epoch 4 | Batch 27 | Loss: 0.31050726771354675\n",
      "Epoch 4 | Batch 28 | Loss: 0.21330894529819489\n",
      "Epoch 4 | Batch 29 | Loss: 0.5800992846488953\n",
      "Epoch 4 | Batch 30 | Loss: 0.13893020153045654\n",
      "Epoch 4 | Batch 31 | Loss: 0.23215028643608093\n",
      "Epoch 4 | Batch 32 | Loss: 0.2882305979728699\n",
      "Epoch 4 | Batch 33 | Loss: 0.3056944012641907\n",
      "Epoch 4 | Batch 34 | Loss: 0.07675065845251083\n",
      "Epoch 4 | Batch 35 | Loss: 0.03582993894815445\n",
      "Epoch 4 | Batch 36 | Loss: 0.24571821093559265\n",
      "Epoch 4 | Batch 37 | Loss: 0.4192464351654053\n",
      "Epoch 4 | Batch 38 | Loss: 0.263507604598999\n",
      "Epoch 4 | Batch 39 | Loss: 0.20735402405261993\n",
      "Epoch 4 | Batch 40 | Loss: 0.07337072491645813\n",
      "Epoch 4 | Batch 41 | Loss: 0.4508588910102844\n",
      "Epoch 4 | Batch 42 | Loss: 0.9668565988540649\n",
      "Epoch 4 | Batch 43 | Loss: 0.6063166856765747\n",
      "Epoch 4 | Batch 44 | Loss: 0.5012079477310181\n",
      "Epoch 4 | Batch 45 | Loss: 0.1547493189573288\n",
      "Epoch 4 | Batch 46 | Loss: 0.5700789093971252\n",
      "Epoch 4 | Batch 47 | Loss: 0.4655076265335083\n",
      "Epoch 4 | Batch 48 | Loss: 0.34715282917022705\n",
      "Epoch 4 | Batch 49 | Loss: 0.35793131589889526\n",
      "Epoch 4 | Batch 50 | Loss: 0.4276696741580963\n",
      "Epoch 4 | Batch 51 | Loss: 0.566673219203949\n",
      "Epoch 4 | Batch 52 | Loss: 0.4304782748222351\n",
      "Epoch 4 | Batch 53 | Loss: 0.5990157127380371\n",
      "Epoch 4 | Batch 54 | Loss: 0.40050947666168213\n",
      "Epoch 4 | Batch 55 | Loss: 0.30896425247192383\n",
      "Epoch 4 | Batch 56 | Loss: 0.18695202469825745\n",
      "Epoch 4 | Batch 57 | Loss: 0.44233787059783936\n",
      "Epoch 4 | Batch 58 | Loss: 0.4077358543872833\n",
      "Epoch 4 | Batch 59 | Loss: 0.33754032850265503\n",
      "Epoch 4 | Batch 60 | Loss: 0.17398899793624878\n",
      "Epoch 4 | Batch 61 | Loss: 0.5569324493408203\n",
      "Epoch 4 | Batch 62 | Loss: 0.3354334831237793\n",
      "Epoch 4 | Batch 63 | Loss: 0.12529009580612183\n",
      "Epoch 4 | Batch 64 | Loss: 1.1385481357574463\n",
      "Epoch 4 | Batch 65 | Loss: 0.43800294399261475\n",
      "Epoch 4 | Batch 66 | Loss: 0.26684021949768066\n",
      "Epoch 4 | Batch 67 | Loss: 0.3106987178325653\n",
      "Epoch 4 | Batch 68 | Loss: 1.071071743965149\n",
      "Epoch 4 | Batch 69 | Loss: 0.15585957467556\n",
      "Epoch 4 | Batch 70 | Loss: 0.5497487783432007\n",
      "Epoch 4 | Batch 71 | Loss: 0.14263802766799927\n",
      "Epoch 4 | Batch 72 | Loss: 0.3079196512699127\n",
      "Epoch 4 | Batch 73 | Loss: 0.2609221637248993\n",
      "Epoch 4 | Batch 74 | Loss: 0.527380645275116\n",
      "Epoch 4 | Batch 75 | Loss: 0.2821352481842041\n",
      "Epoch 4 | Batch 76 | Loss: 0.49756330251693726\n",
      "Epoch 4 | Batch 77 | Loss: 0.4191807806491852\n",
      "Epoch 4 | Batch 78 | Loss: 0.31748005747795105\n",
      "Epoch 4 | Batch 79 | Loss: 0.6985470652580261\n",
      "Epoch 4 | Batch 80 | Loss: 0.9218966960906982\n",
      "Epoch 4 | Batch 81 | Loss: 0.5234069228172302\n",
      "Epoch 4 | Batch 82 | Loss: 0.6704726815223694\n",
      "Epoch 4 | Batch 83 | Loss: 0.19106239080429077\n",
      "Epoch 4 | Batch 84 | Loss: 0.4841322600841522\n",
      "Epoch 4 | Batch 85 | Loss: 0.6007291674613953\n",
      "Epoch 4 | Batch 86 | Loss: 0.4065820872783661\n",
      "Epoch 4 | Batch 87 | Loss: 0.3054484724998474\n",
      "Epoch 4 | Batch 88 | Loss: 0.3330712616443634\n",
      "Epoch 4 | Batch 89 | Loss: 0.3473571538925171\n",
      "Epoch 4 | Batch 90 | Loss: 0.13688357174396515\n",
      "Epoch 5 | Batch 1 | Loss: 0.4626721739768982\n",
      "Epoch 5 | Batch 2 | Loss: 0.7912817597389221\n",
      "Epoch 5 | Batch 3 | Loss: 0.19378185272216797\n",
      "Epoch 5 | Batch 4 | Loss: 0.506216287612915\n",
      "Epoch 5 | Batch 5 | Loss: 0.6956348419189453\n",
      "Epoch 5 | Batch 6 | Loss: 0.5139838457107544\n",
      "Epoch 5 | Batch 7 | Loss: 0.4163544476032257\n",
      "Epoch 5 | Batch 8 | Loss: 0.12966567277908325\n",
      "Epoch 5 | Batch 9 | Loss: 0.42654532194137573\n",
      "Epoch 5 | Batch 10 | Loss: 0.29332318902015686\n",
      "Epoch 5 | Batch 11 | Loss: 0.2193559855222702\n",
      "Epoch 5 | Batch 12 | Loss: 0.1677149534225464\n",
      "Epoch 5 | Batch 13 | Loss: 0.35221803188323975\n",
      "Epoch 5 | Batch 14 | Loss: 0.7966048717498779\n",
      "Epoch 5 | Batch 15 | Loss: 0.24061235785484314\n",
      "Epoch 5 | Batch 16 | Loss: 0.16774511337280273\n",
      "Epoch 5 | Batch 17 | Loss: 0.14678093791007996\n",
      "Epoch 5 | Batch 18 | Loss: 0.42489737272262573\n",
      "Epoch 5 | Batch 19 | Loss: 0.18380986154079437\n",
      "Epoch 5 | Batch 20 | Loss: 0.4188852608203888\n",
      "Epoch 5 | Batch 21 | Loss: 0.47373664379119873\n",
      "Epoch 5 | Batch 22 | Loss: 0.27800244092941284\n",
      "Epoch 5 | Batch 23 | Loss: 0.5312737226486206\n",
      "Epoch 5 | Batch 24 | Loss: 0.17913322150707245\n",
      "Epoch 5 | Batch 25 | Loss: 0.18007880449295044\n",
      "Epoch 5 | Batch 26 | Loss: 0.39031559228897095\n",
      "Epoch 5 | Batch 27 | Loss: 0.5616296529769897\n",
      "Epoch 5 | Batch 28 | Loss: 0.26403725147247314\n",
      "Epoch 5 | Batch 29 | Loss: 0.503951907157898\n",
      "Epoch 5 | Batch 30 | Loss: 0.8081516027450562\n",
      "Epoch 5 | Batch 31 | Loss: 0.537044107913971\n",
      "Epoch 5 | Batch 32 | Loss: 0.4750592112541199\n",
      "Epoch 5 | Batch 33 | Loss: 0.2499604970216751\n",
      "Epoch 5 | Batch 34 | Loss: 0.6393120884895325\n",
      "Epoch 5 | Batch 35 | Loss: 0.10282102227210999\n",
      "Epoch 5 | Batch 36 | Loss: 0.17937956750392914\n",
      "Epoch 5 | Batch 37 | Loss: 0.11246684938669205\n",
      "Epoch 5 | Batch 38 | Loss: 0.35916808247566223\n",
      "Epoch 5 | Batch 39 | Loss: 0.31578630208969116\n",
      "Epoch 5 | Batch 40 | Loss: 0.6357978582382202\n",
      "Epoch 5 | Batch 41 | Loss: 0.30655306577682495\n",
      "Epoch 5 | Batch 42 | Loss: 0.2866518199443817\n",
      "Epoch 5 | Batch 43 | Loss: 0.3897892236709595\n",
      "Epoch 5 | Batch 44 | Loss: 0.31205248832702637\n",
      "Epoch 5 | Batch 45 | Loss: 0.4342651069164276\n",
      "Epoch 5 | Batch 46 | Loss: 0.5016303062438965\n",
      "Epoch 5 | Batch 47 | Loss: 0.6666724681854248\n",
      "Epoch 5 | Batch 48 | Loss: 0.6884520053863525\n",
      "Epoch 5 | Batch 49 | Loss: 0.19391241669654846\n",
      "Epoch 5 | Batch 50 | Loss: 0.15190133452415466\n",
      "Epoch 5 | Batch 51 | Loss: 0.34715962409973145\n",
      "Epoch 5 | Batch 52 | Loss: 0.17490825057029724\n",
      "Epoch 5 | Batch 53 | Loss: 0.48177269101142883\n",
      "Epoch 5 | Batch 54 | Loss: 0.7818050980567932\n",
      "Epoch 5 | Batch 55 | Loss: 0.3421812951564789\n",
      "Epoch 5 | Batch 56 | Loss: 0.09065981954336166\n",
      "Epoch 5 | Batch 57 | Loss: 0.2928713858127594\n",
      "Epoch 5 | Batch 58 | Loss: 0.3165460228919983\n",
      "Epoch 5 | Batch 59 | Loss: 0.47363194823265076\n",
      "Epoch 5 | Batch 60 | Loss: 0.4082280695438385\n",
      "Epoch 5 | Batch 61 | Loss: 0.28883904218673706\n",
      "Epoch 5 | Batch 62 | Loss: 0.08134637027978897\n",
      "Epoch 5 | Batch 63 | Loss: 0.4162144362926483\n",
      "Epoch 5 | Batch 64 | Loss: 0.6111769080162048\n",
      "Epoch 5 | Batch 65 | Loss: 0.36081188917160034\n",
      "Epoch 5 | Batch 66 | Loss: 0.5793578624725342\n",
      "Epoch 5 | Batch 67 | Loss: 0.30120664834976196\n",
      "Epoch 5 | Batch 68 | Loss: 0.18813182413578033\n",
      "Epoch 5 | Batch 69 | Loss: 0.4767165780067444\n",
      "Epoch 5 | Batch 70 | Loss: 0.5461497902870178\n",
      "Epoch 5 | Batch 71 | Loss: 0.5499014854431152\n",
      "Epoch 5 | Batch 72 | Loss: 0.0967043787240982\n",
      "Epoch 5 | Batch 73 | Loss: 0.4338475465774536\n",
      "Epoch 5 | Batch 74 | Loss: 0.477780282497406\n",
      "Epoch 5 | Batch 75 | Loss: 0.31977275013923645\n",
      "Epoch 5 | Batch 76 | Loss: 0.22806474566459656\n",
      "Epoch 5 | Batch 77 | Loss: 0.3654804825782776\n",
      "Epoch 5 | Batch 78 | Loss: 0.39553090929985046\n",
      "Epoch 5 | Batch 79 | Loss: 0.6013736724853516\n",
      "Epoch 5 | Batch 80 | Loss: 0.5501983165740967\n",
      "Epoch 5 | Batch 81 | Loss: 0.1565285176038742\n",
      "Epoch 5 | Batch 82 | Loss: 0.7208292484283447\n",
      "Epoch 5 | Batch 83 | Loss: 0.3284026086330414\n",
      "Epoch 5 | Batch 84 | Loss: 0.15980474650859833\n",
      "Epoch 5 | Batch 85 | Loss: 0.21896791458129883\n",
      "Epoch 5 | Batch 86 | Loss: 0.7223212718963623\n",
      "Epoch 5 | Batch 87 | Loss: 0.5466418862342834\n",
      "Epoch 5 | Batch 88 | Loss: 0.49235081672668457\n",
      "Epoch 5 | Batch 89 | Loss: 0.16693007946014404\n",
      "Epoch 5 | Batch 90 | Loss: 0.7072329521179199\n",
      "Epoch 6 | Batch 1 | Loss: 0.4963924288749695\n",
      "Epoch 6 | Batch 2 | Loss: 0.2881532907485962\n",
      "Epoch 6 | Batch 3 | Loss: 0.4068228602409363\n",
      "Epoch 6 | Batch 4 | Loss: 0.23087157309055328\n",
      "Epoch 6 | Batch 5 | Loss: 0.3283568024635315\n",
      "Epoch 6 | Batch 6 | Loss: 0.19492295384407043\n",
      "Epoch 6 | Batch 7 | Loss: 0.2960144877433777\n",
      "Epoch 6 | Batch 8 | Loss: 0.32419756054878235\n",
      "Epoch 6 | Batch 9 | Loss: 0.27770504355430603\n",
      "Epoch 6 | Batch 10 | Loss: 0.19929224252700806\n",
      "Epoch 6 | Batch 11 | Loss: 0.15843632817268372\n",
      "Epoch 6 | Batch 12 | Loss: 0.1947460174560547\n",
      "Epoch 6 | Batch 13 | Loss: 0.7802816033363342\n",
      "Epoch 6 | Batch 14 | Loss: 0.2273135781288147\n",
      "Epoch 6 | Batch 15 | Loss: 0.14035049080848694\n",
      "Epoch 6 | Batch 16 | Loss: 0.2264828085899353\n",
      "Epoch 6 | Batch 17 | Loss: 0.3747009038925171\n",
      "Epoch 6 | Batch 18 | Loss: 0.4479998052120209\n",
      "Epoch 6 | Batch 19 | Loss: 0.09609854221343994\n",
      "Epoch 6 | Batch 20 | Loss: 0.5480403900146484\n",
      "Epoch 6 | Batch 21 | Loss: 0.11778382956981659\n",
      "Epoch 6 | Batch 22 | Loss: 0.6446768045425415\n",
      "Epoch 6 | Batch 23 | Loss: 0.6854937076568604\n",
      "Epoch 6 | Batch 24 | Loss: 0.6371541023254395\n",
      "Epoch 6 | Batch 25 | Loss: 0.46640831232070923\n",
      "Epoch 6 | Batch 26 | Loss: 0.3622179925441742\n",
      "Epoch 6 | Batch 27 | Loss: 0.32490211725234985\n",
      "Epoch 6 | Batch 28 | Loss: 0.11807725578546524\n",
      "Epoch 6 | Batch 29 | Loss: 0.11716223508119583\n",
      "Epoch 6 | Batch 30 | Loss: 0.21740469336509705\n",
      "Epoch 6 | Batch 31 | Loss: 0.3621676564216614\n",
      "Epoch 6 | Batch 32 | Loss: 0.45929262042045593\n",
      "Epoch 6 | Batch 33 | Loss: 0.22462981939315796\n",
      "Epoch 6 | Batch 34 | Loss: 0.12151302397251129\n",
      "Epoch 6 | Batch 35 | Loss: 0.23612701892852783\n",
      "Epoch 6 | Batch 36 | Loss: 0.3725780248641968\n",
      "Epoch 6 | Batch 37 | Loss: 0.9210150241851807\n",
      "Epoch 6 | Batch 38 | Loss: 0.6119093894958496\n",
      "Epoch 6 | Batch 39 | Loss: 0.7442216873168945\n",
      "Epoch 6 | Batch 40 | Loss: 0.20571689307689667\n",
      "Epoch 6 | Batch 41 | Loss: 0.4677509665489197\n",
      "Epoch 6 | Batch 42 | Loss: 0.3700321316719055\n",
      "Epoch 6 | Batch 43 | Loss: 0.5482250452041626\n",
      "Epoch 6 | Batch 44 | Loss: 1.0039554834365845\n",
      "Epoch 6 | Batch 45 | Loss: 0.8371224999427795\n",
      "Epoch 6 | Batch 46 | Loss: 0.5908265113830566\n",
      "Epoch 6 | Batch 47 | Loss: 0.5297008752822876\n",
      "Epoch 6 | Batch 48 | Loss: 0.39437419176101685\n",
      "Epoch 6 | Batch 49 | Loss: 0.3991210460662842\n",
      "Epoch 6 | Batch 50 | Loss: 0.3170771598815918\n",
      "Epoch 6 | Batch 51 | Loss: 0.1637180745601654\n",
      "Epoch 6 | Batch 52 | Loss: 0.14605939388275146\n",
      "Epoch 6 | Batch 53 | Loss: 0.29937151074409485\n",
      "Epoch 6 | Batch 54 | Loss: 0.35815665125846863\n",
      "Epoch 6 | Batch 55 | Loss: 0.3666493892669678\n",
      "Epoch 6 | Batch 56 | Loss: 0.6522136926651001\n",
      "Epoch 6 | Batch 57 | Loss: 0.3729274868965149\n",
      "Epoch 6 | Batch 58 | Loss: 0.16654276847839355\n",
      "Epoch 6 | Batch 59 | Loss: 0.2946529984474182\n",
      "Epoch 6 | Batch 60 | Loss: 0.30396732687950134\n",
      "Epoch 6 | Batch 61 | Loss: 0.6482435464859009\n",
      "Epoch 6 | Batch 62 | Loss: 0.19021792709827423\n",
      "Epoch 6 | Batch 63 | Loss: 0.3513096570968628\n",
      "Epoch 6 | Batch 64 | Loss: 0.6381834745407104\n",
      "Epoch 6 | Batch 65 | Loss: 0.6333310008049011\n",
      "Epoch 6 | Batch 66 | Loss: 0.18548405170440674\n",
      "Epoch 6 | Batch 67 | Loss: 0.4330853819847107\n",
      "Epoch 6 | Batch 68 | Loss: 0.3511836528778076\n",
      "Epoch 6 | Batch 69 | Loss: 0.2768971920013428\n",
      "Epoch 6 | Batch 70 | Loss: 0.19480475783348083\n",
      "Epoch 6 | Batch 71 | Loss: 0.5655912160873413\n",
      "Epoch 6 | Batch 72 | Loss: 0.6707659959793091\n",
      "Epoch 6 | Batch 73 | Loss: 0.34533923864364624\n",
      "Epoch 6 | Batch 74 | Loss: 0.20926691591739655\n",
      "Epoch 6 | Batch 75 | Loss: 0.21679070591926575\n",
      "Epoch 6 | Batch 76 | Loss: 0.8424069285392761\n",
      "Epoch 6 | Batch 77 | Loss: 0.47440963983535767\n",
      "Epoch 6 | Batch 78 | Loss: 0.5259947776794434\n",
      "Epoch 6 | Batch 79 | Loss: 0.4800008535385132\n",
      "Epoch 6 | Batch 80 | Loss: 0.1911802738904953\n",
      "Epoch 6 | Batch 81 | Loss: 0.30051976442337036\n",
      "Epoch 6 | Batch 82 | Loss: 0.4952421188354492\n",
      "Epoch 6 | Batch 83 | Loss: 0.11635269969701767\n",
      "Epoch 6 | Batch 84 | Loss: 0.3784482777118683\n",
      "Epoch 6 | Batch 85 | Loss: 0.1679486632347107\n",
      "Epoch 6 | Batch 86 | Loss: 0.30672985315322876\n",
      "Epoch 6 | Batch 87 | Loss: 0.7361147999763489\n",
      "Epoch 6 | Batch 88 | Loss: 0.1408074051141739\n",
      "Epoch 6 | Batch 89 | Loss: 0.31874820590019226\n",
      "Epoch 6 | Batch 90 | Loss: 0.04169877618551254\n",
      "Epoch 7 | Batch 1 | Loss: 0.40604281425476074\n",
      "Epoch 7 | Batch 2 | Loss: 0.4092312157154083\n",
      "Epoch 7 | Batch 3 | Loss: 0.37419456243515015\n",
      "Epoch 7 | Batch 4 | Loss: 0.4833962321281433\n",
      "Epoch 7 | Batch 5 | Loss: 0.36114898324012756\n",
      "Epoch 7 | Batch 6 | Loss: 0.20401841402053833\n",
      "Epoch 7 | Batch 7 | Loss: 0.3934031128883362\n",
      "Epoch 7 | Batch 8 | Loss: 0.2789810001850128\n",
      "Epoch 7 | Batch 9 | Loss: 0.14505447447299957\n",
      "Epoch 7 | Batch 10 | Loss: 0.43369436264038086\n",
      "Epoch 7 | Batch 11 | Loss: 0.12184687703847885\n",
      "Epoch 7 | Batch 12 | Loss: 0.4713923931121826\n",
      "Epoch 7 | Batch 13 | Loss: 0.2644399404525757\n",
      "Epoch 7 | Batch 14 | Loss: 0.4991624355316162\n",
      "Epoch 7 | Batch 15 | Loss: 0.13896653056144714\n",
      "Epoch 7 | Batch 16 | Loss: 0.49501922726631165\n",
      "Epoch 7 | Batch 17 | Loss: 0.536936342716217\n",
      "Epoch 7 | Batch 18 | Loss: 0.4816904067993164\n",
      "Epoch 7 | Batch 19 | Loss: 0.43382057547569275\n",
      "Epoch 7 | Batch 20 | Loss: 0.7368029356002808\n",
      "Epoch 7 | Batch 21 | Loss: 0.5249252319335938\n",
      "Epoch 7 | Batch 22 | Loss: 0.45127782225608826\n",
      "Epoch 7 | Batch 23 | Loss: 0.08811433613300323\n",
      "Epoch 7 | Batch 24 | Loss: 0.26148536801338196\n",
      "Epoch 7 | Batch 25 | Loss: 0.35602205991744995\n",
      "Epoch 7 | Batch 26 | Loss: 0.15960097312927246\n",
      "Epoch 7 | Batch 27 | Loss: 0.39782407879829407\n",
      "Epoch 7 | Batch 28 | Loss: 0.29945528507232666\n",
      "Epoch 7 | Batch 29 | Loss: 0.0779554694890976\n",
      "Epoch 7 | Batch 30 | Loss: 0.3960001766681671\n",
      "Epoch 7 | Batch 31 | Loss: 0.5638301968574524\n",
      "Epoch 7 | Batch 32 | Loss: 0.6425517797470093\n",
      "Epoch 7 | Batch 33 | Loss: 0.3632301688194275\n",
      "Epoch 7 | Batch 34 | Loss: 0.2380100041627884\n",
      "Epoch 7 | Batch 35 | Loss: 0.49627792835235596\n",
      "Epoch 7 | Batch 36 | Loss: 0.09170307964086533\n",
      "Epoch 7 | Batch 37 | Loss: 0.4056330919265747\n",
      "Epoch 7 | Batch 38 | Loss: 0.465912401676178\n",
      "Epoch 7 | Batch 39 | Loss: 0.11700086295604706\n",
      "Epoch 7 | Batch 40 | Loss: 0.1569412350654602\n",
      "Epoch 7 | Batch 41 | Loss: 0.12938843667507172\n",
      "Epoch 7 | Batch 42 | Loss: 0.6988025307655334\n",
      "Epoch 7 | Batch 43 | Loss: 0.4275849461555481\n",
      "Epoch 7 | Batch 44 | Loss: 0.5183945894241333\n",
      "Epoch 7 | Batch 45 | Loss: 0.6464939117431641\n",
      "Epoch 7 | Batch 46 | Loss: 0.5855278968811035\n",
      "Epoch 7 | Batch 47 | Loss: 0.4706481993198395\n",
      "Epoch 7 | Batch 48 | Loss: 0.6662801504135132\n",
      "Epoch 7 | Batch 49 | Loss: 0.3857015371322632\n",
      "Epoch 7 | Batch 50 | Loss: 0.4209572672843933\n",
      "Epoch 7 | Batch 51 | Loss: 0.2841545343399048\n",
      "Epoch 7 | Batch 52 | Loss: 0.2768925726413727\n",
      "Epoch 7 | Batch 53 | Loss: 0.6126166582107544\n",
      "Epoch 7 | Batch 54 | Loss: 0.2732120454311371\n",
      "Epoch 7 | Batch 55 | Loss: 0.3179292678833008\n",
      "Epoch 7 | Batch 56 | Loss: 0.2046380490064621\n",
      "Epoch 7 | Batch 57 | Loss: 0.1903625875711441\n",
      "Epoch 7 | Batch 58 | Loss: 0.4711996912956238\n",
      "Epoch 7 | Batch 59 | Loss: 0.6363109350204468\n",
      "Epoch 7 | Batch 60 | Loss: 0.7281806468963623\n",
      "Epoch 7 | Batch 61 | Loss: 0.24340656399726868\n",
      "Epoch 7 | Batch 62 | Loss: 0.6878536939620972\n",
      "Epoch 7 | Batch 63 | Loss: 0.6278628706932068\n",
      "Epoch 7 | Batch 64 | Loss: 0.43011385202407837\n",
      "Epoch 7 | Batch 65 | Loss: 0.05679488927125931\n",
      "Epoch 7 | Batch 66 | Loss: 0.8682005405426025\n",
      "Epoch 7 | Batch 67 | Loss: 0.2547205686569214\n",
      "Epoch 7 | Batch 68 | Loss: 0.8840057849884033\n",
      "Epoch 7 | Batch 69 | Loss: 0.18729349970817566\n",
      "Epoch 7 | Batch 70 | Loss: 0.4326690435409546\n",
      "Epoch 7 | Batch 71 | Loss: 0.19917170703411102\n",
      "Epoch 7 | Batch 72 | Loss: 0.13272282481193542\n",
      "Epoch 7 | Batch 73 | Loss: 0.5583995580673218\n",
      "Epoch 7 | Batch 74 | Loss: 0.4543505907058716\n",
      "Epoch 7 | Batch 75 | Loss: 0.24865016341209412\n",
      "Epoch 7 | Batch 76 | Loss: 0.2438879907131195\n",
      "Epoch 7 | Batch 77 | Loss: 0.36651352047920227\n",
      "Epoch 7 | Batch 78 | Loss: 0.5202455520629883\n",
      "Epoch 7 | Batch 79 | Loss: 0.16991175711154938\n",
      "Epoch 7 | Batch 80 | Loss: 0.4903140068054199\n",
      "Epoch 7 | Batch 81 | Loss: 0.19111193716526031\n",
      "Epoch 7 | Batch 82 | Loss: 0.2161780148744583\n",
      "Epoch 7 | Batch 83 | Loss: 0.6296013593673706\n",
      "Epoch 7 | Batch 84 | Loss: 0.2029138207435608\n",
      "Epoch 7 | Batch 85 | Loss: 0.35939744114875793\n",
      "Epoch 7 | Batch 86 | Loss: 0.20227396488189697\n",
      "Epoch 7 | Batch 87 | Loss: 0.9189637899398804\n",
      "Epoch 7 | Batch 88 | Loss: 0.15870323777198792\n",
      "Epoch 7 | Batch 89 | Loss: 0.46715348958969116\n",
      "Epoch 7 | Batch 90 | Loss: 0.08489728718996048\n",
      "Epoch 8 | Batch 1 | Loss: 0.7185970544815063\n",
      "Epoch 8 | Batch 2 | Loss: 0.557824432849884\n",
      "Epoch 8 | Batch 3 | Loss: 0.2914327383041382\n",
      "Epoch 8 | Batch 4 | Loss: 0.32734814286231995\n",
      "Epoch 8 | Batch 5 | Loss: 0.24901127815246582\n",
      "Epoch 8 | Batch 6 | Loss: 0.4731258153915405\n",
      "Epoch 8 | Batch 7 | Loss: 0.23191827535629272\n",
      "Epoch 8 | Batch 8 | Loss: 0.28734642267227173\n",
      "Epoch 8 | Batch 9 | Loss: 0.5013078451156616\n",
      "Epoch 8 | Batch 10 | Loss: 0.32428401708602905\n",
      "Epoch 8 | Batch 11 | Loss: 0.3534092307090759\n",
      "Epoch 8 | Batch 12 | Loss: 0.5069136023521423\n",
      "Epoch 8 | Batch 13 | Loss: 0.4188116192817688\n",
      "Epoch 8 | Batch 14 | Loss: 0.5023250579833984\n",
      "Epoch 8 | Batch 15 | Loss: 0.2937934994697571\n",
      "Epoch 8 | Batch 16 | Loss: 0.4896341562271118\n",
      "Epoch 8 | Batch 17 | Loss: 0.26542767882347107\n",
      "Epoch 8 | Batch 18 | Loss: 0.2218150794506073\n",
      "Epoch 8 | Batch 19 | Loss: 0.28794974088668823\n",
      "Epoch 8 | Batch 20 | Loss: 0.29568397998809814\n",
      "Epoch 8 | Batch 21 | Loss: 0.7791833877563477\n",
      "Epoch 8 | Batch 22 | Loss: 0.9441391229629517\n",
      "Epoch 8 | Batch 23 | Loss: 0.6618475914001465\n",
      "Epoch 8 | Batch 24 | Loss: 0.44007131457328796\n",
      "Epoch 8 | Batch 25 | Loss: 0.6050751805305481\n",
      "Epoch 8 | Batch 26 | Loss: 0.589445948600769\n",
      "Epoch 8 | Batch 27 | Loss: 0.696195662021637\n",
      "Epoch 8 | Batch 28 | Loss: 0.37102946639060974\n",
      "Epoch 8 | Batch 29 | Loss: 0.23841099441051483\n",
      "Epoch 8 | Batch 30 | Loss: 0.42509639263153076\n",
      "Epoch 8 | Batch 31 | Loss: 0.45512405037879944\n",
      "Epoch 8 | Batch 32 | Loss: 0.3586214780807495\n",
      "Epoch 8 | Batch 33 | Loss: 0.44144755601882935\n",
      "Epoch 8 | Batch 34 | Loss: 0.1705404669046402\n",
      "Epoch 8 | Batch 35 | Loss: 0.5586841702461243\n",
      "Epoch 8 | Batch 36 | Loss: 0.4009561538696289\n",
      "Epoch 8 | Batch 37 | Loss: 0.266457736492157\n",
      "Epoch 8 | Batch 38 | Loss: 0.44645461440086365\n",
      "Epoch 8 | Batch 39 | Loss: 0.5388127565383911\n",
      "Epoch 8 | Batch 40 | Loss: 0.28128138184547424\n",
      "Epoch 8 | Batch 41 | Loss: 0.24759836494922638\n",
      "Epoch 8 | Batch 42 | Loss: 0.19762787222862244\n",
      "Epoch 8 | Batch 43 | Loss: 0.4683378040790558\n",
      "Epoch 8 | Batch 44 | Loss: 0.6440849304199219\n",
      "Epoch 8 | Batch 45 | Loss: 0.5769388675689697\n",
      "Epoch 8 | Batch 46 | Loss: 0.07667693495750427\n",
      "Epoch 8 | Batch 47 | Loss: 0.27423834800720215\n",
      "Epoch 8 | Batch 48 | Loss: 0.3510900139808655\n",
      "Epoch 8 | Batch 49 | Loss: 0.5055892467498779\n",
      "Epoch 8 | Batch 50 | Loss: 0.21498072147369385\n",
      "Epoch 8 | Batch 51 | Loss: 0.3632257878780365\n",
      "Epoch 8 | Batch 52 | Loss: 0.3929150104522705\n",
      "Epoch 8 | Batch 53 | Loss: 0.2859727740287781\n",
      "Epoch 8 | Batch 54 | Loss: 0.23665285110473633\n",
      "Epoch 8 | Batch 55 | Loss: 0.12932230532169342\n",
      "Epoch 8 | Batch 56 | Loss: 0.197295680642128\n",
      "Epoch 8 | Batch 57 | Loss: 0.19853490591049194\n",
      "Epoch 8 | Batch 58 | Loss: 0.44754552841186523\n",
      "Epoch 8 | Batch 59 | Loss: 0.3913295269012451\n",
      "Epoch 8 | Batch 60 | Loss: 0.5397569537162781\n",
      "Epoch 8 | Batch 61 | Loss: 0.3166552484035492\n",
      "Epoch 8 | Batch 62 | Loss: 0.259406179189682\n",
      "Epoch 8 | Batch 63 | Loss: 0.28814396262168884\n",
      "Epoch 8 | Batch 64 | Loss: 0.20084235072135925\n",
      "Epoch 8 | Batch 65 | Loss: 0.1161007359623909\n",
      "Epoch 8 | Batch 66 | Loss: 0.2572479546070099\n",
      "Epoch 8 | Batch 67 | Loss: 0.18011173605918884\n",
      "Epoch 8 | Batch 68 | Loss: 0.7448259592056274\n",
      "Epoch 8 | Batch 69 | Loss: 0.491061270236969\n",
      "Epoch 8 | Batch 70 | Loss: 0.1435391902923584\n",
      "Epoch 8 | Batch 71 | Loss: 0.16874465346336365\n",
      "Epoch 8 | Batch 72 | Loss: 0.19598613679409027\n",
      "Epoch 8 | Batch 73 | Loss: 0.0737733393907547\n",
      "Epoch 8 | Batch 74 | Loss: 0.3799610435962677\n",
      "Epoch 8 | Batch 75 | Loss: 1.1756013631820679\n",
      "Epoch 8 | Batch 76 | Loss: 0.25807425379753113\n",
      "Epoch 8 | Batch 77 | Loss: 0.8408946394920349\n",
      "Epoch 8 | Batch 78 | Loss: 0.4089130759239197\n",
      "Epoch 8 | Batch 79 | Loss: 0.06495005637407303\n",
      "Epoch 8 | Batch 80 | Loss: 0.6173192262649536\n",
      "Epoch 8 | Batch 81 | Loss: 0.15431176126003265\n",
      "Epoch 8 | Batch 82 | Loss: 0.29282692074775696\n",
      "Epoch 8 | Batch 83 | Loss: 0.2806278169155121\n",
      "Epoch 8 | Batch 84 | Loss: 0.4014173150062561\n",
      "Epoch 8 | Batch 85 | Loss: 0.5207862854003906\n",
      "Epoch 8 | Batch 86 | Loss: 0.15746861696243286\n",
      "Epoch 8 | Batch 87 | Loss: 0.4318292737007141\n",
      "Epoch 8 | Batch 88 | Loss: 0.16625458002090454\n",
      "Epoch 8 | Batch 89 | Loss: 0.15341845154762268\n",
      "Epoch 8 | Batch 90 | Loss: 0.15301699936389923\n",
      "Epoch 9 | Batch 1 | Loss: 0.24988828599452972\n",
      "Epoch 9 | Batch 2 | Loss: 0.6211583614349365\n",
      "Epoch 9 | Batch 3 | Loss: 0.5982232093811035\n",
      "Epoch 9 | Batch 4 | Loss: 0.2383049726486206\n",
      "Epoch 9 | Batch 5 | Loss: 0.38455504179000854\n",
      "Epoch 9 | Batch 6 | Loss: 0.118878573179245\n",
      "Epoch 9 | Batch 7 | Loss: 0.7509894371032715\n",
      "Epoch 9 | Batch 8 | Loss: 0.4111293852329254\n",
      "Epoch 9 | Batch 9 | Loss: 0.1974020004272461\n",
      "Epoch 9 | Batch 10 | Loss: 0.2265925109386444\n",
      "Epoch 9 | Batch 11 | Loss: 0.7726827263832092\n",
      "Epoch 9 | Batch 12 | Loss: 0.40982696413993835\n",
      "Epoch 9 | Batch 13 | Loss: 0.10487700253725052\n",
      "Epoch 9 | Batch 14 | Loss: 0.5386816263198853\n",
      "Epoch 9 | Batch 15 | Loss: 0.4362972676753998\n",
      "Epoch 9 | Batch 16 | Loss: 0.6522879600524902\n",
      "Epoch 9 | Batch 17 | Loss: 0.6676579713821411\n",
      "Epoch 9 | Batch 18 | Loss: 0.21530890464782715\n",
      "Epoch 9 | Batch 19 | Loss: 0.2030491828918457\n",
      "Epoch 9 | Batch 20 | Loss: 0.34698057174682617\n",
      "Epoch 9 | Batch 21 | Loss: 0.6009088158607483\n",
      "Epoch 9 | Batch 22 | Loss: 0.24638278782367706\n",
      "Epoch 9 | Batch 23 | Loss: 0.31850379705429077\n",
      "Epoch 9 | Batch 24 | Loss: 0.5100133419036865\n",
      "Epoch 9 | Batch 25 | Loss: 0.17415018379688263\n",
      "Epoch 9 | Batch 26 | Loss: 0.20597994327545166\n",
      "Epoch 9 | Batch 27 | Loss: 0.22296911478042603\n",
      "Epoch 9 | Batch 28 | Loss: 0.40552496910095215\n",
      "Epoch 9 | Batch 29 | Loss: 0.27324235439300537\n",
      "Epoch 9 | Batch 30 | Loss: 0.13364818692207336\n",
      "Epoch 9 | Batch 31 | Loss: 0.1179899051785469\n",
      "Epoch 9 | Batch 32 | Loss: 0.2587694227695465\n",
      "Epoch 9 | Batch 33 | Loss: 0.38750654458999634\n",
      "Epoch 9 | Batch 34 | Loss: 0.3374693691730499\n",
      "Epoch 9 | Batch 35 | Loss: 0.45864927768707275\n",
      "Epoch 9 | Batch 36 | Loss: 0.1859239637851715\n",
      "Epoch 9 | Batch 37 | Loss: 0.10306791961193085\n",
      "Epoch 9 | Batch 38 | Loss: 0.16584284603595734\n",
      "Epoch 9 | Batch 39 | Loss: 0.9832130670547485\n",
      "Epoch 9 | Batch 40 | Loss: 0.46414244174957275\n",
      "Epoch 9 | Batch 41 | Loss: 0.6604514122009277\n",
      "Epoch 9 | Batch 42 | Loss: 0.09669303148984909\n",
      "Epoch 9 | Batch 43 | Loss: 0.6701743602752686\n",
      "Epoch 9 | Batch 44 | Loss: 0.09291058778762817\n",
      "Epoch 9 | Batch 45 | Loss: 1.1023491621017456\n",
      "Epoch 9 | Batch 46 | Loss: 0.15205366909503937\n",
      "Epoch 9 | Batch 47 | Loss: 0.9671902656555176\n",
      "Epoch 9 | Batch 48 | Loss: 0.06962719559669495\n",
      "Epoch 9 | Batch 49 | Loss: 0.16864803433418274\n",
      "Epoch 9 | Batch 50 | Loss: 0.4687349796295166\n",
      "Epoch 9 | Batch 51 | Loss: 0.10250665992498398\n",
      "Epoch 9 | Batch 52 | Loss: 0.12313336133956909\n",
      "Epoch 9 | Batch 53 | Loss: 0.24240675568580627\n",
      "Epoch 9 | Batch 54 | Loss: 0.6610512733459473\n",
      "Epoch 9 | Batch 55 | Loss: 0.5171928405761719\n",
      "Epoch 9 | Batch 56 | Loss: 0.23701079189777374\n",
      "Epoch 9 | Batch 57 | Loss: 0.42134368419647217\n",
      "Epoch 9 | Batch 58 | Loss: 0.40612030029296875\n",
      "Epoch 9 | Batch 59 | Loss: 0.6593680381774902\n",
      "Epoch 9 | Batch 60 | Loss: 0.5537736415863037\n",
      "Epoch 9 | Batch 61 | Loss: 0.7669551372528076\n",
      "Epoch 9 | Batch 62 | Loss: 0.2818952202796936\n",
      "Epoch 9 | Batch 63 | Loss: 0.617057204246521\n",
      "Epoch 9 | Batch 64 | Loss: 0.2551570534706116\n",
      "Epoch 9 | Batch 65 | Loss: 0.37723034620285034\n",
      "Epoch 9 | Batch 66 | Loss: 0.6834689378738403\n",
      "Epoch 9 | Batch 67 | Loss: 0.2626451253890991\n",
      "Epoch 9 | Batch 68 | Loss: 0.32489413022994995\n",
      "Epoch 9 | Batch 69 | Loss: 0.2595241963863373\n",
      "Epoch 9 | Batch 70 | Loss: 0.26477882266044617\n",
      "Epoch 9 | Batch 71 | Loss: 0.17636056244373322\n",
      "Epoch 9 | Batch 72 | Loss: 0.4620266854763031\n",
      "Epoch 9 | Batch 73 | Loss: 0.3262487053871155\n",
      "Epoch 9 | Batch 74 | Loss: 0.2678392827510834\n",
      "Epoch 9 | Batch 75 | Loss: 0.3857492506504059\n",
      "Epoch 9 | Batch 76 | Loss: 0.7216912508010864\n",
      "Epoch 9 | Batch 77 | Loss: 0.2213662713766098\n",
      "Epoch 9 | Batch 78 | Loss: 0.2565445303916931\n",
      "Epoch 9 | Batch 79 | Loss: 0.4972858726978302\n",
      "Epoch 9 | Batch 80 | Loss: 0.6441507339477539\n",
      "Epoch 9 | Batch 81 | Loss: 0.41801363229751587\n",
      "Epoch 9 | Batch 82 | Loss: 0.4669691026210785\n",
      "Epoch 9 | Batch 83 | Loss: 0.2864096462726593\n",
      "Epoch 9 | Batch 84 | Loss: 0.5862903594970703\n",
      "Epoch 9 | Batch 85 | Loss: 0.3637682795524597\n",
      "Epoch 9 | Batch 86 | Loss: 0.15898637473583221\n",
      "Epoch 9 | Batch 87 | Loss: 0.34737229347229004\n",
      "Epoch 9 | Batch 88 | Loss: 0.11935798078775406\n",
      "Epoch 9 | Batch 89 | Loss: 0.3047950267791748\n",
      "Epoch 9 | Batch 90 | Loss: 0.01588684692978859\n",
      "Epoch 10 | Batch 1 | Loss: 0.19987252354621887\n",
      "Epoch 10 | Batch 2 | Loss: 1.0232609510421753\n",
      "Epoch 10 | Batch 3 | Loss: 0.45362940430641174\n",
      "Epoch 10 | Batch 4 | Loss: 0.228093683719635\n",
      "Epoch 10 | Batch 5 | Loss: 0.22176983952522278\n",
      "Epoch 10 | Batch 6 | Loss: 0.5887022018432617\n",
      "Epoch 10 | Batch 7 | Loss: 0.4576316177845001\n",
      "Epoch 10 | Batch 8 | Loss: 0.6119484901428223\n",
      "Epoch 10 | Batch 9 | Loss: 0.6418763995170593\n",
      "Epoch 10 | Batch 10 | Loss: 0.3675645887851715\n",
      "Epoch 10 | Batch 11 | Loss: 0.7047921419143677\n",
      "Epoch 10 | Batch 12 | Loss: 0.10652248561382294\n",
      "Epoch 10 | Batch 13 | Loss: 0.22050368785858154\n",
      "Epoch 10 | Batch 14 | Loss: 0.564566433429718\n",
      "Epoch 10 | Batch 15 | Loss: 0.4324185848236084\n",
      "Epoch 10 | Batch 16 | Loss: 0.10021127015352249\n",
      "Epoch 10 | Batch 17 | Loss: 0.4537805914878845\n",
      "Epoch 10 | Batch 18 | Loss: 0.3967392146587372\n",
      "Epoch 10 | Batch 19 | Loss: 0.13001927733421326\n",
      "Epoch 10 | Batch 20 | Loss: 0.5001837611198425\n",
      "Epoch 10 | Batch 21 | Loss: 0.2895320951938629\n",
      "Epoch 10 | Batch 22 | Loss: 0.16289234161376953\n",
      "Epoch 10 | Batch 23 | Loss: 0.3728206157684326\n",
      "Epoch 10 | Batch 24 | Loss: 0.0957280844449997\n",
      "Epoch 10 | Batch 25 | Loss: 0.32589519023895264\n",
      "Epoch 10 | Batch 26 | Loss: 0.37873056530952454\n",
      "Epoch 10 | Batch 27 | Loss: 0.2566261887550354\n",
      "Epoch 10 | Batch 28 | Loss: 0.3943110704421997\n",
      "Epoch 10 | Batch 29 | Loss: 0.11438696086406708\n",
      "Epoch 10 | Batch 30 | Loss: 0.16217327117919922\n",
      "Epoch 10 | Batch 31 | Loss: 0.3289702236652374\n",
      "Epoch 10 | Batch 32 | Loss: 0.22678092122077942\n",
      "Epoch 10 | Batch 33 | Loss: 0.5392829775810242\n",
      "Epoch 10 | Batch 34 | Loss: 0.3593316078186035\n",
      "Epoch 10 | Batch 35 | Loss: 0.9373363852500916\n",
      "Epoch 10 | Batch 36 | Loss: 0.3425499200820923\n",
      "Epoch 10 | Batch 37 | Loss: 0.25437837839126587\n",
      "Epoch 10 | Batch 38 | Loss: 0.6740061044692993\n",
      "Epoch 10 | Batch 39 | Loss: 0.21996577084064484\n",
      "Epoch 10 | Batch 40 | Loss: 0.5110903978347778\n",
      "Epoch 10 | Batch 41 | Loss: 0.08060376346111298\n",
      "Epoch 10 | Batch 42 | Loss: 0.796682596206665\n",
      "Epoch 10 | Batch 43 | Loss: 0.5821461081504822\n",
      "Epoch 10 | Batch 44 | Loss: 0.26545947790145874\n",
      "Epoch 10 | Batch 45 | Loss: 0.6311102509498596\n",
      "Epoch 10 | Batch 46 | Loss: 0.11467386782169342\n",
      "Epoch 10 | Batch 47 | Loss: 0.5239377021789551\n",
      "Epoch 10 | Batch 48 | Loss: 0.3512874245643616\n",
      "Epoch 10 | Batch 49 | Loss: 0.44912195205688477\n",
      "Epoch 10 | Batch 50 | Loss: 0.47784972190856934\n",
      "Epoch 10 | Batch 51 | Loss: 0.41780900955200195\n",
      "Epoch 10 | Batch 52 | Loss: 0.14497072994709015\n",
      "Epoch 10 | Batch 53 | Loss: 0.3941071331501007\n",
      "Epoch 10 | Batch 54 | Loss: 0.35823869705200195\n",
      "Epoch 10 | Batch 55 | Loss: 0.2697380483150482\n",
      "Epoch 10 | Batch 56 | Loss: 0.5798138976097107\n",
      "Epoch 10 | Batch 57 | Loss: 0.38665080070495605\n",
      "Epoch 10 | Batch 58 | Loss: 0.6435583829879761\n",
      "Epoch 10 | Batch 59 | Loss: 0.4210030734539032\n",
      "Epoch 10 | Batch 60 | Loss: 0.3361302614212036\n",
      "Epoch 10 | Batch 61 | Loss: 0.20674078166484833\n",
      "Epoch 10 | Batch 62 | Loss: 0.9409380555152893\n",
      "Epoch 10 | Batch 63 | Loss: 0.4291776418685913\n",
      "Epoch 10 | Batch 64 | Loss: 0.6137192249298096\n",
      "Epoch 10 | Batch 65 | Loss: 0.18035240471363068\n",
      "Epoch 10 | Batch 66 | Loss: 0.18460017442703247\n",
      "Epoch 10 | Batch 67 | Loss: 0.14653280377388\n",
      "Epoch 10 | Batch 68 | Loss: 0.5433394908905029\n",
      "Epoch 10 | Batch 69 | Loss: 0.15933571755886078\n",
      "Epoch 10 | Batch 70 | Loss: 0.17438435554504395\n",
      "Epoch 10 | Batch 71 | Loss: 0.38442519307136536\n",
      "Epoch 10 | Batch 72 | Loss: 0.9921188354492188\n",
      "Epoch 10 | Batch 73 | Loss: 0.6568730473518372\n",
      "Epoch 10 | Batch 74 | Loss: 0.24349988996982574\n",
      "Epoch 10 | Batch 75 | Loss: 0.6643638610839844\n",
      "Epoch 10 | Batch 76 | Loss: 0.5310556292533875\n",
      "Epoch 10 | Batch 77 | Loss: 0.1705119013786316\n",
      "Epoch 10 | Batch 78 | Loss: 0.45793986320495605\n",
      "Epoch 10 | Batch 79 | Loss: 0.1632656306028366\n",
      "Epoch 10 | Batch 80 | Loss: 0.13710305094718933\n",
      "Epoch 10 | Batch 81 | Loss: 0.10835465788841248\n",
      "Epoch 10 | Batch 82 | Loss: 0.5927565097808838\n",
      "Epoch 10 | Batch 83 | Loss: 0.3275671899318695\n",
      "Epoch 10 | Batch 84 | Loss: 0.12341918796300888\n",
      "Epoch 10 | Batch 85 | Loss: 0.37260761857032776\n",
      "Epoch 10 | Batch 86 | Loss: 0.24546578526496887\n",
      "Epoch 10 | Batch 87 | Loss: 0.20835164189338684\n",
      "Epoch 10 | Batch 88 | Loss: 0.29716479778289795\n",
      "Epoch 10 | Batch 89 | Loss: 0.2816796600818634\n",
      "Epoch 10 | Batch 90 | Loss: 0.0011911330511793494\n",
      "Epoch 11 | Batch 1 | Loss: 0.4810335636138916\n",
      "Epoch 11 | Batch 2 | Loss: 0.2149062156677246\n",
      "Epoch 11 | Batch 3 | Loss: 0.14038240909576416\n",
      "Epoch 11 | Batch 4 | Loss: 0.133755624294281\n",
      "Epoch 11 | Batch 5 | Loss: 0.2346934676170349\n",
      "Epoch 11 | Batch 6 | Loss: 0.33628425002098083\n",
      "Epoch 11 | Batch 7 | Loss: 0.24072641134262085\n",
      "Epoch 11 | Batch 8 | Loss: 0.428500235080719\n",
      "Epoch 11 | Batch 9 | Loss: 0.47760826349258423\n",
      "Epoch 11 | Batch 10 | Loss: 0.23428139090538025\n",
      "Epoch 11 | Batch 11 | Loss: 0.319911926984787\n",
      "Epoch 11 | Batch 12 | Loss: 0.2079680860042572\n",
      "Epoch 11 | Batch 13 | Loss: 0.8673855662345886\n",
      "Epoch 11 | Batch 14 | Loss: 0.5165489912033081\n",
      "Epoch 11 | Batch 15 | Loss: 0.24477064609527588\n",
      "Epoch 11 | Batch 16 | Loss: 0.5608553290367126\n",
      "Epoch 11 | Batch 17 | Loss: 0.32089194655418396\n",
      "Epoch 11 | Batch 18 | Loss: 0.355791300535202\n",
      "Epoch 11 | Batch 19 | Loss: 0.2950406074523926\n",
      "Epoch 11 | Batch 20 | Loss: 0.4476303458213806\n",
      "Epoch 11 | Batch 21 | Loss: 0.75987309217453\n",
      "Epoch 11 | Batch 22 | Loss: 0.6251418590545654\n",
      "Epoch 11 | Batch 23 | Loss: 0.22387099266052246\n",
      "Epoch 11 | Batch 24 | Loss: 1.2494432926177979\n",
      "Epoch 11 | Batch 25 | Loss: 0.2829904854297638\n",
      "Epoch 11 | Batch 26 | Loss: 0.4638732075691223\n",
      "Epoch 11 | Batch 27 | Loss: 0.12025855481624603\n",
      "Epoch 11 | Batch 28 | Loss: 0.15429282188415527\n",
      "Epoch 11 | Batch 29 | Loss: 0.39723140001296997\n",
      "Epoch 11 | Batch 30 | Loss: 0.36849862337112427\n",
      "Epoch 11 | Batch 31 | Loss: 0.08410896360874176\n",
      "Epoch 11 | Batch 32 | Loss: 0.48113179206848145\n",
      "Epoch 11 | Batch 33 | Loss: 0.251890629529953\n",
      "Epoch 11 | Batch 34 | Loss: 0.39250269532203674\n",
      "Epoch 11 | Batch 35 | Loss: 0.4513523578643799\n",
      "Epoch 11 | Batch 36 | Loss: 0.11647726595401764\n",
      "Epoch 11 | Batch 37 | Loss: 0.2936159670352936\n",
      "Epoch 11 | Batch 38 | Loss: 0.1950617879629135\n",
      "Epoch 11 | Batch 39 | Loss: 0.2955320179462433\n",
      "Epoch 11 | Batch 40 | Loss: 0.2593957185745239\n",
      "Epoch 11 | Batch 41 | Loss: 0.22550539672374725\n",
      "Epoch 11 | Batch 42 | Loss: 0.6524008512496948\n",
      "Epoch 11 | Batch 43 | Loss: 0.16282933950424194\n",
      "Epoch 11 | Batch 44 | Loss: 0.6391980051994324\n",
      "Epoch 11 | Batch 45 | Loss: 0.256748229265213\n",
      "Epoch 11 | Batch 46 | Loss: 0.41272208094596863\n",
      "Epoch 11 | Batch 47 | Loss: 0.14147166907787323\n",
      "Epoch 11 | Batch 48 | Loss: 0.7490481734275818\n",
      "Epoch 11 | Batch 49 | Loss: 0.40016117691993713\n",
      "Epoch 11 | Batch 50 | Loss: 0.4364584684371948\n",
      "Epoch 11 | Batch 51 | Loss: 0.9037784337997437\n",
      "Epoch 11 | Batch 52 | Loss: 0.09670019149780273\n",
      "Epoch 11 | Batch 53 | Loss: 0.2316620647907257\n",
      "Epoch 11 | Batch 54 | Loss: 0.21170979738235474\n",
      "Epoch 11 | Batch 55 | Loss: 0.7624954581260681\n",
      "Epoch 11 | Batch 56 | Loss: 0.14946766197681427\n",
      "Epoch 11 | Batch 57 | Loss: 0.7071946859359741\n",
      "Epoch 11 | Batch 58 | Loss: 0.3315957188606262\n",
      "Epoch 11 | Batch 59 | Loss: 0.878685474395752\n",
      "Epoch 11 | Batch 60 | Loss: 0.6203235983848572\n",
      "Epoch 11 | Batch 61 | Loss: 0.20270299911499023\n",
      "Epoch 11 | Batch 62 | Loss: 0.6670262217521667\n",
      "Epoch 11 | Batch 63 | Loss: 0.2934829890727997\n",
      "Epoch 11 | Batch 64 | Loss: 0.5382460355758667\n",
      "Epoch 11 | Batch 65 | Loss: 0.18366611003875732\n",
      "Epoch 11 | Batch 66 | Loss: 0.1671827882528305\n",
      "Epoch 11 | Batch 67 | Loss: 0.3068802058696747\n",
      "Epoch 11 | Batch 68 | Loss: 0.2860444188117981\n",
      "Epoch 11 | Batch 69 | Loss: 0.2034156620502472\n",
      "Epoch 11 | Batch 70 | Loss: 0.2946334481239319\n",
      "Epoch 11 | Batch 71 | Loss: 0.4679495692253113\n",
      "Epoch 11 | Batch 72 | Loss: 0.5489428043365479\n",
      "Epoch 11 | Batch 73 | Loss: 0.3977130651473999\n",
      "Epoch 11 | Batch 74 | Loss: 0.1534775197505951\n",
      "Epoch 11 | Batch 75 | Loss: 0.5000424981117249\n",
      "Epoch 11 | Batch 76 | Loss: 0.7502924799919128\n",
      "Epoch 11 | Batch 77 | Loss: 0.26746809482574463\n",
      "Epoch 11 | Batch 78 | Loss: 0.44629308581352234\n",
      "Epoch 11 | Batch 79 | Loss: 0.46483489871025085\n",
      "Epoch 11 | Batch 80 | Loss: 0.2743340730667114\n",
      "Epoch 11 | Batch 81 | Loss: 0.4051474332809448\n",
      "Epoch 11 | Batch 82 | Loss: 0.10551755130290985\n",
      "Epoch 11 | Batch 83 | Loss: 0.20598140358924866\n",
      "Epoch 11 | Batch 84 | Loss: 0.2276059091091156\n",
      "Epoch 11 | Batch 85 | Loss: 0.2655353844165802\n",
      "Epoch 11 | Batch 86 | Loss: 0.368627667427063\n",
      "Epoch 11 | Batch 87 | Loss: 0.31515559554100037\n",
      "Epoch 11 | Batch 88 | Loss: 0.4959719777107239\n",
      "Epoch 11 | Batch 89 | Loss: 0.4127606749534607\n",
      "Epoch 11 | Batch 90 | Loss: 0.14006862044334412\n",
      "Epoch 12 | Batch 1 | Loss: 0.27000266313552856\n",
      "Epoch 12 | Batch 2 | Loss: 0.37197399139404297\n",
      "Epoch 12 | Batch 3 | Loss: 0.30193132162094116\n",
      "Epoch 12 | Batch 4 | Loss: 0.6182488203048706\n",
      "Epoch 12 | Batch 5 | Loss: 0.6766796112060547\n",
      "Epoch 12 | Batch 6 | Loss: 0.34604814648628235\n",
      "Epoch 12 | Batch 7 | Loss: 0.42975401878356934\n",
      "Epoch 12 | Batch 8 | Loss: 0.08184431493282318\n",
      "Epoch 12 | Batch 9 | Loss: 0.20741289854049683\n",
      "Epoch 12 | Batch 10 | Loss: 0.07425098121166229\n",
      "Epoch 12 | Batch 11 | Loss: 0.30388033390045166\n",
      "Epoch 12 | Batch 12 | Loss: 0.37743836641311646\n",
      "Epoch 12 | Batch 13 | Loss: 0.22005963325500488\n",
      "Epoch 12 | Batch 14 | Loss: 0.23379918932914734\n",
      "Epoch 12 | Batch 15 | Loss: 0.7443677186965942\n",
      "Epoch 12 | Batch 16 | Loss: 0.40267717838287354\n",
      "Epoch 12 | Batch 17 | Loss: 0.43001165986061096\n",
      "Epoch 12 | Batch 18 | Loss: 0.3588000237941742\n",
      "Epoch 12 | Batch 19 | Loss: 0.1721489429473877\n",
      "Epoch 12 | Batch 20 | Loss: 0.5059691667556763\n",
      "Epoch 12 | Batch 21 | Loss: 0.1054774820804596\n",
      "Epoch 12 | Batch 22 | Loss: 0.4562739133834839\n",
      "Epoch 12 | Batch 23 | Loss: 0.2782810628414154\n",
      "Epoch 12 | Batch 24 | Loss: 0.5662781000137329\n",
      "Epoch 12 | Batch 25 | Loss: 0.2620663642883301\n",
      "Epoch 12 | Batch 26 | Loss: 0.24891024827957153\n",
      "Epoch 12 | Batch 27 | Loss: 0.7374932765960693\n",
      "Epoch 12 | Batch 28 | Loss: 0.5697934627532959\n",
      "Epoch 12 | Batch 29 | Loss: 0.3824312686920166\n",
      "Epoch 12 | Batch 30 | Loss: 0.12025578320026398\n",
      "Epoch 12 | Batch 31 | Loss: 0.20587459206581116\n",
      "Epoch 12 | Batch 32 | Loss: 0.41904380917549133\n",
      "Epoch 12 | Batch 33 | Loss: 0.24544653296470642\n",
      "Epoch 12 | Batch 34 | Loss: 0.22999370098114014\n",
      "Epoch 12 | Batch 35 | Loss: 0.7738528847694397\n",
      "Epoch 12 | Batch 36 | Loss: 0.19750042259693146\n",
      "Epoch 12 | Batch 37 | Loss: 0.5985463857650757\n",
      "Epoch 12 | Batch 38 | Loss: 0.6278493404388428\n",
      "Epoch 12 | Batch 39 | Loss: 0.31865909695625305\n",
      "Epoch 12 | Batch 40 | Loss: 0.17617082595825195\n",
      "Epoch 12 | Batch 41 | Loss: 0.6835222840309143\n",
      "Epoch 12 | Batch 42 | Loss: 0.4655202329158783\n",
      "Epoch 12 | Batch 43 | Loss: 0.3975178003311157\n",
      "Epoch 12 | Batch 44 | Loss: 0.17945775389671326\n",
      "Epoch 12 | Batch 45 | Loss: 0.405846506357193\n",
      "Epoch 12 | Batch 46 | Loss: 0.26697006821632385\n",
      "Epoch 12 | Batch 47 | Loss: 0.698859453201294\n",
      "Epoch 12 | Batch 48 | Loss: 0.34937459230422974\n",
      "Epoch 12 | Batch 49 | Loss: 0.5804152488708496\n",
      "Epoch 12 | Batch 50 | Loss: 0.16745862364768982\n",
      "Epoch 12 | Batch 51 | Loss: 0.49968212842941284\n",
      "Epoch 12 | Batch 52 | Loss: 0.3676137328147888\n",
      "Epoch 12 | Batch 53 | Loss: 0.22976809740066528\n",
      "Epoch 12 | Batch 54 | Loss: 0.42451584339141846\n",
      "Epoch 12 | Batch 55 | Loss: 0.6953007578849792\n",
      "Epoch 12 | Batch 56 | Loss: 0.16915442049503326\n",
      "Epoch 12 | Batch 57 | Loss: 0.4844599962234497\n",
      "Epoch 12 | Batch 58 | Loss: 0.4286513924598694\n",
      "Epoch 12 | Batch 59 | Loss: 0.1432960480451584\n",
      "Epoch 12 | Batch 60 | Loss: 0.2449495941400528\n",
      "Epoch 12 | Batch 61 | Loss: 0.17855723202228546\n",
      "Epoch 12 | Batch 62 | Loss: 0.2836046814918518\n",
      "Epoch 12 | Batch 63 | Loss: 0.49768146872520447\n",
      "Epoch 12 | Batch 64 | Loss: 0.1461765170097351\n",
      "Epoch 12 | Batch 65 | Loss: 0.5412799715995789\n",
      "Epoch 12 | Batch 66 | Loss: 0.17767231166362762\n",
      "Epoch 12 | Batch 67 | Loss: 0.822008490562439\n",
      "Epoch 12 | Batch 68 | Loss: 0.3243789076805115\n",
      "Epoch 12 | Batch 69 | Loss: 0.3339967429637909\n",
      "Epoch 12 | Batch 70 | Loss: 0.31578630208969116\n",
      "Epoch 12 | Batch 71 | Loss: 0.7668333053588867\n",
      "Epoch 12 | Batch 72 | Loss: 0.45514512062072754\n",
      "Epoch 12 | Batch 73 | Loss: 0.1265546679496765\n",
      "Epoch 12 | Batch 74 | Loss: 0.34788304567337036\n",
      "Epoch 12 | Batch 75 | Loss: 0.3024289011955261\n",
      "Epoch 12 | Batch 76 | Loss: 0.8358420729637146\n",
      "Epoch 12 | Batch 77 | Loss: 0.1997031420469284\n",
      "Epoch 12 | Batch 78 | Loss: 0.24617446959018707\n",
      "Epoch 12 | Batch 79 | Loss: 0.29705578088760376\n",
      "Epoch 12 | Batch 80 | Loss: 0.40985557436943054\n",
      "Epoch 12 | Batch 81 | Loss: 0.19373127818107605\n",
      "Epoch 12 | Batch 82 | Loss: 0.1569921374320984\n",
      "Epoch 12 | Batch 83 | Loss: 0.739109992980957\n",
      "Epoch 12 | Batch 84 | Loss: 0.5488777160644531\n",
      "Epoch 12 | Batch 85 | Loss: 0.28902468085289\n",
      "Epoch 12 | Batch 86 | Loss: 0.40189653635025024\n",
      "Epoch 12 | Batch 87 | Loss: 0.5087701082229614\n",
      "Epoch 12 | Batch 88 | Loss: 0.24215830862522125\n",
      "Epoch 12 | Batch 89 | Loss: 0.24116528034210205\n",
      "Epoch 12 | Batch 90 | Loss: 0.10151413828134537\n",
      "Epoch 13 | Batch 1 | Loss: 0.256987988948822\n",
      "Epoch 13 | Batch 2 | Loss: 0.24866260588169098\n",
      "Epoch 13 | Batch 3 | Loss: 0.5393906831741333\n",
      "Epoch 13 | Batch 4 | Loss: 0.4035237431526184\n",
      "Epoch 13 | Batch 5 | Loss: 0.4981074035167694\n",
      "Epoch 13 | Batch 6 | Loss: 0.28759557008743286\n",
      "Epoch 13 | Batch 7 | Loss: 0.3510601818561554\n",
      "Epoch 13 | Batch 8 | Loss: 0.6915987730026245\n",
      "Epoch 13 | Batch 9 | Loss: 0.6448325514793396\n",
      "Epoch 13 | Batch 10 | Loss: 0.4228270351886749\n",
      "Epoch 13 | Batch 11 | Loss: 0.17563477158546448\n",
      "Epoch 13 | Batch 12 | Loss: 0.376880019903183\n",
      "Epoch 13 | Batch 13 | Loss: 0.19063520431518555\n",
      "Epoch 13 | Batch 14 | Loss: 0.18280191719532013\n",
      "Epoch 13 | Batch 15 | Loss: 0.3105020821094513\n",
      "Epoch 13 | Batch 16 | Loss: 0.24362055957317352\n",
      "Epoch 13 | Batch 17 | Loss: 0.06822434067726135\n",
      "Epoch 13 | Batch 18 | Loss: 0.847878098487854\n",
      "Epoch 13 | Batch 19 | Loss: 0.2943885624408722\n",
      "Epoch 13 | Batch 20 | Loss: 0.2991812527179718\n",
      "Epoch 13 | Batch 21 | Loss: 0.3549419939517975\n",
      "Epoch 13 | Batch 22 | Loss: 0.5030369758605957\n",
      "Epoch 13 | Batch 23 | Loss: 0.68105149269104\n",
      "Epoch 13 | Batch 24 | Loss: 0.36889776587486267\n",
      "Epoch 13 | Batch 25 | Loss: 0.28699126839637756\n",
      "Epoch 13 | Batch 26 | Loss: 0.356557697057724\n",
      "Epoch 13 | Batch 27 | Loss: 0.18321676552295685\n",
      "Epoch 13 | Batch 28 | Loss: 0.656549870967865\n",
      "Epoch 13 | Batch 29 | Loss: 0.1864410936832428\n",
      "Epoch 13 | Batch 30 | Loss: 0.7834698557853699\n",
      "Epoch 13 | Batch 31 | Loss: 0.2644524872303009\n",
      "Epoch 13 | Batch 32 | Loss: 0.1307397186756134\n",
      "Epoch 13 | Batch 33 | Loss: 0.11267372220754623\n",
      "Epoch 13 | Batch 34 | Loss: 0.5702424645423889\n",
      "Epoch 13 | Batch 35 | Loss: 0.3378315567970276\n",
      "Epoch 13 | Batch 36 | Loss: 0.16344588994979858\n",
      "Epoch 13 | Batch 37 | Loss: 0.4517034888267517\n",
      "Epoch 13 | Batch 38 | Loss: 0.556197464466095\n",
      "Epoch 13 | Batch 39 | Loss: 0.4214313328266144\n",
      "Epoch 13 | Batch 40 | Loss: 0.5025381445884705\n",
      "Epoch 13 | Batch 41 | Loss: 0.3011706471443176\n",
      "Epoch 13 | Batch 42 | Loss: 0.5392290949821472\n",
      "Epoch 13 | Batch 43 | Loss: 0.4467020034790039\n",
      "Epoch 13 | Batch 44 | Loss: 0.3362065255641937\n",
      "Epoch 13 | Batch 45 | Loss: 0.31107228994369507\n",
      "Epoch 13 | Batch 46 | Loss: 0.8325010538101196\n",
      "Epoch 13 | Batch 47 | Loss: 0.24826845526695251\n",
      "Epoch 13 | Batch 48 | Loss: 0.29589152336120605\n",
      "Epoch 13 | Batch 49 | Loss: 0.16001096367835999\n",
      "Epoch 13 | Batch 50 | Loss: 0.2839387059211731\n",
      "Epoch 13 | Batch 51 | Loss: 0.36071205139160156\n",
      "Epoch 13 | Batch 52 | Loss: 0.7807541489601135\n",
      "Epoch 13 | Batch 53 | Loss: 0.6270620822906494\n",
      "Epoch 13 | Batch 54 | Loss: 0.5594748854637146\n",
      "Epoch 13 | Batch 55 | Loss: 0.21165713667869568\n",
      "Epoch 13 | Batch 56 | Loss: 0.41701260209083557\n",
      "Epoch 13 | Batch 57 | Loss: 0.677820086479187\n",
      "Epoch 13 | Batch 58 | Loss: 0.2001558542251587\n",
      "Epoch 13 | Batch 59 | Loss: 0.3798888921737671\n",
      "Epoch 13 | Batch 60 | Loss: 0.5067024230957031\n",
      "Epoch 13 | Batch 61 | Loss: 0.21897272765636444\n",
      "Epoch 13 | Batch 62 | Loss: 0.24573178589344025\n",
      "Epoch 13 | Batch 63 | Loss: 0.2360841929912567\n",
      "Epoch 13 | Batch 64 | Loss: 0.20821109414100647\n",
      "Epoch 13 | Batch 65 | Loss: 0.3486565351486206\n",
      "Epoch 13 | Batch 66 | Loss: 0.5473677515983582\n",
      "Epoch 13 | Batch 67 | Loss: 0.23414303362369537\n",
      "Epoch 13 | Batch 68 | Loss: 0.5624100565910339\n",
      "Epoch 13 | Batch 69 | Loss: 0.2594996988773346\n",
      "Epoch 13 | Batch 70 | Loss: 0.3892669081687927\n",
      "Epoch 13 | Batch 71 | Loss: 0.6123857498168945\n",
      "Epoch 13 | Batch 72 | Loss: 0.32342591881752014\n",
      "Epoch 13 | Batch 73 | Loss: 0.2094598412513733\n",
      "Epoch 13 | Batch 74 | Loss: 0.19695810973644257\n",
      "Epoch 13 | Batch 75 | Loss: 0.11717960983514786\n",
      "Epoch 13 | Batch 76 | Loss: 0.4029478430747986\n",
      "Epoch 13 | Batch 77 | Loss: 0.5713783502578735\n",
      "Epoch 13 | Batch 78 | Loss: 0.4933321475982666\n",
      "Epoch 13 | Batch 79 | Loss: 0.2948232591152191\n",
      "Epoch 13 | Batch 80 | Loss: 0.7417349219322205\n",
      "Epoch 13 | Batch 81 | Loss: 0.44788500666618347\n",
      "Epoch 13 | Batch 82 | Loss: 0.41851139068603516\n",
      "Epoch 13 | Batch 83 | Loss: 0.20002788305282593\n",
      "Epoch 13 | Batch 84 | Loss: 0.3076289892196655\n",
      "Epoch 13 | Batch 85 | Loss: 0.3616389036178589\n",
      "Epoch 13 | Batch 86 | Loss: 0.12089958786964417\n",
      "Epoch 13 | Batch 87 | Loss: 0.19688807427883148\n",
      "Epoch 13 | Batch 88 | Loss: 0.1603451669216156\n",
      "Epoch 13 | Batch 89 | Loss: 0.4965796172618866\n",
      "Epoch 13 | Batch 90 | Loss: 0.6232865452766418\n",
      "Epoch 14 | Batch 1 | Loss: 0.5353511571884155\n",
      "Epoch 14 | Batch 2 | Loss: 0.5429771542549133\n",
      "Epoch 14 | Batch 3 | Loss: 0.9139739871025085\n",
      "Epoch 14 | Batch 4 | Loss: 0.3173410892486572\n",
      "Epoch 14 | Batch 5 | Loss: 0.42971017956733704\n",
      "Epoch 14 | Batch 6 | Loss: 0.36211279034614563\n",
      "Epoch 14 | Batch 7 | Loss: 0.32296496629714966\n",
      "Epoch 14 | Batch 8 | Loss: 0.43321749567985535\n",
      "Epoch 14 | Batch 9 | Loss: 0.8525494337081909\n",
      "Epoch 14 | Batch 10 | Loss: 0.2507963180541992\n",
      "Epoch 14 | Batch 11 | Loss: 0.3296787142753601\n",
      "Epoch 14 | Batch 12 | Loss: 0.3744547963142395\n",
      "Epoch 14 | Batch 13 | Loss: 0.2578583359718323\n",
      "Epoch 14 | Batch 14 | Loss: 0.4989553689956665\n",
      "Epoch 14 | Batch 15 | Loss: 0.08237835764884949\n",
      "Epoch 14 | Batch 16 | Loss: 0.1045132428407669\n",
      "Epoch 14 | Batch 17 | Loss: 0.4366052746772766\n",
      "Epoch 14 | Batch 18 | Loss: 0.8644272685050964\n",
      "Epoch 14 | Batch 19 | Loss: 0.32949504256248474\n",
      "Epoch 14 | Batch 20 | Loss: 0.6491039991378784\n",
      "Epoch 14 | Batch 21 | Loss: 0.2527375817298889\n",
      "Epoch 14 | Batch 22 | Loss: 0.4964865744113922\n",
      "Epoch 14 | Batch 23 | Loss: 0.5328587889671326\n",
      "Epoch 14 | Batch 24 | Loss: 0.5635831952095032\n",
      "Epoch 14 | Batch 25 | Loss: 0.2633536458015442\n",
      "Epoch 14 | Batch 26 | Loss: 0.5959556102752686\n",
      "Epoch 14 | Batch 27 | Loss: 0.11307303607463837\n",
      "Epoch 14 | Batch 28 | Loss: 0.335846483707428\n",
      "Epoch 14 | Batch 29 | Loss: 0.20528972148895264\n",
      "Epoch 14 | Batch 30 | Loss: 0.42444688081741333\n",
      "Epoch 14 | Batch 31 | Loss: 0.5238862037658691\n",
      "Epoch 14 | Batch 32 | Loss: 0.13569310307502747\n",
      "Epoch 14 | Batch 33 | Loss: 0.3512880802154541\n",
      "Epoch 14 | Batch 34 | Loss: 0.28215521574020386\n",
      "Epoch 14 | Batch 35 | Loss: 0.18464788794517517\n",
      "Epoch 14 | Batch 36 | Loss: 0.23144565522670746\n",
      "Epoch 14 | Batch 37 | Loss: 0.40534457564353943\n",
      "Epoch 14 | Batch 38 | Loss: 0.12017360329627991\n",
      "Epoch 14 | Batch 39 | Loss: 0.5623538494110107\n",
      "Epoch 14 | Batch 40 | Loss: 0.3243582546710968\n",
      "Epoch 14 | Batch 41 | Loss: 0.4487576484680176\n",
      "Epoch 14 | Batch 42 | Loss: 0.6027559041976929\n",
      "Epoch 14 | Batch 43 | Loss: 0.2885141670703888\n",
      "Epoch 14 | Batch 44 | Loss: 0.11975030601024628\n",
      "Epoch 14 | Batch 45 | Loss: 0.3058379888534546\n",
      "Epoch 14 | Batch 46 | Loss: 0.5363290309906006\n",
      "Epoch 14 | Batch 47 | Loss: 0.29205796122550964\n",
      "Epoch 14 | Batch 48 | Loss: 0.19989944994449615\n",
      "Epoch 14 | Batch 49 | Loss: 0.6172980070114136\n",
      "Epoch 14 | Batch 50 | Loss: 0.17740842700004578\n",
      "Epoch 14 | Batch 51 | Loss: 0.5716627836227417\n",
      "Epoch 14 | Batch 52 | Loss: 0.06493566930294037\n",
      "Epoch 14 | Batch 53 | Loss: 0.5066887140274048\n",
      "Epoch 14 | Batch 54 | Loss: 0.2816368043422699\n",
      "Epoch 14 | Batch 55 | Loss: 0.10299450159072876\n",
      "Epoch 14 | Batch 56 | Loss: 0.5847167372703552\n",
      "Epoch 14 | Batch 57 | Loss: 0.4497169256210327\n",
      "Epoch 14 | Batch 58 | Loss: 0.17886167764663696\n",
      "Epoch 14 | Batch 59 | Loss: 0.24988672137260437\n",
      "Epoch 14 | Batch 60 | Loss: 0.3114217519760132\n",
      "Epoch 14 | Batch 61 | Loss: 0.09260500967502594\n",
      "Epoch 14 | Batch 62 | Loss: 0.24738168716430664\n",
      "Epoch 14 | Batch 63 | Loss: 0.44508787989616394\n",
      "Epoch 14 | Batch 64 | Loss: 1.4582293033599854\n",
      "Epoch 14 | Batch 65 | Loss: 0.1270870417356491\n",
      "Epoch 14 | Batch 66 | Loss: 0.6753940582275391\n",
      "Epoch 14 | Batch 67 | Loss: 0.18840965628623962\n",
      "Epoch 14 | Batch 68 | Loss: 0.39045360684394836\n",
      "Epoch 14 | Batch 69 | Loss: 0.49806898832321167\n",
      "Epoch 14 | Batch 70 | Loss: 0.18284767866134644\n",
      "Epoch 14 | Batch 71 | Loss: 0.23690909147262573\n",
      "Epoch 14 | Batch 72 | Loss: 0.24003687500953674\n",
      "Epoch 14 | Batch 73 | Loss: 0.3301147222518921\n",
      "Epoch 14 | Batch 74 | Loss: 0.8171730041503906\n",
      "Epoch 14 | Batch 75 | Loss: 0.5107641220092773\n",
      "Epoch 14 | Batch 76 | Loss: 0.6758719682693481\n",
      "Epoch 14 | Batch 77 | Loss: 0.4952206313610077\n",
      "Epoch 14 | Batch 78 | Loss: 0.8279056549072266\n",
      "Epoch 14 | Batch 79 | Loss: 0.43040892481803894\n",
      "Epoch 14 | Batch 80 | Loss: 0.23794089257717133\n",
      "Epoch 14 | Batch 81 | Loss: 0.11079107969999313\n",
      "Epoch 14 | Batch 82 | Loss: 0.21719364821910858\n",
      "Epoch 14 | Batch 83 | Loss: 0.288146436214447\n",
      "Epoch 14 | Batch 84 | Loss: 0.31986063718795776\n",
      "Epoch 14 | Batch 85 | Loss: 0.6827889680862427\n",
      "Epoch 14 | Batch 86 | Loss: 0.5207505822181702\n",
      "Epoch 14 | Batch 87 | Loss: 0.3745017349720001\n",
      "Epoch 14 | Batch 88 | Loss: 0.1882943958044052\n",
      "Epoch 14 | Batch 89 | Loss: 0.26787269115448\n",
      "Epoch 14 | Batch 90 | Loss: 0.18382899463176727\n",
      "Epoch 15 | Batch 1 | Loss: 0.15483184158802032\n",
      "Epoch 15 | Batch 2 | Loss: 0.1593359112739563\n",
      "Epoch 15 | Batch 3 | Loss: 0.10976538062095642\n",
      "Epoch 15 | Batch 4 | Loss: 0.46752291917800903\n",
      "Epoch 15 | Batch 5 | Loss: 0.4570921063423157\n",
      "Epoch 15 | Batch 6 | Loss: 0.2972598671913147\n",
      "Epoch 15 | Batch 7 | Loss: 0.4619835913181305\n",
      "Epoch 15 | Batch 8 | Loss: 0.39422476291656494\n",
      "Epoch 15 | Batch 9 | Loss: 0.41112345457077026\n",
      "Epoch 15 | Batch 10 | Loss: 0.2639671266078949\n",
      "Epoch 15 | Batch 11 | Loss: 0.23741742968559265\n",
      "Epoch 15 | Batch 12 | Loss: 0.45848348736763\n",
      "Epoch 15 | Batch 13 | Loss: 0.6322600841522217\n",
      "Epoch 15 | Batch 14 | Loss: 0.24185502529144287\n",
      "Epoch 15 | Batch 15 | Loss: 0.10617658495903015\n",
      "Epoch 15 | Batch 16 | Loss: 0.40184006094932556\n",
      "Epoch 15 | Batch 17 | Loss: 0.4606363773345947\n",
      "Epoch 15 | Batch 18 | Loss: 0.5130968689918518\n",
      "Epoch 15 | Batch 19 | Loss: 0.13274624943733215\n",
      "Epoch 15 | Batch 20 | Loss: 0.7764145135879517\n",
      "Epoch 15 | Batch 21 | Loss: 0.49640384316444397\n",
      "Epoch 15 | Batch 22 | Loss: 0.23762953281402588\n",
      "Epoch 15 | Batch 23 | Loss: 1.0583438873291016\n",
      "Epoch 15 | Batch 24 | Loss: 0.3386138677597046\n",
      "Epoch 15 | Batch 25 | Loss: 0.5171068906784058\n",
      "Epoch 15 | Batch 26 | Loss: 0.8425693511962891\n",
      "Epoch 15 | Batch 27 | Loss: 0.32955580949783325\n",
      "Epoch 15 | Batch 28 | Loss: 0.7215913534164429\n",
      "Epoch 15 | Batch 29 | Loss: 0.5962414741516113\n",
      "Epoch 15 | Batch 30 | Loss: 0.45353075861930847\n",
      "Epoch 15 | Batch 31 | Loss: 0.15294817090034485\n",
      "Epoch 15 | Batch 32 | Loss: 0.323585569858551\n",
      "Epoch 15 | Batch 33 | Loss: 0.31563082337379456\n",
      "Epoch 15 | Batch 34 | Loss: 0.3122771978378296\n",
      "Epoch 15 | Batch 35 | Loss: 0.7183430194854736\n",
      "Epoch 15 | Batch 36 | Loss: 0.11362426728010178\n",
      "Epoch 15 | Batch 37 | Loss: 0.7077928185462952\n",
      "Epoch 15 | Batch 38 | Loss: 0.3161408305168152\n",
      "Epoch 15 | Batch 39 | Loss: 0.37602412700653076\n",
      "Epoch 15 | Batch 40 | Loss: 0.33242204785346985\n",
      "Epoch 15 | Batch 41 | Loss: 0.45265257358551025\n",
      "Epoch 15 | Batch 42 | Loss: 0.16088083386421204\n",
      "Epoch 15 | Batch 43 | Loss: 0.35900622606277466\n",
      "Epoch 15 | Batch 44 | Loss: 0.4441714584827423\n",
      "Epoch 15 | Batch 45 | Loss: 0.4151363968849182\n",
      "Epoch 15 | Batch 46 | Loss: 0.5271861553192139\n",
      "Epoch 15 | Batch 47 | Loss: 0.6060137748718262\n",
      "Epoch 15 | Batch 48 | Loss: 0.20803944766521454\n",
      "Epoch 15 | Batch 49 | Loss: 0.4744362235069275\n",
      "Epoch 15 | Batch 50 | Loss: 0.6009234189987183\n",
      "Epoch 15 | Batch 51 | Loss: 0.2949012219905853\n",
      "Epoch 15 | Batch 52 | Loss: 0.30366674065589905\n",
      "Epoch 15 | Batch 53 | Loss: 0.15469887852668762\n",
      "Epoch 15 | Batch 54 | Loss: 0.1218240037560463\n",
      "Epoch 15 | Batch 55 | Loss: 0.4971315860748291\n",
      "Epoch 15 | Batch 56 | Loss: 0.17696698009967804\n",
      "Epoch 15 | Batch 57 | Loss: 0.1453438550233841\n",
      "Epoch 15 | Batch 58 | Loss: 0.4046013355255127\n",
      "Epoch 15 | Batch 59 | Loss: 0.5053809881210327\n",
      "Epoch 15 | Batch 60 | Loss: 0.6029525995254517\n",
      "Epoch 15 | Batch 61 | Loss: 0.25076529383659363\n",
      "Epoch 15 | Batch 62 | Loss: 0.2735457420349121\n",
      "Epoch 15 | Batch 63 | Loss: 0.4918379783630371\n",
      "Epoch 15 | Batch 64 | Loss: 0.15210473537445068\n",
      "Epoch 15 | Batch 65 | Loss: 0.5396615266799927\n",
      "Epoch 15 | Batch 66 | Loss: 0.6782735586166382\n",
      "Epoch 15 | Batch 67 | Loss: 0.2593029737472534\n",
      "Epoch 15 | Batch 68 | Loss: 0.20452439785003662\n",
      "Epoch 15 | Batch 69 | Loss: 0.07565134763717651\n",
      "Epoch 15 | Batch 70 | Loss: 0.3191681206226349\n",
      "Epoch 15 | Batch 71 | Loss: 0.5585682392120361\n",
      "Epoch 15 | Batch 72 | Loss: 0.2988167107105255\n",
      "Epoch 15 | Batch 73 | Loss: 0.14215314388275146\n",
      "Epoch 15 | Batch 74 | Loss: 0.16831916570663452\n",
      "Epoch 15 | Batch 75 | Loss: 0.501753568649292\n",
      "Epoch 15 | Batch 76 | Loss: 0.23327916860580444\n",
      "Epoch 15 | Batch 77 | Loss: 0.34206390380859375\n",
      "Epoch 15 | Batch 78 | Loss: 0.41956543922424316\n",
      "Epoch 15 | Batch 79 | Loss: 0.193878173828125\n",
      "Epoch 15 | Batch 80 | Loss: 0.3410983085632324\n",
      "Epoch 15 | Batch 81 | Loss: 0.35428836941719055\n",
      "Epoch 15 | Batch 82 | Loss: 0.9218044281005859\n",
      "Epoch 15 | Batch 83 | Loss: 0.07579359412193298\n",
      "Epoch 15 | Batch 84 | Loss: 0.31484994292259216\n",
      "Epoch 15 | Batch 85 | Loss: 0.5425973534584045\n",
      "Epoch 15 | Batch 86 | Loss: 0.2918190360069275\n",
      "Epoch 15 | Batch 87 | Loss: 0.6161319613456726\n",
      "Epoch 15 | Batch 88 | Loss: 0.4030076861381531\n",
      "Epoch 15 | Batch 89 | Loss: 0.3049781620502472\n",
      "Epoch 15 | Batch 90 | Loss: 0.6833631992340088\n",
      "Epoch 16 | Batch 1 | Loss: 0.8372663855552673\n",
      "Epoch 16 | Batch 2 | Loss: 0.5674772262573242\n",
      "Epoch 16 | Batch 3 | Loss: 0.500034749507904\n",
      "Epoch 16 | Batch 4 | Loss: 0.16570228338241577\n",
      "Epoch 16 | Batch 5 | Loss: 0.6332820057868958\n",
      "Epoch 16 | Batch 6 | Loss: 0.8228222131729126\n",
      "Epoch 16 | Batch 7 | Loss: 0.2878328859806061\n",
      "Epoch 16 | Batch 8 | Loss: 0.3311682343482971\n",
      "Epoch 16 | Batch 9 | Loss: 0.1668100357055664\n",
      "Epoch 16 | Batch 10 | Loss: 0.4645344316959381\n",
      "Epoch 16 | Batch 11 | Loss: 0.589759111404419\n",
      "Epoch 16 | Batch 12 | Loss: 0.36154910922050476\n",
      "Epoch 16 | Batch 13 | Loss: 0.09684751182794571\n",
      "Epoch 16 | Batch 14 | Loss: 0.17244264483451843\n",
      "Epoch 16 | Batch 15 | Loss: 0.5218867063522339\n",
      "Epoch 16 | Batch 16 | Loss: 0.17966516315937042\n",
      "Epoch 16 | Batch 17 | Loss: 0.19800904393196106\n",
      "Epoch 16 | Batch 18 | Loss: 0.5130494832992554\n",
      "Epoch 16 | Batch 19 | Loss: 0.1553589403629303\n",
      "Epoch 16 | Batch 20 | Loss: 0.4051424264907837\n",
      "Epoch 16 | Batch 21 | Loss: 0.38637104630470276\n",
      "Epoch 16 | Batch 22 | Loss: 0.2714266777038574\n",
      "Epoch 16 | Batch 23 | Loss: 0.3193240761756897\n",
      "Epoch 16 | Batch 24 | Loss: 0.3727700114250183\n",
      "Epoch 16 | Batch 25 | Loss: 0.7147579193115234\n",
      "Epoch 16 | Batch 26 | Loss: 0.2293803095817566\n",
      "Epoch 16 | Batch 27 | Loss: 0.10002969205379486\n",
      "Epoch 16 | Batch 28 | Loss: 0.5993507504463196\n",
      "Epoch 16 | Batch 29 | Loss: 0.39880216121673584\n",
      "Epoch 16 | Batch 30 | Loss: 0.45707234740257263\n",
      "Epoch 16 | Batch 31 | Loss: 0.19993774592876434\n",
      "Epoch 16 | Batch 32 | Loss: 0.5206669569015503\n",
      "Epoch 16 | Batch 33 | Loss: 0.18319806456565857\n",
      "Epoch 16 | Batch 34 | Loss: 0.5500986576080322\n",
      "Epoch 16 | Batch 35 | Loss: 0.32537466287612915\n",
      "Epoch 16 | Batch 36 | Loss: 0.35870012640953064\n",
      "Epoch 16 | Batch 37 | Loss: 0.23198989033699036\n",
      "Epoch 16 | Batch 38 | Loss: 0.27873075008392334\n",
      "Epoch 16 | Batch 39 | Loss: 0.30056414008140564\n",
      "Epoch 16 | Batch 40 | Loss: 0.40440261363983154\n",
      "Epoch 16 | Batch 41 | Loss: 0.387429416179657\n",
      "Epoch 16 | Batch 42 | Loss: 0.15095259249210358\n",
      "Epoch 16 | Batch 43 | Loss: 0.8386980891227722\n",
      "Epoch 16 | Batch 44 | Loss: 0.20229853689670563\n",
      "Epoch 16 | Batch 45 | Loss: 0.4829876720905304\n",
      "Epoch 16 | Batch 46 | Loss: 0.16656818985939026\n",
      "Epoch 16 | Batch 47 | Loss: 0.36968252062797546\n",
      "Epoch 16 | Batch 48 | Loss: 0.3517126142978668\n",
      "Epoch 16 | Batch 49 | Loss: 0.21882563829421997\n",
      "Epoch 16 | Batch 50 | Loss: 0.6336469650268555\n",
      "Epoch 16 | Batch 51 | Loss: 0.16272538900375366\n",
      "Epoch 16 | Batch 52 | Loss: 0.2049475610256195\n",
      "Epoch 16 | Batch 53 | Loss: 0.288859486579895\n",
      "Epoch 16 | Batch 54 | Loss: 0.13287070393562317\n",
      "Epoch 16 | Batch 55 | Loss: 0.32750189304351807\n",
      "Epoch 16 | Batch 56 | Loss: 0.7985894680023193\n",
      "Epoch 16 | Batch 57 | Loss: 0.7649712562561035\n",
      "Epoch 16 | Batch 58 | Loss: 0.22369253635406494\n",
      "Epoch 16 | Batch 59 | Loss: 0.1461227685213089\n",
      "Epoch 16 | Batch 60 | Loss: 0.8325586318969727\n",
      "Epoch 16 | Batch 61 | Loss: 0.4517175555229187\n",
      "Epoch 16 | Batch 62 | Loss: 0.6122698783874512\n",
      "Epoch 16 | Batch 63 | Loss: 0.30544573068618774\n",
      "Epoch 16 | Batch 64 | Loss: 0.19520288705825806\n",
      "Epoch 16 | Batch 65 | Loss: 0.2307627946138382\n",
      "Epoch 16 | Batch 66 | Loss: 0.22086070477962494\n",
      "Epoch 16 | Batch 67 | Loss: 0.7669129371643066\n",
      "Epoch 16 | Batch 68 | Loss: 0.08660869300365448\n",
      "Epoch 16 | Batch 69 | Loss: 0.15534277260303497\n",
      "Epoch 16 | Batch 70 | Loss: 0.12638169527053833\n",
      "Epoch 16 | Batch 71 | Loss: 0.24880799651145935\n",
      "Epoch 16 | Batch 72 | Loss: 0.4214351177215576\n",
      "Epoch 16 | Batch 73 | Loss: 0.7683680653572083\n",
      "Epoch 16 | Batch 74 | Loss: 0.645225465297699\n",
      "Epoch 16 | Batch 75 | Loss: 0.35914137959480286\n",
      "Epoch 16 | Batch 76 | Loss: 0.6040138602256775\n",
      "Epoch 16 | Batch 77 | Loss: 0.42737680673599243\n",
      "Epoch 16 | Batch 78 | Loss: 0.12692993879318237\n",
      "Epoch 16 | Batch 79 | Loss: 0.6098233461380005\n",
      "Epoch 16 | Batch 80 | Loss: 0.4348372220993042\n",
      "Epoch 16 | Batch 81 | Loss: 0.3936269283294678\n",
      "Epoch 16 | Batch 82 | Loss: 1.331594705581665\n",
      "Epoch 16 | Batch 83 | Loss: 0.34940123558044434\n",
      "Epoch 16 | Batch 84 | Loss: 0.2788213789463043\n",
      "Epoch 16 | Batch 85 | Loss: 0.1817200630903244\n",
      "Epoch 16 | Batch 86 | Loss: 0.1809820532798767\n",
      "Epoch 16 | Batch 87 | Loss: 0.15620586276054382\n",
      "Epoch 16 | Batch 88 | Loss: 0.24753141403198242\n",
      "Epoch 16 | Batch 89 | Loss: 0.29528868198394775\n",
      "Epoch 16 | Batch 90 | Loss: 0.15672525763511658\n",
      "Epoch 17 | Batch 1 | Loss: 0.5169262886047363\n",
      "Epoch 17 | Batch 2 | Loss: 0.10459096729755402\n",
      "Epoch 17 | Batch 3 | Loss: 0.5226444005966187\n",
      "Epoch 17 | Batch 4 | Loss: 0.34041041135787964\n",
      "Epoch 17 | Batch 5 | Loss: 0.23525001108646393\n",
      "Epoch 17 | Batch 6 | Loss: 0.5724402666091919\n",
      "Epoch 17 | Batch 7 | Loss: 0.1071237325668335\n",
      "Epoch 17 | Batch 8 | Loss: 0.19517168402671814\n",
      "Epoch 17 | Batch 9 | Loss: 0.21613362431526184\n",
      "Epoch 17 | Batch 10 | Loss: 0.18428610265254974\n",
      "Epoch 17 | Batch 11 | Loss: 0.2878667712211609\n",
      "Epoch 17 | Batch 12 | Loss: 0.4157702326774597\n",
      "Epoch 17 | Batch 13 | Loss: 0.4028511643409729\n",
      "Epoch 17 | Batch 14 | Loss: 0.5838059186935425\n",
      "Epoch 17 | Batch 15 | Loss: 0.32796791195869446\n",
      "Epoch 17 | Batch 16 | Loss: 0.49873924255371094\n",
      "Epoch 17 | Batch 17 | Loss: 0.17454250156879425\n",
      "Epoch 17 | Batch 18 | Loss: 0.7998529076576233\n",
      "Epoch 17 | Batch 19 | Loss: 0.5170556306838989\n",
      "Epoch 17 | Batch 20 | Loss: 0.4358412027359009\n",
      "Epoch 17 | Batch 21 | Loss: 0.23986206948757172\n",
      "Epoch 17 | Batch 22 | Loss: 0.3469316363334656\n",
      "Epoch 17 | Batch 23 | Loss: 0.3585410714149475\n",
      "Epoch 17 | Batch 24 | Loss: 0.31441158056259155\n",
      "Epoch 17 | Batch 25 | Loss: 0.4295938313007355\n",
      "Epoch 17 | Batch 26 | Loss: 0.6957966685295105\n",
      "Epoch 17 | Batch 27 | Loss: 0.35244327783584595\n",
      "Epoch 17 | Batch 28 | Loss: 0.46134743094444275\n",
      "Epoch 17 | Batch 29 | Loss: 0.5228554606437683\n",
      "Epoch 17 | Batch 30 | Loss: 0.32231664657592773\n",
      "Epoch 17 | Batch 31 | Loss: 0.26353690028190613\n",
      "Epoch 17 | Batch 32 | Loss: 0.2141755223274231\n",
      "Epoch 17 | Batch 33 | Loss: 0.41657784581184387\n",
      "Epoch 17 | Batch 34 | Loss: 0.3682499825954437\n",
      "Epoch 17 | Batch 35 | Loss: 0.32698971033096313\n",
      "Epoch 17 | Batch 36 | Loss: 0.5710912346839905\n",
      "Epoch 17 | Batch 37 | Loss: 0.8941755294799805\n",
      "Epoch 17 | Batch 38 | Loss: 0.4078311324119568\n",
      "Epoch 17 | Batch 39 | Loss: 0.34973055124282837\n",
      "Epoch 17 | Batch 40 | Loss: 0.4658966064453125\n",
      "Epoch 17 | Batch 41 | Loss: 0.3678790330886841\n",
      "Epoch 17 | Batch 42 | Loss: 0.4209080636501312\n",
      "Epoch 17 | Batch 43 | Loss: 0.7723770141601562\n",
      "Epoch 17 | Batch 44 | Loss: 0.34084802865982056\n",
      "Epoch 17 | Batch 45 | Loss: 0.32665473222732544\n",
      "Epoch 17 | Batch 46 | Loss: 0.36662065982818604\n",
      "Epoch 17 | Batch 47 | Loss: 0.5536324381828308\n",
      "Epoch 17 | Batch 48 | Loss: 0.3405349850654602\n",
      "Epoch 17 | Batch 49 | Loss: 0.23833952844142914\n",
      "Epoch 17 | Batch 50 | Loss: 0.5044890642166138\n",
      "Epoch 17 | Batch 51 | Loss: 0.1686423420906067\n",
      "Epoch 17 | Batch 52 | Loss: 0.4702453017234802\n",
      "Epoch 17 | Batch 53 | Loss: 0.31300440430641174\n",
      "Epoch 17 | Batch 54 | Loss: 0.14462745189666748\n",
      "Epoch 17 | Batch 55 | Loss: 0.3971121907234192\n",
      "Epoch 17 | Batch 56 | Loss: 0.885968029499054\n",
      "Epoch 17 | Batch 57 | Loss: 0.18051239848136902\n",
      "Epoch 17 | Batch 58 | Loss: 0.17205168306827545\n",
      "Epoch 17 | Batch 59 | Loss: 0.2348075956106186\n",
      "Epoch 17 | Batch 60 | Loss: 0.6311504244804382\n",
      "Epoch 17 | Batch 61 | Loss: 0.27057015895843506\n",
      "Epoch 17 | Batch 62 | Loss: 0.6462719440460205\n",
      "Epoch 17 | Batch 63 | Loss: 0.30471962690353394\n",
      "Epoch 17 | Batch 64 | Loss: 0.7419145703315735\n",
      "Epoch 17 | Batch 65 | Loss: 0.2789056599140167\n",
      "Epoch 17 | Batch 66 | Loss: 0.6746704578399658\n",
      "Epoch 17 | Batch 67 | Loss: 0.5348925590515137\n",
      "Epoch 17 | Batch 68 | Loss: 0.397739052772522\n",
      "Epoch 17 | Batch 69 | Loss: 0.5175766944885254\n",
      "Epoch 17 | Batch 70 | Loss: 0.18562057614326477\n",
      "Epoch 17 | Batch 71 | Loss: 0.36706504225730896\n",
      "Epoch 17 | Batch 72 | Loss: 0.3642037510871887\n",
      "Epoch 17 | Batch 73 | Loss: 0.17738434672355652\n",
      "Epoch 17 | Batch 74 | Loss: 0.09944382309913635\n",
      "Epoch 17 | Batch 75 | Loss: 0.144748717546463\n",
      "Epoch 17 | Batch 76 | Loss: 0.4796127378940582\n",
      "Epoch 17 | Batch 77 | Loss: 0.25043168663978577\n",
      "Epoch 17 | Batch 78 | Loss: 0.32391005754470825\n",
      "Epoch 17 | Batch 79 | Loss: 0.5535039901733398\n",
      "Epoch 17 | Batch 80 | Loss: 0.36055687069892883\n",
      "Epoch 17 | Batch 81 | Loss: 0.2364579290151596\n",
      "Epoch 17 | Batch 82 | Loss: 0.14324705302715302\n",
      "Epoch 17 | Batch 83 | Loss: 0.2306189239025116\n",
      "Epoch 17 | Batch 84 | Loss: 0.4694725275039673\n",
      "Epoch 17 | Batch 85 | Loss: 0.11900933086872101\n",
      "Epoch 17 | Batch 86 | Loss: 0.22282381355762482\n",
      "Epoch 17 | Batch 87 | Loss: 0.4867253601551056\n",
      "Epoch 17 | Batch 88 | Loss: 0.669121265411377\n",
      "Epoch 17 | Batch 89 | Loss: 0.20760220289230347\n",
      "Epoch 17 | Batch 90 | Loss: 0.0039044369477778673\n",
      "Epoch 18 | Batch 1 | Loss: 0.40756291151046753\n",
      "Epoch 18 | Batch 2 | Loss: 0.2065587341785431\n",
      "Epoch 18 | Batch 3 | Loss: 0.24877049028873444\n",
      "Epoch 18 | Batch 4 | Loss: 0.19488859176635742\n",
      "Epoch 18 | Batch 5 | Loss: 0.7030541896820068\n",
      "Epoch 18 | Batch 6 | Loss: 0.3377135992050171\n",
      "Epoch 18 | Batch 7 | Loss: 0.41391509771347046\n",
      "Epoch 18 | Batch 8 | Loss: 0.5192504525184631\n",
      "Epoch 18 | Batch 9 | Loss: 0.12006119638681412\n",
      "Epoch 18 | Batch 10 | Loss: 0.28576692938804626\n",
      "Epoch 18 | Batch 11 | Loss: 0.19192665815353394\n",
      "Epoch 18 | Batch 12 | Loss: 0.14713363349437714\n",
      "Epoch 18 | Batch 13 | Loss: 0.5722476840019226\n",
      "Epoch 18 | Batch 14 | Loss: 0.6298673152923584\n",
      "Epoch 18 | Batch 15 | Loss: 0.179519921541214\n",
      "Epoch 18 | Batch 16 | Loss: 0.33961814641952515\n",
      "Epoch 18 | Batch 17 | Loss: 0.22720541059970856\n",
      "Epoch 18 | Batch 18 | Loss: 0.6744077205657959\n",
      "Epoch 18 | Batch 19 | Loss: 0.33302077651023865\n",
      "Epoch 18 | Batch 20 | Loss: 0.36882394552230835\n",
      "Epoch 18 | Batch 21 | Loss: 0.4060773253440857\n",
      "Epoch 18 | Batch 22 | Loss: 0.12146703898906708\n",
      "Epoch 18 | Batch 23 | Loss: 0.138992041349411\n",
      "Epoch 18 | Batch 24 | Loss: 0.5288834571838379\n",
      "Epoch 18 | Batch 25 | Loss: 0.8061302900314331\n",
      "Epoch 18 | Batch 26 | Loss: 0.696954071521759\n",
      "Epoch 18 | Batch 27 | Loss: 0.5548100471496582\n",
      "Epoch 18 | Batch 28 | Loss: 0.6907883882522583\n",
      "Epoch 18 | Batch 29 | Loss: 0.4216978847980499\n",
      "Epoch 18 | Batch 30 | Loss: 0.7715426683425903\n",
      "Epoch 18 | Batch 31 | Loss: 0.19202810525894165\n",
      "Epoch 18 | Batch 32 | Loss: 0.1737225204706192\n",
      "Epoch 18 | Batch 33 | Loss: 0.3201444149017334\n",
      "Epoch 18 | Batch 34 | Loss: 0.24941962957382202\n",
      "Epoch 18 | Batch 35 | Loss: 0.5983991026878357\n",
      "Epoch 18 | Batch 36 | Loss: 0.35227930545806885\n",
      "Epoch 18 | Batch 37 | Loss: 0.26102215051651\n",
      "Epoch 18 | Batch 38 | Loss: 0.24874748289585114\n",
      "Epoch 18 | Batch 39 | Loss: 0.21713364124298096\n",
      "Epoch 18 | Batch 40 | Loss: 0.14216552674770355\n",
      "Epoch 18 | Batch 41 | Loss: 0.3296521306037903\n",
      "Epoch 18 | Batch 42 | Loss: 0.9406489133834839\n",
      "Epoch 18 | Batch 43 | Loss: 0.20441406965255737\n",
      "Epoch 18 | Batch 44 | Loss: 0.5589808821678162\n",
      "Epoch 18 | Batch 45 | Loss: 0.5795390605926514\n",
      "Epoch 18 | Batch 46 | Loss: 1.0763828754425049\n",
      "Epoch 18 | Batch 47 | Loss: 0.2704732418060303\n",
      "Epoch 18 | Batch 48 | Loss: 0.5062848925590515\n",
      "Epoch 18 | Batch 49 | Loss: 0.30115288496017456\n",
      "Epoch 18 | Batch 50 | Loss: 0.3339715003967285\n",
      "Epoch 18 | Batch 51 | Loss: 0.19930508732795715\n",
      "Epoch 18 | Batch 52 | Loss: 0.4365598261356354\n",
      "Epoch 18 | Batch 53 | Loss: 0.28458917140960693\n",
      "Epoch 18 | Batch 54 | Loss: 0.15740913152694702\n",
      "Epoch 18 | Batch 55 | Loss: 0.24402852356433868\n",
      "Epoch 18 | Batch 56 | Loss: 0.2557145357131958\n",
      "Epoch 18 | Batch 57 | Loss: 0.4633195102214813\n",
      "Epoch 18 | Batch 58 | Loss: 0.11493600904941559\n",
      "Epoch 18 | Batch 59 | Loss: 0.6475905179977417\n",
      "Epoch 18 | Batch 60 | Loss: 0.467814177274704\n",
      "Epoch 18 | Batch 61 | Loss: 0.47431492805480957\n",
      "Epoch 18 | Batch 62 | Loss: 0.4789165258407593\n",
      "Epoch 18 | Batch 63 | Loss: 0.32735395431518555\n",
      "Epoch 18 | Batch 64 | Loss: 0.12040556222200394\n",
      "Epoch 18 | Batch 65 | Loss: 0.8535981178283691\n",
      "Epoch 18 | Batch 66 | Loss: 0.32899415493011475\n",
      "Epoch 18 | Batch 67 | Loss: 0.4479537606239319\n",
      "Epoch 18 | Batch 68 | Loss: 0.30364614725112915\n",
      "Epoch 18 | Batch 69 | Loss: 0.3226940631866455\n",
      "Epoch 18 | Batch 70 | Loss: 0.1804346740245819\n",
      "Epoch 18 | Batch 71 | Loss: 0.2767704725265503\n",
      "Epoch 18 | Batch 72 | Loss: 0.24593186378479004\n",
      "Epoch 18 | Batch 73 | Loss: 0.10937157273292542\n",
      "Epoch 18 | Batch 74 | Loss: 0.15093553066253662\n",
      "Epoch 18 | Batch 75 | Loss: 0.5389836430549622\n",
      "Epoch 18 | Batch 76 | Loss: 0.2832660377025604\n",
      "Epoch 18 | Batch 77 | Loss: 0.2630104720592499\n",
      "Epoch 18 | Batch 78 | Loss: 0.6318556070327759\n",
      "Epoch 18 | Batch 79 | Loss: 0.5162159204483032\n",
      "Epoch 18 | Batch 80 | Loss: 0.25357604026794434\n",
      "Epoch 18 | Batch 81 | Loss: 0.4107087254524231\n",
      "Epoch 18 | Batch 82 | Loss: 0.440707802772522\n",
      "Epoch 18 | Batch 83 | Loss: 0.269498348236084\n",
      "Epoch 18 | Batch 84 | Loss: 0.8185970783233643\n",
      "Epoch 18 | Batch 85 | Loss: 0.5451995730400085\n",
      "Epoch 18 | Batch 86 | Loss: 0.33937543630599976\n",
      "Epoch 18 | Batch 87 | Loss: 0.27154943346977234\n",
      "Epoch 18 | Batch 88 | Loss: 0.22390364110469818\n",
      "Epoch 18 | Batch 89 | Loss: 0.34162795543670654\n",
      "Epoch 18 | Batch 90 | Loss: 0.05249293893575668\n",
      "Epoch 19 | Batch 1 | Loss: 0.47590401768684387\n",
      "Epoch 19 | Batch 2 | Loss: 0.8931777477264404\n",
      "Epoch 19 | Batch 3 | Loss: 0.2071095108985901\n",
      "Epoch 19 | Batch 4 | Loss: 0.4564934968948364\n",
      "Epoch 19 | Batch 5 | Loss: 0.3122117221355438\n",
      "Epoch 19 | Batch 6 | Loss: 0.22902092337608337\n",
      "Epoch 19 | Batch 7 | Loss: 0.3569503724575043\n",
      "Epoch 19 | Batch 8 | Loss: 0.440116822719574\n",
      "Epoch 19 | Batch 9 | Loss: 0.14072737097740173\n",
      "Epoch 19 | Batch 10 | Loss: 0.46889379620552063\n",
      "Epoch 19 | Batch 11 | Loss: 0.26602810621261597\n",
      "Epoch 19 | Batch 12 | Loss: 0.2704210579395294\n",
      "Epoch 19 | Batch 13 | Loss: 0.9351328611373901\n",
      "Epoch 19 | Batch 14 | Loss: 0.5988622903823853\n",
      "Epoch 19 | Batch 15 | Loss: 0.4641645550727844\n",
      "Epoch 19 | Batch 16 | Loss: 0.4270167946815491\n",
      "Epoch 19 | Batch 17 | Loss: 0.3807222247123718\n",
      "Epoch 19 | Batch 18 | Loss: 0.16960525512695312\n",
      "Epoch 19 | Batch 19 | Loss: 0.46792739629745483\n",
      "Epoch 19 | Batch 20 | Loss: 0.525699257850647\n",
      "Epoch 19 | Batch 21 | Loss: 0.7270705103874207\n",
      "Epoch 19 | Batch 22 | Loss: 0.46325597167015076\n",
      "Epoch 19 | Batch 23 | Loss: 0.467402845621109\n",
      "Epoch 19 | Batch 24 | Loss: 0.4834786653518677\n",
      "Epoch 19 | Batch 25 | Loss: 0.11891062557697296\n",
      "Epoch 19 | Batch 26 | Loss: 0.20661784708499908\n",
      "Epoch 19 | Batch 27 | Loss: 0.4740447998046875\n",
      "Epoch 19 | Batch 28 | Loss: 0.4540206789970398\n",
      "Epoch 19 | Batch 29 | Loss: 0.4756729006767273\n",
      "Epoch 19 | Batch 30 | Loss: 0.34936845302581787\n",
      "Epoch 19 | Batch 31 | Loss: 0.30513736605644226\n",
      "Epoch 19 | Batch 32 | Loss: 0.22582778334617615\n",
      "Epoch 19 | Batch 33 | Loss: 0.47665637731552124\n",
      "Epoch 19 | Batch 34 | Loss: 0.29961949586868286\n",
      "Epoch 19 | Batch 35 | Loss: 0.26700183749198914\n",
      "Epoch 19 | Batch 36 | Loss: 0.8044312000274658\n",
      "Epoch 19 | Batch 37 | Loss: 0.19634076952934265\n",
      "Epoch 19 | Batch 38 | Loss: 0.6605489253997803\n",
      "Epoch 19 | Batch 39 | Loss: 0.7517118453979492\n",
      "Epoch 19 | Batch 40 | Loss: 0.12425367534160614\n",
      "Epoch 19 | Batch 41 | Loss: 0.48357057571411133\n",
      "Epoch 19 | Batch 42 | Loss: 0.5169899463653564\n",
      "Epoch 19 | Batch 43 | Loss: 0.6365342140197754\n",
      "Epoch 19 | Batch 44 | Loss: 0.3618500828742981\n",
      "Epoch 19 | Batch 45 | Loss: 0.5588430166244507\n",
      "Epoch 19 | Batch 46 | Loss: 0.5223061442375183\n",
      "Epoch 19 | Batch 47 | Loss: 0.4107028543949127\n",
      "Epoch 19 | Batch 48 | Loss: 0.13910479843616486\n",
      "Epoch 19 | Batch 49 | Loss: 0.27901774644851685\n",
      "Epoch 19 | Batch 50 | Loss: 0.5352110862731934\n",
      "Epoch 19 | Batch 51 | Loss: 0.3892253041267395\n",
      "Epoch 19 | Batch 52 | Loss: 0.4474163055419922\n",
      "Epoch 19 | Batch 53 | Loss: 0.5278934240341187\n",
      "Epoch 19 | Batch 54 | Loss: 0.4451427757740021\n",
      "Epoch 19 | Batch 55 | Loss: 0.35642480850219727\n",
      "Epoch 19 | Batch 56 | Loss: 0.2707667052745819\n",
      "Epoch 19 | Batch 57 | Loss: 0.4866217374801636\n",
      "Epoch 19 | Batch 58 | Loss: 0.28160083293914795\n",
      "Epoch 19 | Batch 59 | Loss: 0.3413177728652954\n",
      "Epoch 19 | Batch 60 | Loss: 0.182722270488739\n",
      "Epoch 19 | Batch 61 | Loss: 0.10463419556617737\n",
      "Epoch 19 | Batch 62 | Loss: 0.2731727063655853\n",
      "Epoch 19 | Batch 63 | Loss: 0.3673041760921478\n",
      "Epoch 19 | Batch 64 | Loss: 0.07292678952217102\n",
      "Epoch 19 | Batch 65 | Loss: 0.2954477071762085\n",
      "Epoch 19 | Batch 66 | Loss: 0.07140468806028366\n",
      "Epoch 19 | Batch 67 | Loss: 0.3009026348590851\n",
      "Epoch 19 | Batch 68 | Loss: 0.3444415330886841\n",
      "Epoch 19 | Batch 69 | Loss: 0.35850009322166443\n",
      "Epoch 19 | Batch 70 | Loss: 0.5191606879234314\n",
      "Epoch 19 | Batch 71 | Loss: 0.3279078006744385\n",
      "Epoch 19 | Batch 72 | Loss: 0.17026370763778687\n",
      "Epoch 19 | Batch 73 | Loss: 0.6756970882415771\n",
      "Epoch 19 | Batch 74 | Loss: 0.25010383129119873\n",
      "Epoch 19 | Batch 75 | Loss: 0.23536616563796997\n",
      "Epoch 19 | Batch 76 | Loss: 0.16412757337093353\n",
      "Epoch 19 | Batch 77 | Loss: 0.46367669105529785\n",
      "Epoch 19 | Batch 78 | Loss: 0.1111740916967392\n",
      "Epoch 19 | Batch 79 | Loss: 0.21916480362415314\n",
      "Epoch 19 | Batch 80 | Loss: 0.49601274728775024\n",
      "Epoch 19 | Batch 81 | Loss: 0.4480001926422119\n",
      "Epoch 19 | Batch 82 | Loss: 0.774706244468689\n",
      "Epoch 19 | Batch 83 | Loss: 0.4019867777824402\n",
      "Epoch 19 | Batch 84 | Loss: 0.2660333514213562\n",
      "Epoch 19 | Batch 85 | Loss: 0.3345690965652466\n",
      "Epoch 19 | Batch 86 | Loss: 0.44124293327331543\n",
      "Epoch 19 | Batch 87 | Loss: 0.11554259061813354\n",
      "Epoch 19 | Batch 88 | Loss: 0.1277872920036316\n",
      "Epoch 19 | Batch 89 | Loss: 0.0727033019065857\n",
      "Epoch 19 | Batch 90 | Loss: 0.1047486886382103\n",
      "Epoch 20 | Batch 1 | Loss: 0.4629806578159332\n",
      "Epoch 20 | Batch 2 | Loss: 1.0157579183578491\n",
      "Epoch 20 | Batch 3 | Loss: 0.5968542098999023\n",
      "Epoch 20 | Batch 4 | Loss: 0.17419126629829407\n",
      "Epoch 20 | Batch 5 | Loss: 0.30480027198791504\n",
      "Epoch 20 | Batch 6 | Loss: 0.08457955718040466\n",
      "Epoch 20 | Batch 7 | Loss: 0.739040732383728\n",
      "Epoch 20 | Batch 8 | Loss: 0.8054904937744141\n",
      "Epoch 20 | Batch 9 | Loss: 0.7087768316268921\n",
      "Epoch 20 | Batch 10 | Loss: 0.16872407495975494\n",
      "Epoch 20 | Batch 11 | Loss: 0.7114806771278381\n",
      "Epoch 20 | Batch 12 | Loss: 0.22082415223121643\n",
      "Epoch 20 | Batch 13 | Loss: 0.19431620836257935\n",
      "Epoch 20 | Batch 14 | Loss: 0.14100778102874756\n",
      "Epoch 20 | Batch 15 | Loss: 0.43443718552589417\n",
      "Epoch 20 | Batch 16 | Loss: 0.7264394164085388\n",
      "Epoch 20 | Batch 17 | Loss: 0.7458598613739014\n",
      "Epoch 20 | Batch 18 | Loss: 0.3667423129081726\n",
      "Epoch 20 | Batch 19 | Loss: 0.37182265520095825\n",
      "Epoch 20 | Batch 20 | Loss: 0.21216286718845367\n",
      "Epoch 20 | Batch 21 | Loss: 0.7077946662902832\n",
      "Epoch 20 | Batch 22 | Loss: 0.4281189441680908\n",
      "Epoch 20 | Batch 23 | Loss: 0.7948983907699585\n",
      "Epoch 20 | Batch 24 | Loss: 0.2292720377445221\n",
      "Epoch 20 | Batch 25 | Loss: 0.07801319658756256\n",
      "Epoch 20 | Batch 26 | Loss: 0.1377568244934082\n",
      "Epoch 20 | Batch 27 | Loss: 0.3330310583114624\n",
      "Epoch 20 | Batch 28 | Loss: 0.12884727120399475\n",
      "Epoch 20 | Batch 29 | Loss: 0.4252072870731354\n",
      "Epoch 20 | Batch 30 | Loss: 0.36326855421066284\n",
      "Epoch 20 | Batch 31 | Loss: 0.3313130736351013\n",
      "Epoch 20 | Batch 32 | Loss: 0.3145594298839569\n",
      "Epoch 20 | Batch 33 | Loss: 0.3526173233985901\n",
      "Epoch 20 | Batch 34 | Loss: 0.08703796565532684\n",
      "Epoch 20 | Batch 35 | Loss: 0.6520851254463196\n",
      "Epoch 20 | Batch 36 | Loss: 0.3018938899040222\n",
      "Epoch 20 | Batch 37 | Loss: 0.12556049227714539\n",
      "Epoch 20 | Batch 38 | Loss: 0.2422163188457489\n",
      "Epoch 20 | Batch 39 | Loss: 0.2538405656814575\n",
      "Epoch 20 | Batch 40 | Loss: 0.19179867208003998\n",
      "Epoch 20 | Batch 41 | Loss: 0.1729908585548401\n",
      "Epoch 20 | Batch 42 | Loss: 0.2883895933628082\n",
      "Epoch 20 | Batch 43 | Loss: 0.188319131731987\n",
      "Epoch 20 | Batch 44 | Loss: 0.3381478488445282\n",
      "Epoch 20 | Batch 45 | Loss: 0.3853044807910919\n",
      "Epoch 20 | Batch 46 | Loss: 0.20374441146850586\n",
      "Epoch 20 | Batch 47 | Loss: 0.12309730052947998\n",
      "Epoch 20 | Batch 48 | Loss: 0.3715985417366028\n",
      "Epoch 20 | Batch 49 | Loss: 0.650606095790863\n",
      "Epoch 20 | Batch 50 | Loss: 0.32266706228256226\n",
      "Epoch 20 | Batch 51 | Loss: 0.4015498161315918\n",
      "Epoch 20 | Batch 52 | Loss: 0.3246001899242401\n",
      "Epoch 20 | Batch 53 | Loss: 0.473389208316803\n",
      "Epoch 20 | Batch 54 | Loss: 0.4888012409210205\n",
      "Epoch 20 | Batch 55 | Loss: 0.7254922389984131\n",
      "Epoch 20 | Batch 56 | Loss: 0.6287841796875\n",
      "Epoch 20 | Batch 57 | Loss: 0.3507823348045349\n",
      "Epoch 20 | Batch 58 | Loss: 0.7292214632034302\n",
      "Epoch 20 | Batch 59 | Loss: 0.25710874795913696\n",
      "Epoch 20 | Batch 60 | Loss: 0.5071067810058594\n",
      "Epoch 20 | Batch 61 | Loss: 0.5473509430885315\n",
      "Epoch 20 | Batch 62 | Loss: 0.4086395502090454\n",
      "Epoch 20 | Batch 63 | Loss: 0.41369110345840454\n",
      "Epoch 20 | Batch 64 | Loss: 0.6193007826805115\n",
      "Epoch 20 | Batch 65 | Loss: 0.10714147239923477\n",
      "Epoch 20 | Batch 66 | Loss: 0.2798512578010559\n",
      "Epoch 20 | Batch 67 | Loss: 0.5536417961120605\n",
      "Epoch 20 | Batch 68 | Loss: 0.194475919008255\n",
      "Epoch 20 | Batch 69 | Loss: 0.6660187244415283\n",
      "Epoch 20 | Batch 70 | Loss: 0.27785468101501465\n",
      "Epoch 20 | Batch 71 | Loss: 0.4789021611213684\n",
      "Epoch 20 | Batch 72 | Loss: 0.3352063000202179\n",
      "Epoch 20 | Batch 73 | Loss: 0.5205649137496948\n",
      "Epoch 20 | Batch 74 | Loss: 0.13316169381141663\n",
      "Epoch 20 | Batch 75 | Loss: 0.5398025512695312\n",
      "Epoch 20 | Batch 76 | Loss: 0.2368726283311844\n",
      "Epoch 20 | Batch 77 | Loss: 0.42152416706085205\n",
      "Epoch 20 | Batch 78 | Loss: 0.7563177347183228\n",
      "Epoch 20 | Batch 79 | Loss: 0.11287691444158554\n",
      "Epoch 20 | Batch 80 | Loss: 0.15519675612449646\n",
      "Epoch 20 | Batch 81 | Loss: 0.9018769860267639\n",
      "Epoch 20 | Batch 82 | Loss: 0.4029177129268646\n",
      "Epoch 20 | Batch 83 | Loss: 0.13029798865318298\n",
      "Epoch 20 | Batch 84 | Loss: 0.287824422121048\n",
      "Epoch 20 | Batch 85 | Loss: 0.5265136957168579\n",
      "Epoch 20 | Batch 86 | Loss: 0.32818683981895447\n",
      "Epoch 20 | Batch 87 | Loss: 0.3293176293373108\n",
      "Epoch 20 | Batch 88 | Loss: 0.1571161150932312\n",
      "Epoch 20 | Batch 89 | Loss: 0.21143895387649536\n",
      "Epoch 20 | Batch 90 | Loss: 0.7173454761505127\n",
      "Epoch 21 | Batch 1 | Loss: 0.4473210573196411\n",
      "Epoch 21 | Batch 2 | Loss: 0.7268599271774292\n",
      "Epoch 21 | Batch 3 | Loss: 0.48851197957992554\n",
      "Epoch 21 | Batch 4 | Loss: 0.527146577835083\n",
      "Epoch 21 | Batch 5 | Loss: 0.45815378427505493\n",
      "Epoch 21 | Batch 6 | Loss: 0.4141606390476227\n",
      "Epoch 21 | Batch 7 | Loss: 0.562935471534729\n",
      "Epoch 21 | Batch 8 | Loss: 0.20315246284008026\n",
      "Epoch 21 | Batch 9 | Loss: 0.4039701521396637\n",
      "Epoch 21 | Batch 10 | Loss: 0.3068937063217163\n",
      "Epoch 21 | Batch 11 | Loss: 0.42255598306655884\n",
      "Epoch 21 | Batch 12 | Loss: 0.19018611311912537\n",
      "Epoch 21 | Batch 13 | Loss: 0.2522299587726593\n",
      "Epoch 21 | Batch 14 | Loss: 0.5130825638771057\n",
      "Epoch 21 | Batch 15 | Loss: 0.3496914207935333\n",
      "Epoch 21 | Batch 16 | Loss: 0.6548966765403748\n",
      "Epoch 21 | Batch 17 | Loss: 0.37914198637008667\n",
      "Epoch 21 | Batch 18 | Loss: 1.0475658178329468\n",
      "Epoch 21 | Batch 19 | Loss: 0.5319231748580933\n",
      "Epoch 21 | Batch 20 | Loss: 0.4849556088447571\n",
      "Epoch 21 | Batch 21 | Loss: 0.5741463303565979\n",
      "Epoch 21 | Batch 22 | Loss: 0.18634451925754547\n",
      "Epoch 21 | Batch 23 | Loss: 0.7306865453720093\n",
      "Epoch 21 | Batch 24 | Loss: 0.23585234582424164\n",
      "Epoch 21 | Batch 25 | Loss: 0.15667706727981567\n",
      "Epoch 21 | Batch 26 | Loss: 0.4984831213951111\n",
      "Epoch 21 | Batch 27 | Loss: 0.13960756361484528\n",
      "Epoch 21 | Batch 28 | Loss: 0.14056625962257385\n",
      "Epoch 21 | Batch 29 | Loss: 0.6364543437957764\n",
      "Epoch 21 | Batch 30 | Loss: 0.2890037000179291\n",
      "Epoch 21 | Batch 31 | Loss: 0.8232514262199402\n",
      "Epoch 21 | Batch 32 | Loss: 0.4820588231086731\n",
      "Epoch 21 | Batch 33 | Loss: 0.44377538561820984\n",
      "Epoch 21 | Batch 34 | Loss: 0.28583723306655884\n",
      "Epoch 21 | Batch 35 | Loss: 0.1388089656829834\n",
      "Epoch 21 | Batch 36 | Loss: 0.18322992324829102\n",
      "Epoch 21 | Batch 37 | Loss: 0.1884291023015976\n",
      "Epoch 21 | Batch 38 | Loss: 0.187195286154747\n",
      "Epoch 21 | Batch 39 | Loss: 0.1527141034603119\n",
      "Epoch 21 | Batch 40 | Loss: 0.15872801840305328\n",
      "Epoch 21 | Batch 41 | Loss: 0.3657439947128296\n",
      "Epoch 21 | Batch 42 | Loss: 0.5468198657035828\n",
      "Epoch 21 | Batch 43 | Loss: 0.14915791153907776\n",
      "Epoch 21 | Batch 44 | Loss: 0.20795097947120667\n",
      "Epoch 21 | Batch 45 | Loss: 0.44565534591674805\n",
      "Epoch 21 | Batch 46 | Loss: 0.9157334566116333\n",
      "Epoch 21 | Batch 47 | Loss: 0.13601121306419373\n",
      "Epoch 21 | Batch 48 | Loss: 0.10183083266019821\n",
      "Epoch 21 | Batch 49 | Loss: 0.27994176745414734\n",
      "Epoch 21 | Batch 50 | Loss: 0.18688343465328217\n",
      "Epoch 21 | Batch 51 | Loss: 0.1603160947561264\n",
      "Epoch 21 | Batch 52 | Loss: 0.22566336393356323\n",
      "Epoch 21 | Batch 53 | Loss: 0.25885888934135437\n",
      "Epoch 21 | Batch 54 | Loss: 0.5846768617630005\n",
      "Epoch 21 | Batch 55 | Loss: 0.15961642563343048\n",
      "Epoch 21 | Batch 56 | Loss: 0.38077157735824585\n",
      "Epoch 21 | Batch 57 | Loss: 0.8576346635818481\n",
      "Epoch 21 | Batch 58 | Loss: 0.6197751760482788\n",
      "Epoch 21 | Batch 59 | Loss: 0.15088573098182678\n",
      "Epoch 21 | Batch 60 | Loss: 0.21838365495204926\n",
      "Epoch 21 | Batch 61 | Loss: 0.14261221885681152\n",
      "Epoch 21 | Batch 62 | Loss: 0.15319232642650604\n",
      "Epoch 21 | Batch 63 | Loss: 0.14382296800613403\n",
      "Epoch 21 | Batch 64 | Loss: 0.20129922032356262\n",
      "Epoch 21 | Batch 65 | Loss: 0.4093194305896759\n",
      "Epoch 21 | Batch 66 | Loss: 0.3514578640460968\n",
      "Epoch 21 | Batch 67 | Loss: 0.3773024082183838\n",
      "Epoch 21 | Batch 68 | Loss: 0.27195027470588684\n",
      "Epoch 21 | Batch 69 | Loss: 0.6870247721672058\n",
      "Epoch 21 | Batch 70 | Loss: 0.2647649347782135\n",
      "Epoch 21 | Batch 71 | Loss: 0.6169166564941406\n",
      "Epoch 21 | Batch 72 | Loss: 0.3653309643268585\n",
      "Epoch 21 | Batch 73 | Loss: 0.32592323422431946\n",
      "Epoch 21 | Batch 74 | Loss: 0.5032711029052734\n",
      "Epoch 21 | Batch 75 | Loss: 0.294215589761734\n",
      "Epoch 21 | Batch 76 | Loss: 0.1328687071800232\n",
      "Epoch 21 | Batch 77 | Loss: 0.5049088597297668\n",
      "Epoch 21 | Batch 78 | Loss: 0.22245889902114868\n",
      "Epoch 21 | Batch 79 | Loss: 0.12859991192817688\n",
      "Epoch 21 | Batch 80 | Loss: 0.41287919878959656\n",
      "Epoch 21 | Batch 81 | Loss: 0.6491950750350952\n",
      "Epoch 21 | Batch 82 | Loss: 0.7658007144927979\n",
      "Epoch 21 | Batch 83 | Loss: 0.32591697573661804\n",
      "Epoch 21 | Batch 84 | Loss: 0.1694437563419342\n",
      "Epoch 21 | Batch 85 | Loss: 0.5546633005142212\n",
      "Epoch 21 | Batch 86 | Loss: 0.17270489037036896\n",
      "Epoch 21 | Batch 87 | Loss: 0.1400962471961975\n",
      "Epoch 21 | Batch 88 | Loss: 0.6470323801040649\n",
      "Epoch 21 | Batch 89 | Loss: 0.3139830231666565\n",
      "Epoch 21 | Batch 90 | Loss: 0.029982639476656914\n",
      "Epoch 22 | Batch 1 | Loss: 0.08571016788482666\n",
      "Epoch 22 | Batch 2 | Loss: 0.7328869104385376\n",
      "Epoch 22 | Batch 3 | Loss: 0.3166772127151489\n",
      "Epoch 22 | Batch 4 | Loss: 0.2339988350868225\n",
      "Epoch 22 | Batch 5 | Loss: 0.30936017632484436\n",
      "Epoch 22 | Batch 6 | Loss: 0.25902998447418213\n",
      "Epoch 22 | Batch 7 | Loss: 0.4674157202243805\n",
      "Epoch 22 | Batch 8 | Loss: 0.5254019498825073\n",
      "Epoch 22 | Batch 9 | Loss: 0.12449310719966888\n",
      "Epoch 22 | Batch 10 | Loss: 0.11423483490943909\n",
      "Epoch 22 | Batch 11 | Loss: 0.41094064712524414\n",
      "Epoch 22 | Batch 12 | Loss: 0.37321221828460693\n",
      "Epoch 22 | Batch 13 | Loss: 0.5274549722671509\n",
      "Epoch 22 | Batch 14 | Loss: 0.3610602915287018\n",
      "Epoch 22 | Batch 15 | Loss: 1.154306411743164\n",
      "Epoch 22 | Batch 16 | Loss: 0.6071075201034546\n",
      "Epoch 22 | Batch 17 | Loss: 0.8534711599349976\n",
      "Epoch 22 | Batch 18 | Loss: 0.12116014212369919\n",
      "Epoch 22 | Batch 19 | Loss: 0.21315637230873108\n",
      "Epoch 22 | Batch 20 | Loss: 0.22982728481292725\n",
      "Epoch 22 | Batch 21 | Loss: 0.22328630089759827\n",
      "Epoch 22 | Batch 22 | Loss: 0.6842087507247925\n",
      "Epoch 22 | Batch 23 | Loss: 0.23476988077163696\n",
      "Epoch 22 | Batch 24 | Loss: 0.17037588357925415\n",
      "Epoch 22 | Batch 25 | Loss: 0.44833284616470337\n",
      "Epoch 22 | Batch 26 | Loss: 0.5544431209564209\n",
      "Epoch 22 | Batch 27 | Loss: 0.3129420280456543\n",
      "Epoch 22 | Batch 28 | Loss: 0.3465864956378937\n",
      "Epoch 22 | Batch 29 | Loss: 0.27750954031944275\n",
      "Epoch 22 | Batch 30 | Loss: 0.5142998099327087\n",
      "Epoch 22 | Batch 31 | Loss: 0.087934210896492\n",
      "Epoch 22 | Batch 32 | Loss: 0.3289012908935547\n",
      "Epoch 22 | Batch 33 | Loss: 0.3836842477321625\n",
      "Epoch 22 | Batch 34 | Loss: 0.42918315529823303\n",
      "Epoch 22 | Batch 35 | Loss: 0.08867602050304413\n",
      "Epoch 22 | Batch 36 | Loss: 0.07125517725944519\n",
      "Epoch 22 | Batch 37 | Loss: 0.28736090660095215\n",
      "Epoch 22 | Batch 38 | Loss: 0.19134145975112915\n",
      "Epoch 22 | Batch 39 | Loss: 0.49726206064224243\n",
      "Epoch 22 | Batch 40 | Loss: 0.8305444717407227\n",
      "Epoch 22 | Batch 41 | Loss: 0.6809273958206177\n",
      "Epoch 22 | Batch 42 | Loss: 0.5654522180557251\n",
      "Epoch 22 | Batch 43 | Loss: 0.29627570509910583\n",
      "Epoch 22 | Batch 44 | Loss: 0.39904603362083435\n",
      "Epoch 22 | Batch 45 | Loss: 0.137814000248909\n",
      "Epoch 22 | Batch 46 | Loss: 0.25363248586654663\n",
      "Epoch 22 | Batch 47 | Loss: 0.13235682249069214\n",
      "Epoch 22 | Batch 48 | Loss: 0.12091506272554398\n",
      "Epoch 22 | Batch 49 | Loss: 0.6310227513313293\n",
      "Epoch 22 | Batch 50 | Loss: 0.3091585636138916\n",
      "Epoch 22 | Batch 51 | Loss: 0.4141601324081421\n",
      "Epoch 22 | Batch 52 | Loss: 0.22890181839466095\n",
      "Epoch 22 | Batch 53 | Loss: 0.2561420500278473\n",
      "Epoch 22 | Batch 54 | Loss: 0.609155535697937\n",
      "Epoch 22 | Batch 55 | Loss: 1.0299742221832275\n",
      "Epoch 22 | Batch 56 | Loss: 0.27473288774490356\n",
      "Epoch 22 | Batch 57 | Loss: 0.3795788288116455\n",
      "Epoch 22 | Batch 58 | Loss: 0.13604527711868286\n",
      "Epoch 22 | Batch 59 | Loss: 0.5146399736404419\n",
      "Epoch 22 | Batch 60 | Loss: 0.6310383081436157\n",
      "Epoch 22 | Batch 61 | Loss: 0.2990414500236511\n",
      "Epoch 22 | Batch 62 | Loss: 0.14725518226623535\n",
      "Epoch 22 | Batch 63 | Loss: 0.19045034050941467\n",
      "Epoch 22 | Batch 64 | Loss: 0.33809906244277954\n",
      "Epoch 22 | Batch 65 | Loss: 0.16048288345336914\n",
      "Epoch 22 | Batch 66 | Loss: 0.4703393578529358\n",
      "Epoch 22 | Batch 67 | Loss: 0.9507929086685181\n",
      "Epoch 22 | Batch 68 | Loss: 0.12401853501796722\n",
      "Epoch 22 | Batch 69 | Loss: 0.12378649413585663\n",
      "Epoch 22 | Batch 70 | Loss: 0.2173687219619751\n",
      "Epoch 22 | Batch 71 | Loss: 0.14765283465385437\n",
      "Epoch 22 | Batch 72 | Loss: 0.22015994787216187\n",
      "Epoch 22 | Batch 73 | Loss: 1.007918119430542\n",
      "Epoch 22 | Batch 74 | Loss: 0.17673176527023315\n",
      "Epoch 22 | Batch 75 | Loss: 0.09643638134002686\n",
      "Epoch 22 | Batch 76 | Loss: 0.596820592880249\n",
      "Epoch 22 | Batch 77 | Loss: 0.5672539472579956\n",
      "Epoch 22 | Batch 78 | Loss: 0.406783789396286\n",
      "Epoch 22 | Batch 79 | Loss: 0.057753171771764755\n",
      "Epoch 22 | Batch 80 | Loss: 0.34069761633872986\n",
      "Epoch 22 | Batch 81 | Loss: 0.3841327130794525\n",
      "Epoch 22 | Batch 82 | Loss: 0.9086027145385742\n",
      "Epoch 22 | Batch 83 | Loss: 0.6232168674468994\n",
      "Epoch 22 | Batch 84 | Loss: 0.6960688829421997\n",
      "Epoch 22 | Batch 85 | Loss: 0.1190265417098999\n",
      "Epoch 22 | Batch 86 | Loss: 0.6558559536933899\n",
      "Epoch 22 | Batch 87 | Loss: 0.44470638036727905\n",
      "Epoch 22 | Batch 88 | Loss: 0.45979395508766174\n",
      "Epoch 22 | Batch 89 | Loss: 0.44587576389312744\n",
      "Epoch 22 | Batch 90 | Loss: 0.14331553876399994\n",
      "Epoch 23 | Batch 1 | Loss: 0.400284081697464\n",
      "Epoch 23 | Batch 2 | Loss: 0.3287493884563446\n",
      "Epoch 23 | Batch 3 | Loss: 0.23044022917747498\n",
      "Epoch 23 | Batch 4 | Loss: 0.5395409464836121\n",
      "Epoch 23 | Batch 5 | Loss: 0.438088059425354\n",
      "Epoch 23 | Batch 6 | Loss: 0.3274068832397461\n",
      "Epoch 23 | Batch 7 | Loss: 0.3452509045600891\n",
      "Epoch 23 | Batch 8 | Loss: 0.43856123089790344\n",
      "Epoch 23 | Batch 9 | Loss: 0.5726560950279236\n",
      "Epoch 23 | Batch 10 | Loss: 0.3850327730178833\n",
      "Epoch 23 | Batch 11 | Loss: 0.6723312139511108\n",
      "Epoch 23 | Batch 12 | Loss: 0.08297783136367798\n",
      "Epoch 23 | Batch 13 | Loss: 0.11407195031642914\n",
      "Epoch 23 | Batch 14 | Loss: 0.25902843475341797\n",
      "Epoch 23 | Batch 15 | Loss: 0.16754132509231567\n",
      "Epoch 23 | Batch 16 | Loss: 0.1432575285434723\n",
      "Epoch 23 | Batch 17 | Loss: 0.11681777238845825\n",
      "Epoch 23 | Batch 18 | Loss: 0.3534843921661377\n",
      "Epoch 23 | Batch 19 | Loss: 0.5144100785255432\n",
      "Epoch 23 | Batch 20 | Loss: 0.7408856153488159\n",
      "Epoch 23 | Batch 21 | Loss: 0.9889938831329346\n",
      "Epoch 23 | Batch 22 | Loss: 0.3551534414291382\n",
      "Epoch 23 | Batch 23 | Loss: 0.7077834606170654\n",
      "Epoch 23 | Batch 24 | Loss: 0.6632182598114014\n",
      "Epoch 23 | Batch 25 | Loss: 0.17914754152297974\n",
      "Epoch 23 | Batch 26 | Loss: 0.4407529830932617\n",
      "Epoch 23 | Batch 27 | Loss: 0.5102771520614624\n",
      "Epoch 23 | Batch 28 | Loss: 0.12328345328569412\n",
      "Epoch 23 | Batch 29 | Loss: 0.5091565847396851\n",
      "Epoch 23 | Batch 30 | Loss: 0.20604687929153442\n",
      "Epoch 23 | Batch 31 | Loss: 0.3701004385948181\n",
      "Epoch 23 | Batch 32 | Loss: 0.2716180384159088\n",
      "Epoch 23 | Batch 33 | Loss: 0.1749066561460495\n",
      "Epoch 23 | Batch 34 | Loss: 0.5635380744934082\n",
      "Epoch 23 | Batch 35 | Loss: 0.6680536270141602\n",
      "Epoch 23 | Batch 36 | Loss: 0.3815421760082245\n",
      "Epoch 23 | Batch 37 | Loss: 0.5147607326507568\n",
      "Epoch 23 | Batch 38 | Loss: 0.4119826555252075\n",
      "Epoch 23 | Batch 39 | Loss: 0.26263684034347534\n",
      "Epoch 23 | Batch 40 | Loss: 0.4388886094093323\n",
      "Epoch 23 | Batch 41 | Loss: 0.39600393176078796\n",
      "Epoch 23 | Batch 42 | Loss: 0.3653241991996765\n",
      "Epoch 23 | Batch 43 | Loss: 0.721339225769043\n",
      "Epoch 23 | Batch 44 | Loss: 0.47198131680488586\n",
      "Epoch 23 | Batch 45 | Loss: 0.2975236773490906\n",
      "Epoch 23 | Batch 46 | Loss: 0.15407535433769226\n",
      "Epoch 23 | Batch 47 | Loss: 0.29523327946662903\n",
      "Epoch 23 | Batch 48 | Loss: 0.42086607217788696\n",
      "Epoch 23 | Batch 49 | Loss: 0.39775604009628296\n",
      "Epoch 23 | Batch 50 | Loss: 0.27925968170166016\n",
      "Epoch 23 | Batch 51 | Loss: 0.47649911046028137\n",
      "Epoch 23 | Batch 52 | Loss: 0.7042402625083923\n",
      "Epoch 23 | Batch 53 | Loss: 0.10989055037498474\n",
      "Epoch 23 | Batch 54 | Loss: 0.5434210896492004\n",
      "Epoch 23 | Batch 55 | Loss: 0.5087484121322632\n",
      "Epoch 23 | Batch 56 | Loss: 0.13016527891159058\n",
      "Epoch 23 | Batch 57 | Loss: 0.1233750581741333\n",
      "Epoch 23 | Batch 58 | Loss: 0.1766466498374939\n",
      "Epoch 23 | Batch 59 | Loss: 0.2717434763908386\n",
      "Epoch 23 | Batch 60 | Loss: 0.11244916915893555\n",
      "Epoch 23 | Batch 61 | Loss: 0.2831636965274811\n",
      "Epoch 23 | Batch 62 | Loss: 0.25342220067977905\n",
      "Epoch 23 | Batch 63 | Loss: 0.48005610704421997\n",
      "Epoch 23 | Batch 64 | Loss: 0.19528448581695557\n",
      "Epoch 23 | Batch 65 | Loss: 0.6672148704528809\n",
      "Epoch 23 | Batch 66 | Loss: 0.17041130363941193\n",
      "Epoch 23 | Batch 67 | Loss: 0.4436687231063843\n",
      "Epoch 23 | Batch 68 | Loss: 0.37494468688964844\n",
      "Epoch 23 | Batch 69 | Loss: 0.5699694752693176\n",
      "Epoch 23 | Batch 70 | Loss: 0.12414029240608215\n",
      "Epoch 23 | Batch 71 | Loss: 0.5681995153427124\n",
      "Epoch 23 | Batch 72 | Loss: 0.7344306707382202\n",
      "Epoch 23 | Batch 73 | Loss: 0.6833271980285645\n",
      "Epoch 23 | Batch 74 | Loss: 0.09221300482749939\n",
      "Epoch 23 | Batch 75 | Loss: 0.3606913387775421\n",
      "Epoch 23 | Batch 76 | Loss: 0.4404464066028595\n",
      "Epoch 23 | Batch 77 | Loss: 0.14748623967170715\n",
      "Epoch 23 | Batch 78 | Loss: 0.6060954928398132\n",
      "Epoch 23 | Batch 79 | Loss: 0.2220238298177719\n",
      "Epoch 23 | Batch 80 | Loss: 0.17932957410812378\n",
      "Epoch 23 | Batch 81 | Loss: 0.940172553062439\n",
      "Epoch 23 | Batch 82 | Loss: 0.10468094795942307\n",
      "Epoch 23 | Batch 83 | Loss: 0.2699146866798401\n",
      "Epoch 23 | Batch 84 | Loss: 0.3347657024860382\n",
      "Epoch 23 | Batch 85 | Loss: 0.4103660583496094\n",
      "Epoch 23 | Batch 86 | Loss: 0.25092750787734985\n",
      "Epoch 23 | Batch 87 | Loss: 0.1618265062570572\n",
      "Epoch 23 | Batch 88 | Loss: 0.20743387937545776\n",
      "Epoch 23 | Batch 89 | Loss: 0.47008460760116577\n",
      "Epoch 23 | Batch 90 | Loss: 0.23003321886062622\n",
      "Epoch 24 | Batch 1 | Loss: 0.2458377182483673\n",
      "Epoch 24 | Batch 2 | Loss: 0.39914295077323914\n",
      "Epoch 24 | Batch 3 | Loss: 0.35842493176460266\n",
      "Epoch 24 | Batch 4 | Loss: 0.21741200983524323\n",
      "Epoch 24 | Batch 5 | Loss: 0.7379769086837769\n",
      "Epoch 24 | Batch 6 | Loss: 0.45318394899368286\n",
      "Epoch 24 | Batch 7 | Loss: 0.27881723642349243\n",
      "Epoch 24 | Batch 8 | Loss: 0.21496117115020752\n",
      "Epoch 24 | Batch 9 | Loss: 0.4051033854484558\n",
      "Epoch 24 | Batch 10 | Loss: 0.3359467685222626\n",
      "Epoch 24 | Batch 11 | Loss: 0.40946337580680847\n",
      "Epoch 24 | Batch 12 | Loss: 0.5055444836616516\n",
      "Epoch 24 | Batch 13 | Loss: 0.45743227005004883\n",
      "Epoch 24 | Batch 14 | Loss: 0.34918510913848877\n",
      "Epoch 24 | Batch 15 | Loss: 0.16703860461711884\n",
      "Epoch 24 | Batch 16 | Loss: 0.6503947973251343\n",
      "Epoch 24 | Batch 17 | Loss: 0.315559983253479\n",
      "Epoch 24 | Batch 18 | Loss: 0.6432076692581177\n",
      "Epoch 24 | Batch 19 | Loss: 0.22354376316070557\n",
      "Epoch 24 | Batch 20 | Loss: 0.10990475863218307\n",
      "Epoch 24 | Batch 21 | Loss: 0.32012203335762024\n",
      "Epoch 24 | Batch 22 | Loss: 0.43781793117523193\n",
      "Epoch 24 | Batch 23 | Loss: 0.5349284410476685\n",
      "Epoch 24 | Batch 24 | Loss: 0.4814213514328003\n",
      "Epoch 24 | Batch 25 | Loss: 0.3414846658706665\n",
      "Epoch 24 | Batch 26 | Loss: 0.24009700119495392\n",
      "Epoch 24 | Batch 27 | Loss: 0.32105541229248047\n",
      "Epoch 24 | Batch 28 | Loss: 0.2924807071685791\n",
      "Epoch 24 | Batch 29 | Loss: 0.1973620057106018\n",
      "Epoch 24 | Batch 30 | Loss: 0.2804187834262848\n",
      "Epoch 24 | Batch 31 | Loss: 0.18843966722488403\n",
      "Epoch 24 | Batch 32 | Loss: 0.3212262988090515\n",
      "Epoch 24 | Batch 33 | Loss: 0.49189168214797974\n",
      "Epoch 24 | Batch 34 | Loss: 0.5804249048233032\n",
      "Epoch 24 | Batch 35 | Loss: 0.16465066373348236\n",
      "Epoch 24 | Batch 36 | Loss: 0.26119935512542725\n",
      "Epoch 24 | Batch 37 | Loss: 0.21293112635612488\n",
      "Epoch 24 | Batch 38 | Loss: 0.5638409852981567\n",
      "Epoch 24 | Batch 39 | Loss: 0.5923815369606018\n",
      "Epoch 24 | Batch 40 | Loss: 0.44882315397262573\n",
      "Epoch 24 | Batch 41 | Loss: 0.17792341113090515\n",
      "Epoch 24 | Batch 42 | Loss: 0.5732786059379578\n",
      "Epoch 24 | Batch 43 | Loss: 0.25761139392852783\n",
      "Epoch 24 | Batch 44 | Loss: 0.09920525550842285\n",
      "Epoch 24 | Batch 45 | Loss: 0.24418318271636963\n",
      "Epoch 24 | Batch 46 | Loss: 0.15231123566627502\n",
      "Epoch 24 | Batch 47 | Loss: 0.5219487547874451\n",
      "Epoch 24 | Batch 48 | Loss: 0.5953707098960876\n",
      "Epoch 24 | Batch 49 | Loss: 0.450897216796875\n",
      "Epoch 24 | Batch 50 | Loss: 0.07665382325649261\n",
      "Epoch 24 | Batch 51 | Loss: 0.6013876795768738\n",
      "Epoch 24 | Batch 52 | Loss: 0.24604752659797668\n",
      "Epoch 24 | Batch 53 | Loss: 0.18052923679351807\n",
      "Epoch 24 | Batch 54 | Loss: 0.40049898624420166\n",
      "Epoch 24 | Batch 55 | Loss: 0.4632769227027893\n",
      "Epoch 24 | Batch 56 | Loss: 0.44545313715934753\n",
      "Epoch 24 | Batch 57 | Loss: 0.1568426787853241\n",
      "Epoch 24 | Batch 58 | Loss: 0.7066527009010315\n",
      "Epoch 24 | Batch 59 | Loss: 0.20144394040107727\n",
      "Epoch 24 | Batch 60 | Loss: 0.14793264865875244\n",
      "Epoch 24 | Batch 61 | Loss: 0.21682803332805634\n",
      "Epoch 24 | Batch 62 | Loss: 0.5438927412033081\n",
      "Epoch 24 | Batch 63 | Loss: 0.37929826974868774\n",
      "Epoch 24 | Batch 64 | Loss: 0.155854731798172\n",
      "Epoch 24 | Batch 65 | Loss: 0.5408974289894104\n",
      "Epoch 24 | Batch 66 | Loss: 0.33969783782958984\n",
      "Epoch 24 | Batch 67 | Loss: 0.3482942283153534\n",
      "Epoch 24 | Batch 68 | Loss: 0.436755508184433\n",
      "Epoch 24 | Batch 69 | Loss: 0.29938173294067383\n",
      "Epoch 24 | Batch 70 | Loss: 0.13556107878684998\n",
      "Epoch 24 | Batch 71 | Loss: 0.552071213722229\n",
      "Epoch 24 | Batch 72 | Loss: 0.3660160303115845\n",
      "Epoch 24 | Batch 73 | Loss: 0.4803934097290039\n",
      "Epoch 24 | Batch 74 | Loss: 0.279147207736969\n",
      "Epoch 24 | Batch 75 | Loss: 0.10363531112670898\n",
      "Epoch 24 | Batch 76 | Loss: 0.5158465504646301\n",
      "Epoch 24 | Batch 77 | Loss: 0.11148817092180252\n",
      "Epoch 24 | Batch 78 | Loss: 0.4001777172088623\n",
      "Epoch 24 | Batch 79 | Loss: 0.2591434121131897\n",
      "Epoch 24 | Batch 80 | Loss: 0.5516316294670105\n",
      "Epoch 24 | Batch 81 | Loss: 0.4321799874305725\n",
      "Epoch 24 | Batch 82 | Loss: 0.4852200746536255\n",
      "Epoch 24 | Batch 83 | Loss: 0.4430661201477051\n",
      "Epoch 24 | Batch 84 | Loss: 0.8185693621635437\n",
      "Epoch 24 | Batch 85 | Loss: 0.6140550374984741\n",
      "Epoch 24 | Batch 86 | Loss: 0.4959380030632019\n",
      "Epoch 24 | Batch 87 | Loss: 0.4865497946739197\n",
      "Epoch 24 | Batch 88 | Loss: 0.258152574300766\n",
      "Epoch 24 | Batch 89 | Loss: 0.6845142245292664\n",
      "Epoch 24 | Batch 90 | Loss: 0.15477623045444489\n",
      "Epoch 25 | Batch 1 | Loss: 0.11891444027423859\n",
      "Epoch 25 | Batch 2 | Loss: 0.8802664279937744\n",
      "Epoch 25 | Batch 3 | Loss: 0.2545556426048279\n",
      "Epoch 25 | Batch 4 | Loss: 0.3418360948562622\n",
      "Epoch 25 | Batch 5 | Loss: 0.2580939531326294\n",
      "Epoch 25 | Batch 6 | Loss: 0.6197993159294128\n",
      "Epoch 25 | Batch 7 | Loss: 0.6695766448974609\n",
      "Epoch 25 | Batch 8 | Loss: 0.19628088176250458\n",
      "Epoch 25 | Batch 9 | Loss: 0.3759092390537262\n",
      "Epoch 25 | Batch 10 | Loss: 0.194478839635849\n",
      "Epoch 25 | Batch 11 | Loss: 0.7896767258644104\n",
      "Epoch 25 | Batch 12 | Loss: 0.21431580185890198\n",
      "Epoch 25 | Batch 13 | Loss: 0.6058250665664673\n",
      "Epoch 25 | Batch 14 | Loss: 0.3843510150909424\n",
      "Epoch 25 | Batch 15 | Loss: 0.3030138909816742\n",
      "Epoch 25 | Batch 16 | Loss: 0.5769513249397278\n",
      "Epoch 25 | Batch 17 | Loss: 0.6981385946273804\n",
      "Epoch 25 | Batch 18 | Loss: 0.07710352540016174\n",
      "Epoch 25 | Batch 19 | Loss: 0.14014798402786255\n",
      "Epoch 25 | Batch 20 | Loss: 0.5012087821960449\n",
      "Epoch 25 | Batch 21 | Loss: 0.7123628854751587\n",
      "Epoch 25 | Batch 22 | Loss: 0.6776862144470215\n",
      "Epoch 25 | Batch 23 | Loss: 0.4315391778945923\n",
      "Epoch 25 | Batch 24 | Loss: 0.37056437134742737\n",
      "Epoch 25 | Batch 25 | Loss: 0.14065095782279968\n",
      "Epoch 25 | Batch 26 | Loss: 0.6402184963226318\n",
      "Epoch 25 | Batch 27 | Loss: 0.27176064252853394\n",
      "Epoch 25 | Batch 28 | Loss: 0.2813066244125366\n",
      "Epoch 25 | Batch 29 | Loss: 0.5384243726730347\n",
      "Epoch 25 | Batch 30 | Loss: 0.47548729181289673\n",
      "Epoch 25 | Batch 31 | Loss: 0.24522683024406433\n",
      "Epoch 25 | Batch 32 | Loss: 0.41162848472595215\n",
      "Epoch 25 | Batch 33 | Loss: 0.4424518048763275\n",
      "Epoch 25 | Batch 34 | Loss: 0.37539270520210266\n",
      "Epoch 25 | Batch 35 | Loss: 0.35750293731689453\n",
      "Epoch 25 | Batch 36 | Loss: 0.5023667812347412\n",
      "Epoch 25 | Batch 37 | Loss: 0.1536935418844223\n",
      "Epoch 25 | Batch 38 | Loss: 0.23831552267074585\n",
      "Epoch 25 | Batch 39 | Loss: 0.5085744857788086\n",
      "Epoch 25 | Batch 40 | Loss: 0.5719022750854492\n",
      "Epoch 25 | Batch 41 | Loss: 0.18804700672626495\n",
      "Epoch 25 | Batch 42 | Loss: 0.5071966052055359\n",
      "Epoch 25 | Batch 43 | Loss: 0.44183021783828735\n",
      "Epoch 25 | Batch 44 | Loss: 0.18999096751213074\n",
      "Epoch 25 | Batch 45 | Loss: 0.3669671416282654\n",
      "Epoch 25 | Batch 46 | Loss: 0.3175181746482849\n",
      "Epoch 25 | Batch 47 | Loss: 0.44720426201820374\n",
      "Epoch 25 | Batch 48 | Loss: 0.3164154291152954\n",
      "Epoch 25 | Batch 49 | Loss: 0.196772962808609\n",
      "Epoch 25 | Batch 50 | Loss: 0.15389008820056915\n",
      "Epoch 25 | Batch 51 | Loss: 0.27325528860092163\n",
      "Epoch 25 | Batch 52 | Loss: 0.24038167297840118\n",
      "Epoch 25 | Batch 53 | Loss: 0.18835091590881348\n",
      "Epoch 25 | Batch 54 | Loss: 0.6854025721549988\n",
      "Epoch 25 | Batch 55 | Loss: 0.4820217490196228\n",
      "Epoch 25 | Batch 56 | Loss: 0.19232550263404846\n",
      "Epoch 25 | Batch 57 | Loss: 0.3553633689880371\n",
      "Epoch 25 | Batch 58 | Loss: 0.2962379455566406\n",
      "Epoch 25 | Batch 59 | Loss: 0.4686140716075897\n",
      "Epoch 25 | Batch 60 | Loss: 0.04568786174058914\n",
      "Epoch 25 | Batch 61 | Loss: 0.32578179240226746\n",
      "Epoch 25 | Batch 62 | Loss: 0.3826226592063904\n",
      "Epoch 25 | Batch 63 | Loss: 0.5901209115982056\n",
      "Epoch 25 | Batch 64 | Loss: 0.6519635915756226\n",
      "Epoch 25 | Batch 65 | Loss: 0.28336790204048157\n",
      "Epoch 25 | Batch 66 | Loss: 0.21057718992233276\n",
      "Epoch 25 | Batch 67 | Loss: 0.4095337986946106\n",
      "Epoch 25 | Batch 68 | Loss: 0.10293099284172058\n",
      "Epoch 25 | Batch 69 | Loss: 0.137125626206398\n",
      "Epoch 25 | Batch 70 | Loss: 0.12873771786689758\n",
      "Epoch 25 | Batch 71 | Loss: 0.42705315351486206\n",
      "Epoch 25 | Batch 72 | Loss: 0.34655892848968506\n",
      "Epoch 25 | Batch 73 | Loss: 0.7602180242538452\n",
      "Epoch 25 | Batch 74 | Loss: 0.3083587884902954\n",
      "Epoch 25 | Batch 75 | Loss: 0.40090644359588623\n",
      "Epoch 25 | Batch 76 | Loss: 0.6450983285903931\n",
      "Epoch 25 | Batch 77 | Loss: 0.3564101457595825\n",
      "Epoch 25 | Batch 78 | Loss: 0.4748157560825348\n",
      "Epoch 25 | Batch 79 | Loss: 0.20368535816669464\n",
      "Epoch 25 | Batch 80 | Loss: 0.3515606224536896\n",
      "Epoch 25 | Batch 81 | Loss: 0.22377890348434448\n",
      "Epoch 25 | Batch 82 | Loss: 0.16737355291843414\n",
      "Epoch 25 | Batch 83 | Loss: 0.4528006315231323\n",
      "Epoch 25 | Batch 84 | Loss: 0.1931433081626892\n",
      "Epoch 25 | Batch 85 | Loss: 0.15804210305213928\n",
      "Epoch 25 | Batch 86 | Loss: 0.2737068235874176\n",
      "Epoch 25 | Batch 87 | Loss: 0.7337729930877686\n",
      "Epoch 25 | Batch 88 | Loss: 0.24159371852874756\n",
      "Epoch 25 | Batch 89 | Loss: 0.318499356508255\n",
      "Epoch 25 | Batch 90 | Loss: 0.7484990358352661\n",
      "Epoch 26 | Batch 1 | Loss: 0.6253156661987305\n",
      "Epoch 26 | Batch 2 | Loss: 0.2237352728843689\n",
      "Epoch 26 | Batch 3 | Loss: 0.17791061103343964\n",
      "Epoch 26 | Batch 4 | Loss: 0.5233269333839417\n",
      "Epoch 26 | Batch 5 | Loss: 0.8583935499191284\n",
      "Epoch 26 | Batch 6 | Loss: 0.447923481464386\n",
      "Epoch 26 | Batch 7 | Loss: 0.5298718214035034\n",
      "Epoch 26 | Batch 8 | Loss: 0.7364257574081421\n",
      "Epoch 26 | Batch 9 | Loss: 0.5252006649971008\n",
      "Epoch 26 | Batch 10 | Loss: 0.30479881167411804\n",
      "Epoch 26 | Batch 11 | Loss: 0.17835745215415955\n",
      "Epoch 26 | Batch 12 | Loss: 0.45997312664985657\n",
      "Epoch 26 | Batch 13 | Loss: 0.3482721447944641\n",
      "Epoch 26 | Batch 14 | Loss: 0.15615320205688477\n",
      "Epoch 26 | Batch 15 | Loss: 0.47936031222343445\n",
      "Epoch 26 | Batch 16 | Loss: 0.529940664768219\n",
      "Epoch 26 | Batch 17 | Loss: 0.17499765753746033\n",
      "Epoch 26 | Batch 18 | Loss: 0.7917325496673584\n",
      "Epoch 26 | Batch 19 | Loss: 0.24393102526664734\n",
      "Epoch 26 | Batch 20 | Loss: 0.35887569189071655\n",
      "Epoch 26 | Batch 21 | Loss: 0.3578965663909912\n",
      "Epoch 26 | Batch 22 | Loss: 0.13645341992378235\n",
      "Epoch 26 | Batch 23 | Loss: 0.27210941910743713\n",
      "Epoch 26 | Batch 24 | Loss: 0.563513994216919\n",
      "Epoch 26 | Batch 25 | Loss: 0.31376171112060547\n",
      "Epoch 26 | Batch 26 | Loss: 0.4934353232383728\n",
      "Epoch 26 | Batch 27 | Loss: 0.26226168870925903\n",
      "Epoch 26 | Batch 28 | Loss: 0.25127872824668884\n",
      "Epoch 26 | Batch 29 | Loss: 0.2659347355365753\n",
      "Epoch 26 | Batch 30 | Loss: 0.5384435057640076\n",
      "Epoch 26 | Batch 31 | Loss: 0.6735050082206726\n",
      "Epoch 26 | Batch 32 | Loss: 0.35652488470077515\n",
      "Epoch 26 | Batch 33 | Loss: 0.15019086003303528\n",
      "Epoch 26 | Batch 34 | Loss: 0.2747932970523834\n",
      "Epoch 26 | Batch 35 | Loss: 0.741764485836029\n",
      "Epoch 26 | Batch 36 | Loss: 0.3197624981403351\n",
      "Epoch 26 | Batch 37 | Loss: 0.6078803539276123\n",
      "Epoch 26 | Batch 38 | Loss: 0.37512120604515076\n",
      "Epoch 26 | Batch 39 | Loss: 0.16334158182144165\n",
      "Epoch 26 | Batch 40 | Loss: 0.3416435122489929\n",
      "Epoch 26 | Batch 41 | Loss: 0.16978241503238678\n",
      "Epoch 26 | Batch 42 | Loss: 0.295531690120697\n",
      "Epoch 26 | Batch 43 | Loss: 0.3829807639122009\n",
      "Epoch 26 | Batch 44 | Loss: 0.3369218707084656\n",
      "Epoch 26 | Batch 45 | Loss: 0.793612003326416\n",
      "Epoch 26 | Batch 46 | Loss: 0.315912127494812\n",
      "Epoch 26 | Batch 47 | Loss: 0.4796038866043091\n",
      "Epoch 26 | Batch 48 | Loss: 0.14555910229682922\n",
      "Epoch 26 | Batch 49 | Loss: 0.3007073402404785\n",
      "Epoch 26 | Batch 50 | Loss: 0.20076167583465576\n",
      "Epoch 26 | Batch 51 | Loss: 0.38826483488082886\n",
      "Epoch 26 | Batch 52 | Loss: 0.43981948494911194\n",
      "Epoch 26 | Batch 53 | Loss: 1.3300875425338745\n",
      "Epoch 26 | Batch 54 | Loss: 0.15295714139938354\n",
      "Epoch 26 | Batch 55 | Loss: 0.16851374506950378\n",
      "Epoch 26 | Batch 56 | Loss: 0.1361597776412964\n",
      "Epoch 26 | Batch 57 | Loss: 0.21062523126602173\n",
      "Epoch 26 | Batch 58 | Loss: 0.20235025882720947\n",
      "Epoch 26 | Batch 59 | Loss: 0.11718729138374329\n",
      "Epoch 26 | Batch 60 | Loss: 0.6089743375778198\n",
      "Epoch 26 | Batch 61 | Loss: 0.5443601608276367\n",
      "Epoch 26 | Batch 62 | Loss: 0.13719817996025085\n",
      "Epoch 26 | Batch 63 | Loss: 0.2484612911939621\n",
      "Epoch 26 | Batch 64 | Loss: 0.5769798755645752\n",
      "Epoch 26 | Batch 65 | Loss: 0.3473834991455078\n",
      "Epoch 26 | Batch 66 | Loss: 0.3210718035697937\n",
      "Epoch 26 | Batch 67 | Loss: 0.34933897852897644\n",
      "Epoch 26 | Batch 68 | Loss: 0.35212838649749756\n",
      "Epoch 26 | Batch 69 | Loss: 0.28566545248031616\n",
      "Epoch 26 | Batch 70 | Loss: 0.40014463663101196\n",
      "Epoch 26 | Batch 71 | Loss: 0.5069735050201416\n",
      "Epoch 26 | Batch 72 | Loss: 0.46595102548599243\n",
      "Epoch 26 | Batch 73 | Loss: 0.31045982241630554\n",
      "Epoch 26 | Batch 74 | Loss: 1.0042318105697632\n",
      "Epoch 26 | Batch 75 | Loss: 0.16257891058921814\n",
      "Epoch 26 | Batch 76 | Loss: 0.20809558033943176\n",
      "Epoch 26 | Batch 77 | Loss: 0.14688074588775635\n",
      "Epoch 26 | Batch 78 | Loss: 0.14845535159111023\n",
      "Epoch 26 | Batch 79 | Loss: 0.5453805923461914\n",
      "Epoch 26 | Batch 80 | Loss: 0.34636521339416504\n",
      "Epoch 26 | Batch 81 | Loss: 0.3688182830810547\n",
      "Epoch 26 | Batch 82 | Loss: 0.40336811542510986\n",
      "Epoch 26 | Batch 83 | Loss: 0.36929306387901306\n",
      "Epoch 26 | Batch 84 | Loss: 0.6575589179992676\n",
      "Epoch 26 | Batch 85 | Loss: 0.275593101978302\n",
      "Epoch 26 | Batch 86 | Loss: 0.09824372082948685\n",
      "Epoch 26 | Batch 87 | Loss: 0.35964810848236084\n",
      "Epoch 26 | Batch 88 | Loss: 0.394788533449173\n",
      "Epoch 26 | Batch 89 | Loss: 0.0904100239276886\n",
      "Epoch 26 | Batch 90 | Loss: 0.09467636048793793\n",
      "Epoch 27 | Batch 1 | Loss: 0.1446121782064438\n",
      "Epoch 27 | Batch 2 | Loss: 0.42118242383003235\n",
      "Epoch 27 | Batch 3 | Loss: 0.5470248460769653\n",
      "Epoch 27 | Batch 4 | Loss: 0.22679655253887177\n",
      "Epoch 27 | Batch 5 | Loss: 0.3488314747810364\n",
      "Epoch 27 | Batch 6 | Loss: 0.17484822869300842\n",
      "Epoch 27 | Batch 7 | Loss: 0.4693281650543213\n",
      "Epoch 27 | Batch 8 | Loss: 0.4500192403793335\n",
      "Epoch 27 | Batch 9 | Loss: 0.4274352192878723\n",
      "Epoch 27 | Batch 10 | Loss: 0.3217167854309082\n",
      "Epoch 27 | Batch 11 | Loss: 0.6530243158340454\n",
      "Epoch 27 | Batch 12 | Loss: 0.5398925542831421\n",
      "Epoch 27 | Batch 13 | Loss: 0.2465602606534958\n",
      "Epoch 27 | Batch 14 | Loss: 0.1376197189092636\n",
      "Epoch 27 | Batch 15 | Loss: 0.4923366904258728\n",
      "Epoch 27 | Batch 16 | Loss: 0.810589075088501\n",
      "Epoch 27 | Batch 17 | Loss: 0.4502015709877014\n",
      "Epoch 27 | Batch 18 | Loss: 0.3125714063644409\n",
      "Epoch 27 | Batch 19 | Loss: 0.49683958292007446\n",
      "Epoch 27 | Batch 20 | Loss: 0.2092859148979187\n",
      "Epoch 27 | Batch 21 | Loss: 0.17572687566280365\n",
      "Epoch 27 | Batch 22 | Loss: 0.10583750903606415\n",
      "Epoch 27 | Batch 23 | Loss: 0.321189820766449\n",
      "Epoch 27 | Batch 24 | Loss: 0.6662781834602356\n",
      "Epoch 27 | Batch 25 | Loss: 0.3111603558063507\n",
      "Epoch 27 | Batch 26 | Loss: 0.2576075792312622\n",
      "Epoch 27 | Batch 27 | Loss: 0.3468438982963562\n",
      "Epoch 27 | Batch 28 | Loss: 0.2711334228515625\n",
      "Epoch 27 | Batch 29 | Loss: 0.19903917610645294\n",
      "Epoch 27 | Batch 30 | Loss: 1.0215544700622559\n",
      "Epoch 27 | Batch 31 | Loss: 0.37086501717567444\n",
      "Epoch 27 | Batch 32 | Loss: 0.21226494014263153\n",
      "Epoch 27 | Batch 33 | Loss: 0.17918410897254944\n",
      "Epoch 27 | Batch 34 | Loss: 0.5459414720535278\n",
      "Epoch 27 | Batch 35 | Loss: 0.45602500438690186\n",
      "Epoch 27 | Batch 36 | Loss: 0.9645440578460693\n",
      "Epoch 27 | Batch 37 | Loss: 0.3478899598121643\n",
      "Epoch 27 | Batch 38 | Loss: 0.3437296152114868\n",
      "Epoch 27 | Batch 39 | Loss: 0.7311235666275024\n",
      "Epoch 27 | Batch 40 | Loss: 0.17360927164554596\n",
      "Epoch 27 | Batch 41 | Loss: 0.15064099431037903\n",
      "Epoch 27 | Batch 42 | Loss: 0.28771844506263733\n",
      "Epoch 27 | Batch 43 | Loss: 0.13547095656394958\n",
      "Epoch 27 | Batch 44 | Loss: 0.10851965844631195\n",
      "Epoch 27 | Batch 45 | Loss: 0.4288628101348877\n",
      "Epoch 27 | Batch 46 | Loss: 0.20573900640010834\n",
      "Epoch 27 | Batch 47 | Loss: 0.3484453558921814\n",
      "Epoch 27 | Batch 48 | Loss: 0.2662605047225952\n",
      "Epoch 27 | Batch 49 | Loss: 0.07258877158164978\n",
      "Epoch 27 | Batch 50 | Loss: 0.30031147599220276\n",
      "Epoch 27 | Batch 51 | Loss: 0.3973469138145447\n",
      "Epoch 27 | Batch 52 | Loss: 0.42564716935157776\n",
      "Epoch 27 | Batch 53 | Loss: 0.2628413736820221\n",
      "Epoch 27 | Batch 54 | Loss: 0.4132525324821472\n",
      "Epoch 27 | Batch 55 | Loss: 0.12899678945541382\n",
      "Epoch 27 | Batch 56 | Loss: 0.3008062541484833\n",
      "Epoch 27 | Batch 57 | Loss: 0.30034172534942627\n",
      "Epoch 27 | Batch 58 | Loss: 0.16139233112335205\n",
      "Epoch 27 | Batch 59 | Loss: 0.9181346893310547\n",
      "Epoch 27 | Batch 60 | Loss: 0.1367713212966919\n",
      "Epoch 27 | Batch 61 | Loss: 0.6951408386230469\n",
      "Epoch 27 | Batch 62 | Loss: 0.0822097510099411\n",
      "Epoch 27 | Batch 63 | Loss: 0.6460720896720886\n",
      "Epoch 27 | Batch 64 | Loss: 0.6095218658447266\n",
      "Epoch 27 | Batch 65 | Loss: 0.7820448279380798\n",
      "Epoch 27 | Batch 66 | Loss: 0.18705055117607117\n",
      "Epoch 27 | Batch 67 | Loss: 0.6175752282142639\n",
      "Epoch 27 | Batch 68 | Loss: 0.15667608380317688\n",
      "Epoch 27 | Batch 69 | Loss: 0.6266903877258301\n",
      "Epoch 27 | Batch 70 | Loss: 0.10316929221153259\n",
      "Epoch 27 | Batch 71 | Loss: 0.8515851497650146\n",
      "Epoch 27 | Batch 72 | Loss: 0.35327035188674927\n",
      "Epoch 27 | Batch 73 | Loss: 0.10311141610145569\n",
      "Epoch 27 | Batch 74 | Loss: 0.3219657838344574\n",
      "Epoch 27 | Batch 75 | Loss: 0.26488229632377625\n",
      "Epoch 27 | Batch 76 | Loss: 0.6238953471183777\n",
      "Epoch 27 | Batch 77 | Loss: 0.5072824954986572\n",
      "Epoch 27 | Batch 78 | Loss: 0.6115731596946716\n",
      "Epoch 27 | Batch 79 | Loss: 0.12484882771968842\n",
      "Epoch 27 | Batch 80 | Loss: 0.5743319392204285\n",
      "Epoch 27 | Batch 81 | Loss: 0.28913164138793945\n",
      "Epoch 27 | Batch 82 | Loss: 0.11667922139167786\n",
      "Epoch 27 | Batch 83 | Loss: 0.21510246396064758\n",
      "Epoch 27 | Batch 84 | Loss: 0.43923455476760864\n",
      "Epoch 27 | Batch 85 | Loss: 0.1801529824733734\n",
      "Epoch 27 | Batch 86 | Loss: 0.43703246116638184\n",
      "Epoch 27 | Batch 87 | Loss: 0.20949076116085052\n",
      "Epoch 27 | Batch 88 | Loss: 0.5953382253646851\n",
      "Epoch 27 | Batch 89 | Loss: 0.34227606654167175\n",
      "Epoch 27 | Batch 90 | Loss: 1.3113030945532955e-06\n",
      "Epoch 28 | Batch 1 | Loss: 0.3565581142902374\n",
      "Epoch 28 | Batch 2 | Loss: 0.3369750380516052\n",
      "Epoch 28 | Batch 3 | Loss: 0.08045060932636261\n",
      "Epoch 28 | Batch 4 | Loss: 0.15525788068771362\n",
      "Epoch 28 | Batch 5 | Loss: 0.3371429443359375\n",
      "Epoch 28 | Batch 6 | Loss: 0.3792223036289215\n",
      "Epoch 28 | Batch 7 | Loss: 0.4175017178058624\n",
      "Epoch 28 | Batch 8 | Loss: 0.3930102288722992\n",
      "Epoch 28 | Batch 9 | Loss: 0.3619743585586548\n",
      "Epoch 28 | Batch 10 | Loss: 0.19996728003025055\n",
      "Epoch 28 | Batch 11 | Loss: 0.42278146743774414\n",
      "Epoch 28 | Batch 12 | Loss: 0.09016583859920502\n",
      "Epoch 28 | Batch 13 | Loss: 0.5386574268341064\n",
      "Epoch 28 | Batch 14 | Loss: 0.05128674954175949\n",
      "Epoch 28 | Batch 15 | Loss: 0.20858661830425262\n",
      "Epoch 28 | Batch 16 | Loss: 0.19011875987052917\n",
      "Epoch 28 | Batch 17 | Loss: 0.30136486887931824\n",
      "Epoch 28 | Batch 18 | Loss: 0.1500193178653717\n",
      "Epoch 28 | Batch 19 | Loss: 0.3857855796813965\n",
      "Epoch 28 | Batch 20 | Loss: 0.7505802512168884\n",
      "Epoch 28 | Batch 21 | Loss: 0.7454373836517334\n",
      "Epoch 28 | Batch 22 | Loss: 0.15304574370384216\n",
      "Epoch 28 | Batch 23 | Loss: 0.12293019890785217\n",
      "Epoch 28 | Batch 24 | Loss: 0.7744699120521545\n",
      "Epoch 28 | Batch 25 | Loss: 0.48028096556663513\n",
      "Epoch 28 | Batch 26 | Loss: 0.5016633868217468\n",
      "Epoch 28 | Batch 27 | Loss: 0.2643652856349945\n",
      "Epoch 28 | Batch 28 | Loss: 0.16471019387245178\n",
      "Epoch 28 | Batch 29 | Loss: 0.5836877226829529\n",
      "Epoch 28 | Batch 30 | Loss: 0.23274633288383484\n",
      "Epoch 28 | Batch 31 | Loss: 0.2459048628807068\n",
      "Epoch 28 | Batch 32 | Loss: 0.3606654405593872\n",
      "Epoch 28 | Batch 33 | Loss: 0.4817364513874054\n",
      "Epoch 28 | Batch 34 | Loss: 0.36445528268814087\n",
      "Epoch 28 | Batch 35 | Loss: 0.3776146471500397\n",
      "Epoch 28 | Batch 36 | Loss: 0.3298702538013458\n",
      "Epoch 28 | Batch 37 | Loss: 0.4087699055671692\n",
      "Epoch 28 | Batch 38 | Loss: 0.5379621982574463\n",
      "Epoch 28 | Batch 39 | Loss: 0.4225943684577942\n",
      "Epoch 28 | Batch 40 | Loss: 0.34989726543426514\n",
      "Epoch 28 | Batch 41 | Loss: 0.4679470360279083\n",
      "Epoch 28 | Batch 42 | Loss: 0.4167339503765106\n",
      "Epoch 28 | Batch 43 | Loss: 0.4151771664619446\n",
      "Epoch 28 | Batch 44 | Loss: 0.22556692361831665\n",
      "Epoch 28 | Batch 45 | Loss: 0.1424463540315628\n",
      "Epoch 28 | Batch 46 | Loss: 0.5432018041610718\n",
      "Epoch 28 | Batch 47 | Loss: 0.5228726267814636\n",
      "Epoch 28 | Batch 48 | Loss: 0.7635806202888489\n",
      "Epoch 28 | Batch 49 | Loss: 0.46428096294403076\n",
      "Epoch 28 | Batch 50 | Loss: 0.3392433524131775\n",
      "Epoch 28 | Batch 51 | Loss: 0.5108922719955444\n",
      "Epoch 28 | Batch 52 | Loss: 0.188075989484787\n",
      "Epoch 28 | Batch 53 | Loss: 0.3510003685951233\n",
      "Epoch 28 | Batch 54 | Loss: 0.08761802315711975\n",
      "Epoch 28 | Batch 55 | Loss: 0.32050400972366333\n",
      "Epoch 28 | Batch 56 | Loss: 0.22002127766609192\n",
      "Epoch 28 | Batch 57 | Loss: 0.3995312750339508\n",
      "Epoch 28 | Batch 58 | Loss: 0.31930896639823914\n",
      "Epoch 28 | Batch 59 | Loss: 1.0310132503509521\n",
      "Epoch 28 | Batch 60 | Loss: 0.860598087310791\n",
      "Epoch 28 | Batch 61 | Loss: 0.5030128955841064\n",
      "Epoch 28 | Batch 62 | Loss: 0.9092063903808594\n",
      "Epoch 28 | Batch 63 | Loss: 0.3880593478679657\n",
      "Epoch 28 | Batch 64 | Loss: 0.3932075798511505\n",
      "Epoch 28 | Batch 65 | Loss: 0.1644425392150879\n",
      "Epoch 28 | Batch 66 | Loss: 0.2694208025932312\n",
      "Epoch 28 | Batch 67 | Loss: 0.5205404758453369\n",
      "Epoch 28 | Batch 68 | Loss: 0.15669113397598267\n",
      "Epoch 28 | Batch 69 | Loss: 0.33214908838272095\n",
      "Epoch 28 | Batch 70 | Loss: 0.11916075646877289\n",
      "Epoch 28 | Batch 71 | Loss: 0.6804512739181519\n",
      "Epoch 28 | Batch 72 | Loss: 0.6309822797775269\n",
      "Epoch 28 | Batch 73 | Loss: 0.43801090121269226\n",
      "Epoch 28 | Batch 74 | Loss: 0.07679667323827744\n",
      "Epoch 28 | Batch 75 | Loss: 0.1256096065044403\n",
      "Epoch 28 | Batch 76 | Loss: 0.31460681557655334\n",
      "Epoch 28 | Batch 77 | Loss: 0.6428670883178711\n",
      "Epoch 28 | Batch 78 | Loss: 0.35856643319129944\n",
      "Epoch 28 | Batch 79 | Loss: 0.3486381769180298\n",
      "Epoch 28 | Batch 80 | Loss: 0.18606363236904144\n",
      "Epoch 28 | Batch 81 | Loss: 1.1651206016540527\n",
      "Epoch 28 | Batch 82 | Loss: 0.3819323778152466\n",
      "Epoch 28 | Batch 83 | Loss: 0.4512943625450134\n",
      "Epoch 28 | Batch 84 | Loss: 0.295085608959198\n",
      "Epoch 28 | Batch 85 | Loss: 0.23112545907497406\n",
      "Epoch 28 | Batch 86 | Loss: 0.15212324261665344\n",
      "Epoch 28 | Batch 87 | Loss: 0.35212260484695435\n",
      "Epoch 28 | Batch 88 | Loss: 0.23751327395439148\n",
      "Epoch 28 | Batch 89 | Loss: 0.35458284616470337\n",
      "Epoch 28 | Batch 90 | Loss: 0.1762920618057251\n",
      "Epoch 29 | Batch 1 | Loss: 0.5948410034179688\n",
      "Epoch 29 | Batch 2 | Loss: 0.5224977731704712\n",
      "Epoch 29 | Batch 3 | Loss: 0.4478246569633484\n",
      "Epoch 29 | Batch 4 | Loss: 0.38059499859809875\n",
      "Epoch 29 | Batch 5 | Loss: 0.15663063526153564\n",
      "Epoch 29 | Batch 6 | Loss: 0.35942959785461426\n",
      "Epoch 29 | Batch 7 | Loss: 0.1660451889038086\n",
      "Epoch 29 | Batch 8 | Loss: 0.4680051803588867\n",
      "Epoch 29 | Batch 9 | Loss: 0.12894266843795776\n",
      "Epoch 29 | Batch 10 | Loss: 0.1569865643978119\n",
      "Epoch 29 | Batch 11 | Loss: 0.10837993025779724\n",
      "Epoch 29 | Batch 12 | Loss: 0.17149870097637177\n",
      "Epoch 29 | Batch 13 | Loss: 0.18352654576301575\n",
      "Epoch 29 | Batch 14 | Loss: 0.31942057609558105\n",
      "Epoch 29 | Batch 15 | Loss: 0.1704505831003189\n",
      "Epoch 29 | Batch 16 | Loss: 0.24960029125213623\n",
      "Epoch 29 | Batch 17 | Loss: 0.09924116730690002\n",
      "Epoch 29 | Batch 18 | Loss: 0.367136687040329\n",
      "Epoch 29 | Batch 19 | Loss: 0.07685919106006622\n",
      "Epoch 29 | Batch 20 | Loss: 0.4286603033542633\n",
      "Epoch 29 | Batch 21 | Loss: 0.24627362191677094\n",
      "Epoch 29 | Batch 22 | Loss: 0.4894495904445648\n",
      "Epoch 29 | Batch 23 | Loss: 0.28600987792015076\n",
      "Epoch 29 | Batch 24 | Loss: 0.3781348466873169\n",
      "Epoch 29 | Batch 25 | Loss: 0.2042519450187683\n",
      "Epoch 29 | Batch 26 | Loss: 0.34763169288635254\n",
      "Epoch 29 | Batch 27 | Loss: 0.533980131149292\n",
      "Epoch 29 | Batch 28 | Loss: 0.3332907557487488\n",
      "Epoch 29 | Batch 29 | Loss: 0.41738277673721313\n",
      "Epoch 29 | Batch 30 | Loss: 0.2137179970741272\n",
      "Epoch 29 | Batch 31 | Loss: 0.945406436920166\n",
      "Epoch 29 | Batch 32 | Loss: 0.2893669605255127\n",
      "Epoch 29 | Batch 33 | Loss: 0.43712419271469116\n",
      "Epoch 29 | Batch 34 | Loss: 0.2858528196811676\n",
      "Epoch 29 | Batch 35 | Loss: 0.42329537868499756\n",
      "Epoch 29 | Batch 36 | Loss: 0.6852138042449951\n",
      "Epoch 29 | Batch 37 | Loss: 0.4439389705657959\n",
      "Epoch 29 | Batch 38 | Loss: 0.7436609268188477\n",
      "Epoch 29 | Batch 39 | Loss: 0.332991361618042\n",
      "Epoch 29 | Batch 40 | Loss: 0.3315882384777069\n",
      "Epoch 29 | Batch 41 | Loss: 0.5508050322532654\n",
      "Epoch 29 | Batch 42 | Loss: 0.373110294342041\n",
      "Epoch 29 | Batch 43 | Loss: 0.2775479555130005\n",
      "Epoch 29 | Batch 44 | Loss: 0.6293577551841736\n",
      "Epoch 29 | Batch 45 | Loss: 0.1576935350894928\n",
      "Epoch 29 | Batch 46 | Loss: 0.22307375073432922\n",
      "Epoch 29 | Batch 47 | Loss: 0.6187365651130676\n",
      "Epoch 29 | Batch 48 | Loss: 0.445489764213562\n",
      "Epoch 29 | Batch 49 | Loss: 0.30036455392837524\n",
      "Epoch 29 | Batch 50 | Loss: 0.6180090308189392\n",
      "Epoch 29 | Batch 51 | Loss: 0.462590754032135\n",
      "Epoch 29 | Batch 52 | Loss: 0.9397066831588745\n",
      "Epoch 29 | Batch 53 | Loss: 0.3117080330848694\n",
      "Epoch 29 | Batch 54 | Loss: 0.2538973391056061\n",
      "Epoch 29 | Batch 55 | Loss: 0.3517332673072815\n",
      "Epoch 29 | Batch 56 | Loss: 0.7001758813858032\n",
      "Epoch 29 | Batch 57 | Loss: 0.23049919307231903\n",
      "Epoch 29 | Batch 58 | Loss: 0.602343738079071\n",
      "Epoch 29 | Batch 59 | Loss: 0.23249077796936035\n",
      "Epoch 29 | Batch 60 | Loss: 0.18067625164985657\n",
      "Epoch 29 | Batch 61 | Loss: 0.28408825397491455\n",
      "Epoch 29 | Batch 62 | Loss: 0.2127447873353958\n",
      "Epoch 29 | Batch 63 | Loss: 0.4354693293571472\n",
      "Epoch 29 | Batch 64 | Loss: 0.3678455650806427\n",
      "Epoch 29 | Batch 65 | Loss: 0.719703197479248\n",
      "Epoch 29 | Batch 66 | Loss: 0.48947131633758545\n",
      "Epoch 29 | Batch 67 | Loss: 0.10249001532793045\n",
      "Epoch 29 | Batch 68 | Loss: 0.5570154190063477\n",
      "Epoch 29 | Batch 69 | Loss: 0.2931607961654663\n",
      "Epoch 29 | Batch 70 | Loss: 0.6604900360107422\n",
      "Epoch 29 | Batch 71 | Loss: 0.25518304109573364\n",
      "Epoch 29 | Batch 72 | Loss: 0.17647844552993774\n",
      "Epoch 29 | Batch 73 | Loss: 0.16967514157295227\n",
      "Epoch 29 | Batch 74 | Loss: 0.24027864634990692\n",
      "Epoch 29 | Batch 75 | Loss: 0.19808712601661682\n",
      "Epoch 29 | Batch 76 | Loss: 0.2110499143600464\n",
      "Epoch 29 | Batch 77 | Loss: 0.171720951795578\n",
      "Epoch 29 | Batch 78 | Loss: 0.3803640604019165\n",
      "Epoch 29 | Batch 79 | Loss: 0.5244313478469849\n",
      "Epoch 29 | Batch 80 | Loss: 0.6127744913101196\n",
      "Epoch 29 | Batch 81 | Loss: 1.0231707096099854\n",
      "Epoch 29 | Batch 82 | Loss: 0.4182635545730591\n",
      "Epoch 29 | Batch 83 | Loss: 0.5989445447921753\n",
      "Epoch 29 | Batch 84 | Loss: 0.4095916152000427\n",
      "Epoch 29 | Batch 85 | Loss: 0.418143630027771\n",
      "Epoch 29 | Batch 86 | Loss: 0.10047531127929688\n",
      "Epoch 29 | Batch 87 | Loss: 0.16608771681785583\n",
      "Epoch 29 | Batch 88 | Loss: 0.41141730546951294\n",
      "Epoch 29 | Batch 89 | Loss: 0.6429525017738342\n",
      "Epoch 29 | Batch 90 | Loss: 0.252135694026947\n",
      "Epoch 30 | Batch 1 | Loss: 0.22523128986358643\n",
      "Epoch 30 | Batch 2 | Loss: 0.20512408018112183\n",
      "Epoch 30 | Batch 3 | Loss: 0.6724712252616882\n",
      "Epoch 30 | Batch 4 | Loss: 0.6058052778244019\n",
      "Epoch 30 | Batch 5 | Loss: 0.20195023715496063\n",
      "Epoch 30 | Batch 6 | Loss: 0.2221248745918274\n",
      "Epoch 30 | Batch 7 | Loss: 0.23228517174720764\n",
      "Epoch 30 | Batch 8 | Loss: 0.0952548086643219\n",
      "Epoch 30 | Batch 9 | Loss: 0.2316935956478119\n",
      "Epoch 30 | Batch 10 | Loss: 0.34961262345314026\n",
      "Epoch 30 | Batch 11 | Loss: 0.20604361593723297\n",
      "Epoch 30 | Batch 12 | Loss: 0.33074289560317993\n",
      "Epoch 30 | Batch 13 | Loss: 0.35781896114349365\n",
      "Epoch 30 | Batch 14 | Loss: 0.36591672897338867\n",
      "Epoch 30 | Batch 15 | Loss: 0.41757556796073914\n",
      "Epoch 30 | Batch 16 | Loss: 0.20178793370723724\n",
      "Epoch 30 | Batch 17 | Loss: 0.692275881767273\n",
      "Epoch 30 | Batch 18 | Loss: 0.4867921769618988\n",
      "Epoch 30 | Batch 19 | Loss: 0.5244238376617432\n",
      "Epoch 30 | Batch 20 | Loss: 0.6869384050369263\n",
      "Epoch 30 | Batch 21 | Loss: 0.14306461811065674\n",
      "Epoch 30 | Batch 22 | Loss: 0.7413748502731323\n",
      "Epoch 30 | Batch 23 | Loss: 0.5976415872573853\n",
      "Epoch 30 | Batch 24 | Loss: 0.2566242218017578\n",
      "Epoch 30 | Batch 25 | Loss: 0.19041568040847778\n",
      "Epoch 30 | Batch 26 | Loss: 0.4519742429256439\n",
      "Epoch 30 | Batch 27 | Loss: 0.3071063160896301\n",
      "Epoch 30 | Batch 28 | Loss: 0.46410149335861206\n",
      "Epoch 30 | Batch 29 | Loss: 0.40013331174850464\n",
      "Epoch 30 | Batch 30 | Loss: 0.2581903338432312\n",
      "Epoch 30 | Batch 31 | Loss: 0.3690131902694702\n",
      "Epoch 30 | Batch 32 | Loss: 0.33833345770835876\n",
      "Epoch 30 | Batch 33 | Loss: 0.3325226902961731\n",
      "Epoch 30 | Batch 34 | Loss: 0.5334899425506592\n",
      "Epoch 30 | Batch 35 | Loss: 0.2982156574726105\n",
      "Epoch 30 | Batch 36 | Loss: 0.2747696042060852\n",
      "Epoch 30 | Batch 37 | Loss: 0.22749195992946625\n",
      "Epoch 30 | Batch 38 | Loss: 0.34933778643608093\n",
      "Epoch 30 | Batch 39 | Loss: 0.3026354908943176\n",
      "Epoch 30 | Batch 40 | Loss: 0.22548526525497437\n",
      "Epoch 30 | Batch 41 | Loss: 0.3740631341934204\n",
      "Epoch 30 | Batch 42 | Loss: 0.5735373497009277\n",
      "Epoch 30 | Batch 43 | Loss: 0.5324568748474121\n",
      "Epoch 30 | Batch 44 | Loss: 0.14952069520950317\n",
      "Epoch 30 | Batch 45 | Loss: 0.293978750705719\n",
      "Epoch 30 | Batch 46 | Loss: 0.1692482978105545\n",
      "Epoch 30 | Batch 47 | Loss: 0.2097749412059784\n",
      "Epoch 30 | Batch 48 | Loss: 0.08650694042444229\n",
      "Epoch 30 | Batch 49 | Loss: 0.332806795835495\n",
      "Epoch 30 | Batch 50 | Loss: 0.458071768283844\n",
      "Epoch 30 | Batch 51 | Loss: 0.2114214152097702\n",
      "Epoch 30 | Batch 52 | Loss: 0.4124515950679779\n",
      "Epoch 30 | Batch 53 | Loss: 0.3385425806045532\n",
      "Epoch 30 | Batch 54 | Loss: 0.6488685607910156\n",
      "Epoch 30 | Batch 55 | Loss: 0.6563565135002136\n",
      "Epoch 30 | Batch 56 | Loss: 0.18756774067878723\n",
      "Epoch 30 | Batch 57 | Loss: 0.5151742696762085\n",
      "Epoch 30 | Batch 58 | Loss: 0.17575225234031677\n",
      "Epoch 30 | Batch 59 | Loss: 0.2953777313232422\n",
      "Epoch 30 | Batch 60 | Loss: 0.22732660174369812\n",
      "Epoch 30 | Batch 61 | Loss: 0.675609827041626\n",
      "Epoch 30 | Batch 62 | Loss: 0.29902198910713196\n",
      "Epoch 30 | Batch 63 | Loss: 0.7021804451942444\n",
      "Epoch 30 | Batch 64 | Loss: 0.1227164939045906\n",
      "Epoch 30 | Batch 65 | Loss: 0.14165087044239044\n",
      "Epoch 30 | Batch 66 | Loss: 0.5788235664367676\n",
      "Epoch 30 | Batch 67 | Loss: 0.48415958881378174\n",
      "Epoch 30 | Batch 68 | Loss: 0.3660767376422882\n",
      "Epoch 30 | Batch 69 | Loss: 0.502701461315155\n",
      "Epoch 30 | Batch 70 | Loss: 0.44606518745422363\n",
      "Epoch 30 | Batch 71 | Loss: 0.2833372950553894\n",
      "Epoch 30 | Batch 72 | Loss: 0.3348526358604431\n",
      "Epoch 30 | Batch 73 | Loss: 0.7366793751716614\n",
      "Epoch 30 | Batch 74 | Loss: 0.39046144485473633\n",
      "Epoch 30 | Batch 75 | Loss: 1.0034325122833252\n",
      "Epoch 30 | Batch 76 | Loss: 0.15045830607414246\n",
      "Epoch 30 | Batch 77 | Loss: 0.20610398054122925\n",
      "Epoch 30 | Batch 78 | Loss: 0.31686675548553467\n",
      "Epoch 30 | Batch 79 | Loss: 0.09039698541164398\n",
      "Epoch 30 | Batch 80 | Loss: 0.15146800875663757\n",
      "Epoch 30 | Batch 81 | Loss: 0.6204777359962463\n",
      "Epoch 30 | Batch 82 | Loss: 1.13276207447052\n",
      "Epoch 30 | Batch 83 | Loss: 0.23802033066749573\n",
      "Epoch 30 | Batch 84 | Loss: 0.28036871552467346\n",
      "Epoch 30 | Batch 85 | Loss: 0.39680391550064087\n",
      "Epoch 30 | Batch 86 | Loss: 0.12188179790973663\n",
      "Epoch 30 | Batch 87 | Loss: 0.30617085099220276\n",
      "Epoch 30 | Batch 88 | Loss: 0.5015880465507507\n",
      "Epoch 30 | Batch 89 | Loss: 0.5149685740470886\n",
      "Epoch 30 | Batch 90 | Loss: 0.0022961015347391367\n",
      "Epoch 31 | Batch 1 | Loss: 0.269755482673645\n",
      "Epoch 31 | Batch 2 | Loss: 0.4298222064971924\n",
      "Epoch 31 | Batch 3 | Loss: 0.144619420170784\n",
      "Epoch 31 | Batch 4 | Loss: 0.39002856612205505\n",
      "Epoch 31 | Batch 5 | Loss: 0.5411951541900635\n",
      "Epoch 31 | Batch 6 | Loss: 0.38394007086753845\n",
      "Epoch 31 | Batch 7 | Loss: 0.17228814959526062\n",
      "Epoch 31 | Batch 8 | Loss: 0.3843275308609009\n",
      "Epoch 31 | Batch 9 | Loss: 0.36807215213775635\n",
      "Epoch 31 | Batch 10 | Loss: 0.26210275292396545\n",
      "Epoch 31 | Batch 11 | Loss: 0.4271235466003418\n",
      "Epoch 31 | Batch 12 | Loss: 0.1346488893032074\n",
      "Epoch 31 | Batch 13 | Loss: 0.17421124875545502\n",
      "Epoch 31 | Batch 14 | Loss: 0.5480775833129883\n",
      "Epoch 31 | Batch 15 | Loss: 0.23395439982414246\n",
      "Epoch 31 | Batch 16 | Loss: 0.1020318940281868\n",
      "Epoch 31 | Batch 17 | Loss: 0.18962472677230835\n",
      "Epoch 31 | Batch 18 | Loss: 1.334019660949707\n",
      "Epoch 31 | Batch 19 | Loss: 0.07861116528511047\n",
      "Epoch 31 | Batch 20 | Loss: 0.15439869463443756\n",
      "Epoch 31 | Batch 21 | Loss: 0.2463025003671646\n",
      "Epoch 31 | Batch 22 | Loss: 0.4254610240459442\n",
      "Epoch 31 | Batch 23 | Loss: 0.41605544090270996\n",
      "Epoch 31 | Batch 24 | Loss: 0.3646922707557678\n",
      "Epoch 31 | Batch 25 | Loss: 0.5481217503547668\n",
      "Epoch 31 | Batch 26 | Loss: 0.7220332622528076\n",
      "Epoch 31 | Batch 27 | Loss: 0.28679972887039185\n",
      "Epoch 31 | Batch 28 | Loss: 0.817423939704895\n",
      "Epoch 31 | Batch 29 | Loss: 0.23714084923267365\n",
      "Epoch 31 | Batch 30 | Loss: 0.5160605311393738\n",
      "Epoch 31 | Batch 31 | Loss: 0.14308646321296692\n",
      "Epoch 31 | Batch 32 | Loss: 0.2392772138118744\n",
      "Epoch 31 | Batch 33 | Loss: 0.345649778842926\n",
      "Epoch 31 | Batch 34 | Loss: 0.2835310697555542\n",
      "Epoch 31 | Batch 35 | Loss: 0.21292974054813385\n",
      "Epoch 31 | Batch 36 | Loss: 0.13822777569293976\n",
      "Epoch 31 | Batch 37 | Loss: 0.48126423358917236\n",
      "Epoch 31 | Batch 38 | Loss: 0.21110162138938904\n",
      "Epoch 31 | Batch 39 | Loss: 0.38563087582588196\n",
      "Epoch 31 | Batch 40 | Loss: 0.222540944814682\n",
      "Epoch 31 | Batch 41 | Loss: 0.34855735301971436\n",
      "Epoch 31 | Batch 42 | Loss: 0.2871458828449249\n",
      "Epoch 31 | Batch 43 | Loss: 0.7383434772491455\n",
      "Epoch 31 | Batch 44 | Loss: 0.16353856027126312\n",
      "Epoch 31 | Batch 45 | Loss: 0.5972210168838501\n",
      "Epoch 31 | Batch 46 | Loss: 0.4249812364578247\n",
      "Epoch 31 | Batch 47 | Loss: 0.11412660032510757\n",
      "Epoch 31 | Batch 48 | Loss: 0.6418033838272095\n",
      "Epoch 31 | Batch 49 | Loss: 0.10109317302703857\n",
      "Epoch 31 | Batch 50 | Loss: 0.3992466330528259\n",
      "Epoch 31 | Batch 51 | Loss: 0.21694205701351166\n",
      "Epoch 31 | Batch 52 | Loss: 0.2840754985809326\n",
      "Epoch 31 | Batch 53 | Loss: 0.7252671718597412\n",
      "Epoch 31 | Batch 54 | Loss: 0.943122148513794\n",
      "Epoch 31 | Batch 55 | Loss: 0.3975750207901001\n",
      "Epoch 31 | Batch 56 | Loss: 0.3503146767616272\n",
      "Epoch 31 | Batch 57 | Loss: 0.457396924495697\n",
      "Epoch 31 | Batch 58 | Loss: 0.3831101059913635\n",
      "Epoch 31 | Batch 59 | Loss: 0.26736515760421753\n",
      "Epoch 31 | Batch 60 | Loss: 0.3695536255836487\n",
      "Epoch 31 | Batch 61 | Loss: 0.3067789077758789\n",
      "Epoch 31 | Batch 62 | Loss: 0.4426308870315552\n",
      "Epoch 31 | Batch 63 | Loss: 0.19970965385437012\n",
      "Epoch 31 | Batch 64 | Loss: 0.35084110498428345\n",
      "Epoch 31 | Batch 65 | Loss: 0.1377047598361969\n",
      "Epoch 31 | Batch 66 | Loss: 0.8015886545181274\n",
      "Epoch 31 | Batch 67 | Loss: 0.41361671686172485\n",
      "Epoch 31 | Batch 68 | Loss: 0.16999296844005585\n",
      "Epoch 31 | Batch 69 | Loss: 0.48581746220588684\n",
      "Epoch 31 | Batch 70 | Loss: 0.18788518011569977\n",
      "Epoch 31 | Batch 71 | Loss: 0.24965256452560425\n",
      "Epoch 31 | Batch 72 | Loss: 0.26356828212738037\n",
      "Epoch 31 | Batch 73 | Loss: 0.7851289510726929\n",
      "Epoch 31 | Batch 74 | Loss: 0.49341368675231934\n",
      "Epoch 31 | Batch 75 | Loss: 0.48103976249694824\n",
      "Epoch 31 | Batch 76 | Loss: 0.5133168697357178\n",
      "Epoch 31 | Batch 77 | Loss: 0.623677134513855\n",
      "Epoch 31 | Batch 78 | Loss: 0.3210619390010834\n",
      "Epoch 31 | Batch 79 | Loss: 0.4106031656265259\n",
      "Epoch 31 | Batch 80 | Loss: 0.7075620293617249\n",
      "Epoch 31 | Batch 81 | Loss: 0.04591914638876915\n",
      "Epoch 31 | Batch 82 | Loss: 0.2937889099121094\n",
      "Epoch 31 | Batch 83 | Loss: 0.34497904777526855\n",
      "Epoch 31 | Batch 84 | Loss: 0.5716487169265747\n",
      "Epoch 31 | Batch 85 | Loss: 0.47064411640167236\n",
      "Epoch 31 | Batch 86 | Loss: 0.3060459792613983\n",
      "Epoch 31 | Batch 87 | Loss: 0.22791129350662231\n",
      "Epoch 31 | Batch 88 | Loss: 0.15236283838748932\n",
      "Epoch 31 | Batch 89 | Loss: 0.20886720716953278\n",
      "Epoch 31 | Batch 90 | Loss: 1.1705390214920044\n",
      "Epoch 32 | Batch 1 | Loss: 0.3045264482498169\n",
      "Epoch 32 | Batch 2 | Loss: 0.6095317602157593\n",
      "Epoch 32 | Batch 3 | Loss: 0.7982446551322937\n",
      "Epoch 32 | Batch 4 | Loss: 0.18078088760375977\n",
      "Epoch 32 | Batch 5 | Loss: 0.5683690905570984\n",
      "Epoch 32 | Batch 6 | Loss: 0.24651485681533813\n",
      "Epoch 32 | Batch 7 | Loss: 0.45779335498809814\n",
      "Epoch 32 | Batch 8 | Loss: 0.6945739984512329\n",
      "Epoch 32 | Batch 9 | Loss: 0.6906670331954956\n",
      "Epoch 32 | Batch 10 | Loss: 0.2557916045188904\n",
      "Epoch 32 | Batch 11 | Loss: 0.1989893913269043\n",
      "Epoch 32 | Batch 12 | Loss: 0.619702160358429\n",
      "Epoch 32 | Batch 13 | Loss: 0.4780169427394867\n",
      "Epoch 32 | Batch 14 | Loss: 0.3191121220588684\n",
      "Epoch 32 | Batch 15 | Loss: 0.2615595757961273\n",
      "Epoch 32 | Batch 16 | Loss: 0.6849206686019897\n",
      "Epoch 32 | Batch 17 | Loss: 0.8811846971511841\n",
      "Epoch 32 | Batch 18 | Loss: 0.1368560791015625\n",
      "Epoch 32 | Batch 19 | Loss: 0.20638585090637207\n",
      "Epoch 32 | Batch 20 | Loss: 0.6316952109336853\n",
      "Epoch 32 | Batch 21 | Loss: 0.19338911771774292\n",
      "Epoch 32 | Batch 22 | Loss: 0.26012295484542847\n",
      "Epoch 32 | Batch 23 | Loss: 0.43449288606643677\n",
      "Epoch 32 | Batch 24 | Loss: 0.4472092092037201\n",
      "Epoch 32 | Batch 25 | Loss: 0.2293747067451477\n",
      "Epoch 32 | Batch 26 | Loss: 0.23550955951213837\n",
      "Epoch 32 | Batch 27 | Loss: 0.3724123239517212\n",
      "Epoch 32 | Batch 28 | Loss: 0.30993813276290894\n",
      "Epoch 32 | Batch 29 | Loss: 0.23581895232200623\n",
      "Epoch 32 | Batch 30 | Loss: 0.18951430916786194\n",
      "Epoch 32 | Batch 31 | Loss: 0.8326922059059143\n",
      "Epoch 32 | Batch 32 | Loss: 0.7006491422653198\n",
      "Epoch 32 | Batch 33 | Loss: 0.24376866221427917\n",
      "Epoch 32 | Batch 34 | Loss: 0.5144809484481812\n",
      "Epoch 32 | Batch 35 | Loss: 0.35304468870162964\n",
      "Epoch 32 | Batch 36 | Loss: 0.224919393658638\n",
      "Epoch 32 | Batch 37 | Loss: 0.545759379863739\n",
      "Epoch 32 | Batch 38 | Loss: 0.33077409863471985\n",
      "Epoch 32 | Batch 39 | Loss: 0.43225380778312683\n",
      "Epoch 32 | Batch 40 | Loss: 0.5985239148139954\n",
      "Epoch 32 | Batch 41 | Loss: 0.1346638947725296\n",
      "Epoch 32 | Batch 42 | Loss: 0.35699060559272766\n",
      "Epoch 32 | Batch 43 | Loss: 0.5393915772438049\n",
      "Epoch 32 | Batch 44 | Loss: 0.5867606997489929\n",
      "Epoch 32 | Batch 45 | Loss: 0.39037299156188965\n",
      "Epoch 32 | Batch 46 | Loss: 0.4099942445755005\n",
      "Epoch 32 | Batch 47 | Loss: 0.15413574874401093\n",
      "Epoch 32 | Batch 48 | Loss: 0.2239580899477005\n",
      "Epoch 32 | Batch 49 | Loss: 0.5578353404998779\n",
      "Epoch 32 | Batch 50 | Loss: 0.43212682008743286\n",
      "Epoch 32 | Batch 51 | Loss: 0.3749326765537262\n",
      "Epoch 32 | Batch 52 | Loss: 0.26845255494117737\n",
      "Epoch 32 | Batch 53 | Loss: 0.4031168520450592\n",
      "Epoch 32 | Batch 54 | Loss: 0.10858413577079773\n",
      "Epoch 32 | Batch 55 | Loss: 0.40434807538986206\n",
      "Epoch 32 | Batch 56 | Loss: 0.48886021971702576\n",
      "Epoch 32 | Batch 57 | Loss: 0.38380491733551025\n",
      "Epoch 32 | Batch 58 | Loss: 0.6942884922027588\n",
      "Epoch 32 | Batch 59 | Loss: 0.153150275349617\n",
      "Epoch 32 | Batch 60 | Loss: 0.46704185009002686\n",
      "Epoch 32 | Batch 61 | Loss: 0.23938527703285217\n",
      "Epoch 32 | Batch 62 | Loss: 0.3086889982223511\n",
      "Epoch 32 | Batch 63 | Loss: 0.4044796824455261\n",
      "Epoch 32 | Batch 64 | Loss: 0.21008218824863434\n",
      "Epoch 32 | Batch 65 | Loss: 0.5222664475440979\n",
      "Epoch 32 | Batch 66 | Loss: 0.1454414278268814\n",
      "Epoch 32 | Batch 67 | Loss: 0.7635887861251831\n",
      "Epoch 32 | Batch 68 | Loss: 0.4305100440979004\n",
      "Epoch 32 | Batch 69 | Loss: 0.10779011994600296\n",
      "Epoch 32 | Batch 70 | Loss: 0.19371548295021057\n",
      "Epoch 32 | Batch 71 | Loss: 0.6090885400772095\n",
      "Epoch 32 | Batch 72 | Loss: 0.46485036611557007\n",
      "Epoch 32 | Batch 73 | Loss: 0.48200762271881104\n",
      "Epoch 32 | Batch 74 | Loss: 0.6234822273254395\n",
      "Epoch 32 | Batch 75 | Loss: 0.23617443442344666\n",
      "Epoch 32 | Batch 76 | Loss: 0.20683419704437256\n",
      "Epoch 32 | Batch 77 | Loss: 0.2725631296634674\n",
      "Epoch 32 | Batch 78 | Loss: 0.28803396224975586\n",
      "Epoch 32 | Batch 79 | Loss: 0.15766647458076477\n",
      "Epoch 32 | Batch 80 | Loss: 0.4826318025588989\n",
      "Epoch 32 | Batch 81 | Loss: 0.6700124740600586\n",
      "Epoch 32 | Batch 82 | Loss: 0.2473655641078949\n",
      "Epoch 32 | Batch 83 | Loss: 0.6911449432373047\n",
      "Epoch 32 | Batch 84 | Loss: 0.2172335535287857\n",
      "Epoch 32 | Batch 85 | Loss: 0.5027265548706055\n",
      "Epoch 32 | Batch 86 | Loss: 0.24348148703575134\n",
      "Epoch 32 | Batch 87 | Loss: 0.2135734260082245\n",
      "Epoch 32 | Batch 88 | Loss: 0.5277854204177856\n",
      "Epoch 32 | Batch 89 | Loss: 0.2635229527950287\n",
      "Epoch 32 | Batch 90 | Loss: 0.1437995880842209\n",
      "Epoch 33 | Batch 1 | Loss: 0.6953977346420288\n",
      "Epoch 33 | Batch 2 | Loss: 0.39448052644729614\n",
      "Epoch 33 | Batch 3 | Loss: 0.31843411922454834\n",
      "Epoch 33 | Batch 4 | Loss: 0.4556330442428589\n",
      "Epoch 33 | Batch 5 | Loss: 0.15973573923110962\n",
      "Epoch 33 | Batch 6 | Loss: 0.5160975456237793\n",
      "Epoch 33 | Batch 7 | Loss: 0.4293825030326843\n",
      "Epoch 33 | Batch 8 | Loss: 0.46151086688041687\n",
      "Epoch 33 | Batch 9 | Loss: 0.5603686571121216\n",
      "Epoch 33 | Batch 10 | Loss: 0.18743908405303955\n",
      "Epoch 33 | Batch 11 | Loss: 0.16588684916496277\n",
      "Epoch 33 | Batch 12 | Loss: 0.18927836418151855\n",
      "Epoch 33 | Batch 13 | Loss: 0.18840597569942474\n",
      "Epoch 33 | Batch 14 | Loss: 0.07854106277227402\n",
      "Epoch 33 | Batch 15 | Loss: 0.44103923439979553\n",
      "Epoch 33 | Batch 16 | Loss: 0.3904125690460205\n",
      "Epoch 33 | Batch 17 | Loss: 0.31687477231025696\n",
      "Epoch 33 | Batch 18 | Loss: 0.3702576756477356\n",
      "Epoch 33 | Batch 19 | Loss: 0.7049847841262817\n",
      "Epoch 33 | Batch 20 | Loss: 0.24523630738258362\n",
      "Epoch 33 | Batch 21 | Loss: 0.3201410174369812\n",
      "Epoch 33 | Batch 22 | Loss: 0.2409886121749878\n",
      "Epoch 33 | Batch 23 | Loss: 0.28079283237457275\n",
      "Epoch 33 | Batch 24 | Loss: 0.20705342292785645\n",
      "Epoch 33 | Batch 25 | Loss: 0.10486534237861633\n",
      "Epoch 33 | Batch 26 | Loss: 0.41948044300079346\n",
      "Epoch 33 | Batch 27 | Loss: 0.6142319440841675\n",
      "Epoch 33 | Batch 28 | Loss: 0.30291905999183655\n",
      "Epoch 33 | Batch 29 | Loss: 0.5268398523330688\n",
      "Epoch 33 | Batch 30 | Loss: 0.2683248519897461\n",
      "Epoch 33 | Batch 31 | Loss: 0.4494180977344513\n",
      "Epoch 33 | Batch 32 | Loss: 0.5520645976066589\n",
      "Epoch 33 | Batch 33 | Loss: 0.07630254328250885\n",
      "Epoch 33 | Batch 34 | Loss: 0.7989592552185059\n",
      "Epoch 33 | Batch 35 | Loss: 0.2734377384185791\n",
      "Epoch 33 | Batch 36 | Loss: 0.8558024168014526\n",
      "Epoch 33 | Batch 37 | Loss: 0.4517418444156647\n",
      "Epoch 33 | Batch 38 | Loss: 0.4569041132926941\n",
      "Epoch 33 | Batch 39 | Loss: 0.09334073960781097\n",
      "Epoch 33 | Batch 40 | Loss: 0.4001040458679199\n",
      "Epoch 33 | Batch 41 | Loss: 0.2129182517528534\n",
      "Epoch 33 | Batch 42 | Loss: 0.3622366786003113\n",
      "Epoch 33 | Batch 43 | Loss: 0.4348383843898773\n",
      "Epoch 33 | Batch 44 | Loss: 0.16077125072479248\n",
      "Epoch 33 | Batch 45 | Loss: 0.5026377439498901\n",
      "Epoch 33 | Batch 46 | Loss: 0.27493032813072205\n",
      "Epoch 33 | Batch 47 | Loss: 0.15853314101696014\n",
      "Epoch 33 | Batch 48 | Loss: 0.270280659198761\n",
      "Epoch 33 | Batch 49 | Loss: 0.40533196926116943\n",
      "Epoch 33 | Batch 50 | Loss: 0.4082510471343994\n",
      "Epoch 33 | Batch 51 | Loss: 0.5391675233840942\n",
      "Epoch 33 | Batch 52 | Loss: 0.1651371419429779\n",
      "Epoch 33 | Batch 53 | Loss: 0.22864550352096558\n",
      "Epoch 33 | Batch 54 | Loss: 0.32043522596359253\n",
      "Epoch 33 | Batch 55 | Loss: 0.5498766303062439\n",
      "Epoch 33 | Batch 56 | Loss: 0.2003934532403946\n",
      "Epoch 33 | Batch 57 | Loss: 0.4912169873714447\n",
      "Epoch 33 | Batch 58 | Loss: 0.3856172561645508\n",
      "Epoch 33 | Batch 59 | Loss: 0.42154163122177124\n",
      "Epoch 33 | Batch 60 | Loss: 0.4487709701061249\n",
      "Epoch 33 | Batch 61 | Loss: 0.21377959847450256\n",
      "Epoch 33 | Batch 62 | Loss: 0.054769933223724365\n",
      "Epoch 33 | Batch 63 | Loss: 0.713610053062439\n",
      "Epoch 33 | Batch 64 | Loss: 0.2043772041797638\n",
      "Epoch 33 | Batch 65 | Loss: 0.30576416850090027\n",
      "Epoch 33 | Batch 66 | Loss: 0.7983088493347168\n",
      "Epoch 33 | Batch 67 | Loss: 0.10913857817649841\n",
      "Epoch 33 | Batch 68 | Loss: 0.3514091968536377\n",
      "Epoch 33 | Batch 69 | Loss: 0.7824069261550903\n",
      "Epoch 33 | Batch 70 | Loss: 0.26170092821121216\n",
      "Epoch 33 | Batch 71 | Loss: 0.08494102209806442\n",
      "Epoch 33 | Batch 72 | Loss: 0.2746603190898895\n",
      "Epoch 33 | Batch 73 | Loss: 0.6191893815994263\n",
      "Epoch 33 | Batch 74 | Loss: 0.2871969938278198\n",
      "Epoch 33 | Batch 75 | Loss: 0.22200483083724976\n",
      "Epoch 33 | Batch 76 | Loss: 0.5856456160545349\n",
      "Epoch 33 | Batch 77 | Loss: 0.3577023446559906\n",
      "Epoch 33 | Batch 78 | Loss: 0.19228017330169678\n",
      "Epoch 33 | Batch 79 | Loss: 0.4899023175239563\n",
      "Epoch 33 | Batch 80 | Loss: 0.35405564308166504\n",
      "Epoch 33 | Batch 81 | Loss: 0.2880784273147583\n",
      "Epoch 33 | Batch 82 | Loss: 0.4915349781513214\n",
      "Epoch 33 | Batch 83 | Loss: 0.7170766592025757\n",
      "Epoch 33 | Batch 84 | Loss: 0.23715239763259888\n",
      "Epoch 33 | Batch 85 | Loss: 0.8665904998779297\n",
      "Epoch 33 | Batch 86 | Loss: 0.16857990622520447\n",
      "Epoch 33 | Batch 87 | Loss: 0.35770314931869507\n",
      "Epoch 33 | Batch 88 | Loss: 0.690490186214447\n",
      "Epoch 33 | Batch 89 | Loss: 0.505202054977417\n",
      "Epoch 33 | Batch 90 | Loss: 0.12453234195709229\n",
      "Epoch 34 | Batch 1 | Loss: 0.41734060645103455\n",
      "Epoch 34 | Batch 2 | Loss: 0.24178887903690338\n",
      "Epoch 34 | Batch 3 | Loss: 0.3837745785713196\n",
      "Epoch 34 | Batch 4 | Loss: 0.18565063178539276\n",
      "Epoch 34 | Batch 5 | Loss: 0.36891424655914307\n",
      "Epoch 34 | Batch 6 | Loss: 0.07335080206394196\n",
      "Epoch 34 | Batch 7 | Loss: 0.1734689325094223\n",
      "Epoch 34 | Batch 8 | Loss: 0.06272020936012268\n",
      "Epoch 34 | Batch 9 | Loss: 0.08834218978881836\n",
      "Epoch 34 | Batch 10 | Loss: 0.349403977394104\n",
      "Epoch 34 | Batch 11 | Loss: 0.21158528327941895\n",
      "Epoch 34 | Batch 12 | Loss: 0.2792230546474457\n",
      "Epoch 34 | Batch 13 | Loss: 0.1861707717180252\n",
      "Epoch 34 | Batch 14 | Loss: 0.04990380257368088\n",
      "Epoch 34 | Batch 15 | Loss: 0.26803848147392273\n",
      "Epoch 34 | Batch 16 | Loss: 0.4854893088340759\n",
      "Epoch 34 | Batch 17 | Loss: 0.7999057769775391\n",
      "Epoch 34 | Batch 18 | Loss: 0.33840805292129517\n",
      "Epoch 34 | Batch 19 | Loss: 0.3636075258255005\n",
      "Epoch 34 | Batch 20 | Loss: 0.32632431387901306\n",
      "Epoch 34 | Batch 21 | Loss: 0.12271811813116074\n",
      "Epoch 34 | Batch 22 | Loss: 0.6597071886062622\n",
      "Epoch 34 | Batch 23 | Loss: 0.4652676582336426\n",
      "Epoch 34 | Batch 24 | Loss: 0.5404433608055115\n",
      "Epoch 34 | Batch 25 | Loss: 0.42618656158447266\n",
      "Epoch 34 | Batch 26 | Loss: 0.23624199628829956\n",
      "Epoch 34 | Batch 27 | Loss: 0.1933649182319641\n",
      "Epoch 34 | Batch 28 | Loss: 0.508944571018219\n",
      "Epoch 34 | Batch 29 | Loss: 0.5087095499038696\n",
      "Epoch 34 | Batch 30 | Loss: 0.461769163608551\n",
      "Epoch 34 | Batch 31 | Loss: 0.49210885167121887\n",
      "Epoch 34 | Batch 32 | Loss: 0.3725619912147522\n",
      "Epoch 34 | Batch 33 | Loss: 0.2713826596736908\n",
      "Epoch 34 | Batch 34 | Loss: 0.22987866401672363\n",
      "Epoch 34 | Batch 35 | Loss: 0.7970145344734192\n",
      "Epoch 34 | Batch 36 | Loss: 0.2271229326725006\n",
      "Epoch 34 | Batch 37 | Loss: 0.6104905009269714\n",
      "Epoch 34 | Batch 38 | Loss: 0.1929812729358673\n",
      "Epoch 34 | Batch 39 | Loss: 0.44842249155044556\n",
      "Epoch 34 | Batch 40 | Loss: 0.10647750645875931\n",
      "Epoch 34 | Batch 41 | Loss: 0.32927483320236206\n",
      "Epoch 34 | Batch 42 | Loss: 0.18619611859321594\n",
      "Epoch 34 | Batch 43 | Loss: 0.16486413776874542\n",
      "Epoch 34 | Batch 44 | Loss: 0.5216158628463745\n",
      "Epoch 34 | Batch 45 | Loss: 0.5429900884628296\n",
      "Epoch 34 | Batch 46 | Loss: 0.22925660014152527\n",
      "Epoch 34 | Batch 47 | Loss: 0.8936789035797119\n",
      "Epoch 34 | Batch 48 | Loss: 0.4341937005519867\n",
      "Epoch 34 | Batch 49 | Loss: 0.09223340451717377\n",
      "Epoch 34 | Batch 50 | Loss: 0.28887078166007996\n",
      "Epoch 34 | Batch 51 | Loss: 0.4369116425514221\n",
      "Epoch 34 | Batch 52 | Loss: 0.23690994083881378\n",
      "Epoch 34 | Batch 53 | Loss: 0.354861319065094\n",
      "Epoch 34 | Batch 54 | Loss: 0.7980953454971313\n",
      "Epoch 34 | Batch 55 | Loss: 0.4343493580818176\n",
      "Epoch 34 | Batch 56 | Loss: 0.1988605409860611\n",
      "Epoch 34 | Batch 57 | Loss: 0.24620568752288818\n",
      "Epoch 34 | Batch 58 | Loss: 0.45847153663635254\n",
      "Epoch 34 | Batch 59 | Loss: 0.6541898846626282\n",
      "Epoch 34 | Batch 60 | Loss: 0.18275727331638336\n",
      "Epoch 34 | Batch 61 | Loss: 0.5390603542327881\n",
      "Epoch 34 | Batch 62 | Loss: 0.3359100818634033\n",
      "Epoch 34 | Batch 63 | Loss: 0.3356412351131439\n",
      "Epoch 34 | Batch 64 | Loss: 0.43548232316970825\n",
      "Epoch 34 | Batch 65 | Loss: 0.0823645144701004\n",
      "Epoch 34 | Batch 66 | Loss: 0.7473050951957703\n",
      "Epoch 34 | Batch 67 | Loss: 0.46272268891334534\n",
      "Epoch 34 | Batch 68 | Loss: 0.6298775672912598\n",
      "Epoch 34 | Batch 69 | Loss: 0.26413920521736145\n",
      "Epoch 34 | Batch 70 | Loss: 0.26735949516296387\n",
      "Epoch 34 | Batch 71 | Loss: 0.6267077922821045\n",
      "Epoch 34 | Batch 72 | Loss: 0.7071079611778259\n",
      "Epoch 34 | Batch 73 | Loss: 0.08915910869836807\n",
      "Epoch 34 | Batch 74 | Loss: 0.2397160530090332\n",
      "Epoch 34 | Batch 75 | Loss: 0.2596854865550995\n",
      "Epoch 34 | Batch 76 | Loss: 0.3346005082130432\n",
      "Epoch 34 | Batch 77 | Loss: 0.571366548538208\n",
      "Epoch 34 | Batch 78 | Loss: 0.4863046407699585\n",
      "Epoch 34 | Batch 79 | Loss: 0.5677418112754822\n",
      "Epoch 34 | Batch 80 | Loss: 0.27953290939331055\n",
      "Epoch 34 | Batch 81 | Loss: 0.19046998023986816\n",
      "Epoch 34 | Batch 82 | Loss: 0.4492681324481964\n",
      "Epoch 34 | Batch 83 | Loss: 0.3816494345664978\n",
      "Epoch 34 | Batch 84 | Loss: 0.2000068873167038\n",
      "Epoch 34 | Batch 85 | Loss: 0.1263427436351776\n",
      "Epoch 34 | Batch 86 | Loss: 0.6836990714073181\n",
      "Epoch 34 | Batch 87 | Loss: 0.5964080691337585\n",
      "Epoch 34 | Batch 88 | Loss: 0.5767444968223572\n",
      "Epoch 34 | Batch 89 | Loss: 0.7876766920089722\n",
      "Epoch 34 | Batch 90 | Loss: 0.5137279033660889\n",
      "Epoch 35 | Batch 1 | Loss: 0.4342491030693054\n",
      "Epoch 35 | Batch 2 | Loss: 0.6478505730628967\n",
      "Epoch 35 | Batch 3 | Loss: 0.4097442030906677\n",
      "Epoch 35 | Batch 4 | Loss: 0.507773220539093\n",
      "Epoch 35 | Batch 5 | Loss: 0.8663733601570129\n",
      "Epoch 35 | Batch 6 | Loss: 0.1647261679172516\n",
      "Epoch 35 | Batch 7 | Loss: 0.43960779905319214\n",
      "Epoch 35 | Batch 8 | Loss: 0.1032448559999466\n",
      "Epoch 35 | Batch 9 | Loss: 0.20878121256828308\n",
      "Epoch 35 | Batch 10 | Loss: 0.37458541989326477\n",
      "Epoch 35 | Batch 11 | Loss: 0.6414046287536621\n",
      "Epoch 35 | Batch 12 | Loss: 0.8472649455070496\n",
      "Epoch 35 | Batch 13 | Loss: 0.22580045461654663\n",
      "Epoch 35 | Batch 14 | Loss: 0.5966122150421143\n",
      "Epoch 35 | Batch 15 | Loss: 0.1556732952594757\n",
      "Epoch 35 | Batch 16 | Loss: 0.25452733039855957\n",
      "Epoch 35 | Batch 17 | Loss: 0.9104834794998169\n",
      "Epoch 35 | Batch 18 | Loss: 0.5429723262786865\n",
      "Epoch 35 | Batch 19 | Loss: 0.2709953784942627\n",
      "Epoch 35 | Batch 20 | Loss: 0.17233183979988098\n",
      "Epoch 35 | Batch 21 | Loss: 0.32028597593307495\n",
      "Epoch 35 | Batch 22 | Loss: 0.3427480161190033\n",
      "Epoch 35 | Batch 23 | Loss: 0.5247328281402588\n",
      "Epoch 35 | Batch 24 | Loss: 0.4410504698753357\n",
      "Epoch 35 | Batch 25 | Loss: 0.28832268714904785\n",
      "Epoch 35 | Batch 26 | Loss: 0.3635939955711365\n",
      "Epoch 35 | Batch 27 | Loss: 0.22070352733135223\n",
      "Epoch 35 | Batch 28 | Loss: 0.1880873739719391\n",
      "Epoch 35 | Batch 29 | Loss: 0.27887997031211853\n",
      "Epoch 35 | Batch 30 | Loss: 0.25519251823425293\n",
      "Epoch 35 | Batch 31 | Loss: 0.20156143605709076\n",
      "Epoch 35 | Batch 32 | Loss: 0.1892624944448471\n",
      "Epoch 35 | Batch 33 | Loss: 0.29562538862228394\n",
      "Epoch 35 | Batch 34 | Loss: 0.6563752293586731\n",
      "Epoch 35 | Batch 35 | Loss: 0.34519127011299133\n",
      "Epoch 35 | Batch 36 | Loss: 0.2499305009841919\n",
      "Epoch 35 | Batch 37 | Loss: 0.5516012907028198\n",
      "Epoch 35 | Batch 38 | Loss: 0.5310258865356445\n",
      "Epoch 35 | Batch 39 | Loss: 0.5661791563034058\n",
      "Epoch 35 | Batch 40 | Loss: 0.2920791506767273\n",
      "Epoch 35 | Batch 41 | Loss: 0.2196272313594818\n",
      "Epoch 35 | Batch 42 | Loss: 0.22143666446208954\n",
      "Epoch 35 | Batch 43 | Loss: 0.5458200573921204\n",
      "Epoch 35 | Batch 44 | Loss: 0.2172394096851349\n",
      "Epoch 35 | Batch 45 | Loss: 0.7352526187896729\n",
      "Epoch 35 | Batch 46 | Loss: 0.2289421260356903\n",
      "Epoch 35 | Batch 47 | Loss: 0.3816121220588684\n",
      "Epoch 35 | Batch 48 | Loss: 0.42801201343536377\n",
      "Epoch 35 | Batch 49 | Loss: 0.20103776454925537\n",
      "Epoch 35 | Batch 50 | Loss: 0.23909145593643188\n",
      "Epoch 35 | Batch 51 | Loss: 0.28544148802757263\n",
      "Epoch 35 | Batch 52 | Loss: 0.13109475374221802\n",
      "Epoch 35 | Batch 53 | Loss: 0.20581719279289246\n",
      "Epoch 35 | Batch 54 | Loss: 0.3953023850917816\n",
      "Epoch 35 | Batch 55 | Loss: 0.8360710144042969\n",
      "Epoch 35 | Batch 56 | Loss: 0.49320274591445923\n",
      "Epoch 35 | Batch 57 | Loss: 0.2941190004348755\n",
      "Epoch 35 | Batch 58 | Loss: 0.6511740684509277\n",
      "Epoch 35 | Batch 59 | Loss: 0.1781141459941864\n",
      "Epoch 35 | Batch 60 | Loss: 0.24164323508739471\n",
      "Epoch 35 | Batch 61 | Loss: 0.35275015234947205\n",
      "Epoch 35 | Batch 62 | Loss: 0.2790479063987732\n",
      "Epoch 35 | Batch 63 | Loss: 0.39842623472213745\n",
      "Epoch 35 | Batch 64 | Loss: 0.22095811367034912\n",
      "Epoch 35 | Batch 65 | Loss: 0.9600125551223755\n",
      "Epoch 35 | Batch 66 | Loss: 0.40316614508628845\n",
      "Epoch 35 | Batch 67 | Loss: 0.5878565907478333\n",
      "Epoch 35 | Batch 68 | Loss: 0.28648245334625244\n",
      "Epoch 35 | Batch 69 | Loss: 0.34498053789138794\n",
      "Epoch 35 | Batch 70 | Loss: 0.2104099541902542\n",
      "Epoch 35 | Batch 71 | Loss: 0.3084937334060669\n",
      "Epoch 35 | Batch 72 | Loss: 0.1441000998020172\n",
      "Epoch 35 | Batch 73 | Loss: 0.6037290096282959\n",
      "Epoch 35 | Batch 74 | Loss: 0.22247403860092163\n",
      "Epoch 35 | Batch 75 | Loss: 0.4307847023010254\n",
      "Epoch 35 | Batch 76 | Loss: 0.4285738468170166\n",
      "Epoch 35 | Batch 77 | Loss: 0.12826129794120789\n",
      "Epoch 35 | Batch 78 | Loss: 0.6232792139053345\n",
      "Epoch 35 | Batch 79 | Loss: 0.20826801657676697\n",
      "Epoch 35 | Batch 80 | Loss: 0.49953368306159973\n",
      "Epoch 35 | Batch 81 | Loss: 0.17030075192451477\n",
      "Epoch 35 | Batch 82 | Loss: 0.3993734121322632\n",
      "Epoch 35 | Batch 83 | Loss: 0.2918793261051178\n",
      "Epoch 35 | Batch 84 | Loss: 0.6084544658660889\n",
      "Epoch 35 | Batch 85 | Loss: 0.35188189148902893\n",
      "Epoch 35 | Batch 86 | Loss: 0.5528092980384827\n",
      "Epoch 35 | Batch 87 | Loss: 0.25508901476860046\n",
      "Epoch 35 | Batch 88 | Loss: 0.3254963159561157\n",
      "Epoch 35 | Batch 89 | Loss: 0.8724139332771301\n",
      "Epoch 35 | Batch 90 | Loss: 0.16102439165115356\n",
      "Epoch 36 | Batch 1 | Loss: 0.4660717248916626\n",
      "Epoch 36 | Batch 2 | Loss: 0.3449387848377228\n",
      "Epoch 36 | Batch 3 | Loss: 0.23063194751739502\n",
      "Epoch 36 | Batch 4 | Loss: 0.6029630899429321\n",
      "Epoch 36 | Batch 5 | Loss: 0.24705395102500916\n",
      "Epoch 36 | Batch 6 | Loss: 0.5364819765090942\n",
      "Epoch 36 | Batch 7 | Loss: 0.4159511923789978\n",
      "Epoch 36 | Batch 8 | Loss: 0.10046077519655228\n",
      "Epoch 36 | Batch 9 | Loss: 0.2282186895608902\n",
      "Epoch 36 | Batch 10 | Loss: 0.42868977785110474\n",
      "Epoch 36 | Batch 11 | Loss: 0.6140963435173035\n",
      "Epoch 36 | Batch 12 | Loss: 0.08258195221424103\n",
      "Epoch 36 | Batch 13 | Loss: 0.2596975862979889\n",
      "Epoch 36 | Batch 14 | Loss: 0.5264405608177185\n",
      "Epoch 36 | Batch 15 | Loss: 0.3964846730232239\n",
      "Epoch 36 | Batch 16 | Loss: 0.5462113618850708\n",
      "Epoch 36 | Batch 17 | Loss: 0.6282705664634705\n",
      "Epoch 36 | Batch 18 | Loss: 0.24247518181800842\n",
      "Epoch 36 | Batch 19 | Loss: 0.4267053008079529\n",
      "Epoch 36 | Batch 20 | Loss: 0.3232482373714447\n",
      "Epoch 36 | Batch 21 | Loss: 0.22368362545967102\n",
      "Epoch 36 | Batch 22 | Loss: 0.47978711128234863\n",
      "Epoch 36 | Batch 23 | Loss: 0.7013149857521057\n",
      "Epoch 36 | Batch 24 | Loss: 0.18570181727409363\n",
      "Epoch 36 | Batch 25 | Loss: 0.23090338706970215\n",
      "Epoch 36 | Batch 26 | Loss: 0.09278995543718338\n",
      "Epoch 36 | Batch 27 | Loss: 0.249826580286026\n",
      "Epoch 36 | Batch 28 | Loss: 0.41615331172943115\n",
      "Epoch 36 | Batch 29 | Loss: 0.21339178085327148\n",
      "Epoch 36 | Batch 30 | Loss: 0.24573791027069092\n",
      "Epoch 36 | Batch 31 | Loss: 0.10473743826150894\n",
      "Epoch 36 | Batch 32 | Loss: 0.2672041356563568\n",
      "Epoch 36 | Batch 33 | Loss: 0.08244948089122772\n",
      "Epoch 36 | Batch 34 | Loss: 0.5906695127487183\n",
      "Epoch 36 | Batch 35 | Loss: 0.2275497317314148\n",
      "Epoch 36 | Batch 36 | Loss: 0.15347625315189362\n",
      "Epoch 36 | Batch 37 | Loss: 0.7869938611984253\n",
      "Epoch 36 | Batch 38 | Loss: 0.3111283779144287\n",
      "Epoch 36 | Batch 39 | Loss: 0.17637521028518677\n",
      "Epoch 36 | Batch 40 | Loss: 0.2077464908361435\n",
      "Epoch 36 | Batch 41 | Loss: 0.24958613514900208\n",
      "Epoch 36 | Batch 42 | Loss: 0.6409255266189575\n",
      "Epoch 36 | Batch 43 | Loss: 0.3755039572715759\n",
      "Epoch 36 | Batch 44 | Loss: 0.6872340440750122\n",
      "Epoch 36 | Batch 45 | Loss: 0.5666950345039368\n",
      "Epoch 36 | Batch 46 | Loss: 0.8883066773414612\n",
      "Epoch 36 | Batch 47 | Loss: 0.20559175312519073\n",
      "Epoch 36 | Batch 48 | Loss: 0.3894588351249695\n",
      "Epoch 36 | Batch 49 | Loss: 0.16225206851959229\n",
      "Epoch 36 | Batch 50 | Loss: 0.16187191009521484\n",
      "Epoch 36 | Batch 51 | Loss: 0.5977181196212769\n",
      "Epoch 36 | Batch 52 | Loss: 0.1817859709262848\n",
      "Epoch 36 | Batch 53 | Loss: 0.3643099069595337\n",
      "Epoch 36 | Batch 54 | Loss: 0.2915387451648712\n",
      "Epoch 36 | Batch 55 | Loss: 0.4212692379951477\n",
      "Epoch 36 | Batch 56 | Loss: 0.6473672389984131\n",
      "Epoch 36 | Batch 57 | Loss: 0.42286452651023865\n",
      "Epoch 36 | Batch 58 | Loss: 0.30283957719802856\n",
      "Epoch 36 | Batch 59 | Loss: 0.765630841255188\n",
      "Epoch 36 | Batch 60 | Loss: 0.30835670232772827\n",
      "Epoch 36 | Batch 61 | Loss: 0.45858120918273926\n",
      "Epoch 36 | Batch 62 | Loss: 0.11294300854206085\n",
      "Epoch 36 | Batch 63 | Loss: 0.402058869600296\n",
      "Epoch 36 | Batch 64 | Loss: 0.10883819311857224\n",
      "Epoch 36 | Batch 65 | Loss: 0.556404709815979\n",
      "Epoch 36 | Batch 66 | Loss: 0.4357467293739319\n",
      "Epoch 36 | Batch 67 | Loss: 0.14247113466262817\n",
      "Epoch 36 | Batch 68 | Loss: 0.19812586903572083\n",
      "Epoch 36 | Batch 69 | Loss: 0.664204478263855\n",
      "Epoch 36 | Batch 70 | Loss: 0.19414159655570984\n",
      "Epoch 36 | Batch 71 | Loss: 0.3610607087612152\n",
      "Epoch 36 | Batch 72 | Loss: 1.08925461769104\n",
      "Epoch 36 | Batch 73 | Loss: 0.5724382996559143\n",
      "Epoch 36 | Batch 74 | Loss: 0.20787248015403748\n",
      "Epoch 36 | Batch 75 | Loss: 0.3204371929168701\n",
      "Epoch 36 | Batch 76 | Loss: 1.0218464136123657\n",
      "Epoch 36 | Batch 77 | Loss: 0.13793322443962097\n",
      "Epoch 36 | Batch 78 | Loss: 0.5510776042938232\n",
      "Epoch 36 | Batch 79 | Loss: 0.12120896577835083\n",
      "Epoch 36 | Batch 80 | Loss: 0.08946964144706726\n",
      "Epoch 36 | Batch 81 | Loss: 0.18614916503429413\n",
      "Epoch 36 | Batch 82 | Loss: 0.3507246971130371\n",
      "Epoch 36 | Batch 83 | Loss: 0.496831476688385\n",
      "Epoch 36 | Batch 84 | Loss: 0.2932249903678894\n",
      "Epoch 36 | Batch 85 | Loss: 0.4838026762008667\n",
      "Epoch 36 | Batch 86 | Loss: 0.5712955594062805\n",
      "Epoch 36 | Batch 87 | Loss: 0.41116687655448914\n",
      "Epoch 36 | Batch 88 | Loss: 0.14571893215179443\n",
      "Epoch 36 | Batch 89 | Loss: 0.34928420186042786\n",
      "Epoch 36 | Batch 90 | Loss: 0.03162349760532379\n",
      "Epoch 37 | Batch 1 | Loss: 0.35530880093574524\n",
      "Epoch 37 | Batch 2 | Loss: 0.3029341995716095\n",
      "Epoch 37 | Batch 3 | Loss: 0.5988982915878296\n",
      "Epoch 37 | Batch 4 | Loss: 0.4775516986846924\n",
      "Epoch 37 | Batch 5 | Loss: 0.45830363035202026\n",
      "Epoch 37 | Batch 6 | Loss: 0.16326719522476196\n",
      "Epoch 37 | Batch 7 | Loss: 0.41382285952568054\n",
      "Epoch 37 | Batch 8 | Loss: 0.16287365555763245\n",
      "Epoch 37 | Batch 9 | Loss: 0.4253697991371155\n",
      "Epoch 37 | Batch 10 | Loss: 0.19158509373664856\n",
      "Epoch 37 | Batch 11 | Loss: 0.6008906960487366\n",
      "Epoch 37 | Batch 12 | Loss: 0.16549909114837646\n",
      "Epoch 37 | Batch 13 | Loss: 0.3422401547431946\n",
      "Epoch 37 | Batch 14 | Loss: 0.5106925368309021\n",
      "Epoch 37 | Batch 15 | Loss: 0.5165882706642151\n",
      "Epoch 37 | Batch 16 | Loss: 0.39421653747558594\n",
      "Epoch 37 | Batch 17 | Loss: 0.2762874364852905\n",
      "Epoch 37 | Batch 18 | Loss: 0.12752656638622284\n",
      "Epoch 37 | Batch 19 | Loss: 0.5308189988136292\n",
      "Epoch 37 | Batch 20 | Loss: 0.16358743607997894\n",
      "Epoch 37 | Batch 21 | Loss: 0.36232811212539673\n",
      "Epoch 37 | Batch 22 | Loss: 0.16727137565612793\n",
      "Epoch 37 | Batch 23 | Loss: 0.14037221670150757\n",
      "Epoch 37 | Batch 24 | Loss: 0.7091501951217651\n",
      "Epoch 37 | Batch 25 | Loss: 0.2088104486465454\n",
      "Epoch 37 | Batch 26 | Loss: 0.41909515857696533\n",
      "Epoch 37 | Batch 27 | Loss: 0.17840515077114105\n",
      "Epoch 37 | Batch 28 | Loss: 0.24901017546653748\n",
      "Epoch 37 | Batch 29 | Loss: 0.21967554092407227\n",
      "Epoch 37 | Batch 30 | Loss: 0.33176761865615845\n",
      "Epoch 37 | Batch 31 | Loss: 0.32780808210372925\n",
      "Epoch 37 | Batch 32 | Loss: 0.23889382183551788\n",
      "Epoch 37 | Batch 33 | Loss: 0.24621689319610596\n",
      "Epoch 37 | Batch 34 | Loss: 0.19194120168685913\n",
      "Epoch 37 | Batch 35 | Loss: 0.11404135823249817\n",
      "Epoch 37 | Batch 36 | Loss: 0.477422297000885\n",
      "Epoch 37 | Batch 37 | Loss: 0.20616766810417175\n",
      "Epoch 37 | Batch 38 | Loss: 0.19020701944828033\n",
      "Epoch 37 | Batch 39 | Loss: 0.6159715056419373\n",
      "Epoch 37 | Batch 40 | Loss: 0.2538689970970154\n",
      "Epoch 37 | Batch 41 | Loss: 0.5364491939544678\n",
      "Epoch 37 | Batch 42 | Loss: 0.08836667239665985\n",
      "Epoch 37 | Batch 43 | Loss: 0.47721272706985474\n",
      "Epoch 37 | Batch 44 | Loss: 0.10661990195512772\n",
      "Epoch 37 | Batch 45 | Loss: 0.23036149144172668\n",
      "Epoch 37 | Batch 46 | Loss: 0.6734622716903687\n",
      "Epoch 37 | Batch 47 | Loss: 0.217536062002182\n",
      "Epoch 37 | Batch 48 | Loss: 0.4617593586444855\n",
      "Epoch 37 | Batch 49 | Loss: 0.286704421043396\n",
      "Epoch 37 | Batch 50 | Loss: 0.5203545689582825\n",
      "Epoch 37 | Batch 51 | Loss: 0.4461444020271301\n",
      "Epoch 37 | Batch 52 | Loss: 0.4049687385559082\n",
      "Epoch 37 | Batch 53 | Loss: 0.24494662880897522\n",
      "Epoch 37 | Batch 54 | Loss: 0.4760163128376007\n",
      "Epoch 37 | Batch 55 | Loss: 0.4800223410129547\n",
      "Epoch 37 | Batch 56 | Loss: 0.07956212759017944\n",
      "Epoch 37 | Batch 57 | Loss: 0.4489520192146301\n",
      "Epoch 37 | Batch 58 | Loss: 0.24125589430332184\n",
      "Epoch 37 | Batch 59 | Loss: 0.5745275020599365\n",
      "Epoch 37 | Batch 60 | Loss: 0.40429913997650146\n",
      "Epoch 37 | Batch 61 | Loss: 0.34448328614234924\n",
      "Epoch 37 | Batch 62 | Loss: 0.6261590719223022\n",
      "Epoch 37 | Batch 63 | Loss: 0.13183318078517914\n",
      "Epoch 37 | Batch 64 | Loss: 0.4929618239402771\n",
      "Epoch 37 | Batch 65 | Loss: 0.6982002854347229\n",
      "Epoch 37 | Batch 66 | Loss: 0.28022491931915283\n",
      "Epoch 37 | Batch 67 | Loss: 0.43991315364837646\n",
      "Epoch 37 | Batch 68 | Loss: 0.3723270893096924\n",
      "Epoch 37 | Batch 69 | Loss: 0.12597864866256714\n",
      "Epoch 37 | Batch 70 | Loss: 0.22937029600143433\n",
      "Epoch 37 | Batch 71 | Loss: 0.07049082219600677\n",
      "Epoch 37 | Batch 72 | Loss: 0.8151232004165649\n",
      "Epoch 37 | Batch 73 | Loss: 0.4205188751220703\n",
      "Epoch 37 | Batch 74 | Loss: 0.6128695607185364\n",
      "Epoch 37 | Batch 75 | Loss: 0.5444579124450684\n",
      "Epoch 37 | Batch 76 | Loss: 0.1123618334531784\n",
      "Epoch 37 | Batch 77 | Loss: 0.3211733102798462\n",
      "Epoch 37 | Batch 78 | Loss: 0.5599254369735718\n",
      "Epoch 37 | Batch 79 | Loss: 0.2861170768737793\n",
      "Epoch 37 | Batch 80 | Loss: 0.1678088754415512\n",
      "Epoch 37 | Batch 81 | Loss: 0.6680557727813721\n",
      "Epoch 37 | Batch 82 | Loss: 0.23276713490486145\n",
      "Epoch 37 | Batch 83 | Loss: 0.5012890696525574\n",
      "Epoch 37 | Batch 84 | Loss: 0.5813430547714233\n",
      "Epoch 37 | Batch 85 | Loss: 0.4243500232696533\n",
      "Epoch 37 | Batch 86 | Loss: 0.6672087907791138\n",
      "Epoch 37 | Batch 87 | Loss: 1.088562250137329\n",
      "Epoch 37 | Batch 88 | Loss: 0.5371752977371216\n",
      "Epoch 37 | Batch 89 | Loss: 0.12114827334880829\n",
      "Epoch 37 | Batch 90 | Loss: 0.16781136393547058\n",
      "Epoch 38 | Batch 1 | Loss: 0.3090774714946747\n",
      "Epoch 38 | Batch 2 | Loss: 0.25498226284980774\n",
      "Epoch 38 | Batch 3 | Loss: 0.44570469856262207\n",
      "Epoch 38 | Batch 4 | Loss: 0.3180896043777466\n",
      "Epoch 38 | Batch 5 | Loss: 0.4322587847709656\n",
      "Epoch 38 | Batch 6 | Loss: 0.20953567326068878\n",
      "Epoch 38 | Batch 7 | Loss: 0.3757566809654236\n",
      "Epoch 38 | Batch 8 | Loss: 0.41408175230026245\n",
      "Epoch 38 | Batch 9 | Loss: 0.33381226658821106\n",
      "Epoch 38 | Batch 10 | Loss: 0.5963838696479797\n",
      "Epoch 38 | Batch 11 | Loss: 0.31717449426651\n",
      "Epoch 38 | Batch 12 | Loss: 0.21757082641124725\n",
      "Epoch 38 | Batch 13 | Loss: 0.3302980661392212\n",
      "Epoch 38 | Batch 14 | Loss: 0.5195735692977905\n",
      "Epoch 38 | Batch 15 | Loss: 0.307955265045166\n",
      "Epoch 38 | Batch 16 | Loss: 0.13098788261413574\n",
      "Epoch 38 | Batch 17 | Loss: 0.13962098956108093\n",
      "Epoch 38 | Batch 18 | Loss: 0.4946001172065735\n",
      "Epoch 38 | Batch 19 | Loss: 0.5465261340141296\n",
      "Epoch 38 | Batch 20 | Loss: 0.7783215641975403\n",
      "Epoch 38 | Batch 21 | Loss: 0.08138881623744965\n",
      "Epoch 38 | Batch 22 | Loss: 0.25643712282180786\n",
      "Epoch 38 | Batch 23 | Loss: 0.157667875289917\n",
      "Epoch 38 | Batch 24 | Loss: 0.5751641392707825\n",
      "Epoch 38 | Batch 25 | Loss: 0.6975744962692261\n",
      "Epoch 38 | Batch 26 | Loss: 0.4277750849723816\n",
      "Epoch 38 | Batch 27 | Loss: 0.13571444153785706\n",
      "Epoch 38 | Batch 28 | Loss: 0.308758020401001\n",
      "Epoch 38 | Batch 29 | Loss: 0.19032453000545502\n",
      "Epoch 38 | Batch 30 | Loss: 0.17629191279411316\n",
      "Epoch 38 | Batch 31 | Loss: 0.5601768493652344\n",
      "Epoch 38 | Batch 32 | Loss: 0.13210716843605042\n",
      "Epoch 38 | Batch 33 | Loss: 0.4223293960094452\n",
      "Epoch 38 | Batch 34 | Loss: 0.6186484098434448\n",
      "Epoch 38 | Batch 35 | Loss: 0.1455295979976654\n",
      "Epoch 38 | Batch 36 | Loss: 0.33008864521980286\n",
      "Epoch 38 | Batch 37 | Loss: 0.2490333467721939\n",
      "Epoch 38 | Batch 38 | Loss: 0.25104472041130066\n",
      "Epoch 38 | Batch 39 | Loss: 0.08715426176786423\n",
      "Epoch 38 | Batch 40 | Loss: 0.24467498064041138\n",
      "Epoch 38 | Batch 41 | Loss: 0.44234246015548706\n",
      "Epoch 38 | Batch 42 | Loss: 0.4830438196659088\n",
      "Epoch 38 | Batch 43 | Loss: 0.21838456392288208\n",
      "Epoch 38 | Batch 44 | Loss: 0.47130927443504333\n",
      "Epoch 38 | Batch 45 | Loss: 0.3598115146160126\n",
      "Epoch 38 | Batch 46 | Loss: 0.6089911460876465\n",
      "Epoch 38 | Batch 47 | Loss: 0.3197101652622223\n",
      "Epoch 38 | Batch 48 | Loss: 0.10351424664258957\n",
      "Epoch 38 | Batch 49 | Loss: 0.6272326707839966\n",
      "Epoch 38 | Batch 50 | Loss: 0.4388776421546936\n",
      "Epoch 38 | Batch 51 | Loss: 0.3214429020881653\n",
      "Epoch 38 | Batch 52 | Loss: 0.6763748526573181\n",
      "Epoch 38 | Batch 53 | Loss: 0.3390991687774658\n",
      "Epoch 38 | Batch 54 | Loss: 0.8003832697868347\n",
      "Epoch 38 | Batch 55 | Loss: 0.3355078399181366\n",
      "Epoch 38 | Batch 56 | Loss: 0.4895401895046234\n",
      "Epoch 38 | Batch 57 | Loss: 0.23993434011936188\n",
      "Epoch 38 | Batch 58 | Loss: 0.5250349044799805\n",
      "Epoch 38 | Batch 59 | Loss: 0.40876248478889465\n",
      "Epoch 38 | Batch 60 | Loss: 0.46312275528907776\n",
      "Epoch 38 | Batch 61 | Loss: 0.3808670938014984\n",
      "Epoch 38 | Batch 62 | Loss: 0.3522963523864746\n",
      "Epoch 38 | Batch 63 | Loss: 0.48108503222465515\n",
      "Epoch 38 | Batch 64 | Loss: 0.5404692888259888\n",
      "Epoch 38 | Batch 65 | Loss: 0.11689504981040955\n",
      "Epoch 38 | Batch 66 | Loss: 0.27069956064224243\n",
      "Epoch 38 | Batch 67 | Loss: 0.1934485137462616\n",
      "Epoch 38 | Batch 68 | Loss: 0.6228998899459839\n",
      "Epoch 38 | Batch 69 | Loss: 0.6178315877914429\n",
      "Epoch 38 | Batch 70 | Loss: 0.12532058358192444\n",
      "Epoch 38 | Batch 71 | Loss: 0.8696637153625488\n",
      "Epoch 38 | Batch 72 | Loss: 0.5116505026817322\n",
      "Epoch 38 | Batch 73 | Loss: 0.36815232038497925\n",
      "Epoch 38 | Batch 74 | Loss: 0.22019554674625397\n",
      "Epoch 38 | Batch 75 | Loss: 0.4247363805770874\n",
      "Epoch 38 | Batch 76 | Loss: 0.31001049280166626\n",
      "Epoch 38 | Batch 77 | Loss: 0.38866615295410156\n",
      "Epoch 38 | Batch 78 | Loss: 0.33900222182273865\n",
      "Epoch 38 | Batch 79 | Loss: 0.3658316731452942\n",
      "Epoch 38 | Batch 80 | Loss: 0.26361191272735596\n",
      "Epoch 38 | Batch 81 | Loss: 0.5714964270591736\n",
      "Epoch 38 | Batch 82 | Loss: 0.3674861192703247\n",
      "Epoch 38 | Batch 83 | Loss: 0.18711014091968536\n",
      "Epoch 38 | Batch 84 | Loss: 0.11532019078731537\n",
      "Epoch 38 | Batch 85 | Loss: 0.5832039713859558\n",
      "Epoch 38 | Batch 86 | Loss: 0.6897435784339905\n",
      "Epoch 38 | Batch 87 | Loss: 0.3043898344039917\n",
      "Epoch 38 | Batch 88 | Loss: 0.6501629948616028\n",
      "Epoch 38 | Batch 89 | Loss: 0.11117026209831238\n",
      "Epoch 38 | Batch 90 | Loss: 0.18867403268814087\n",
      "Epoch 39 | Batch 1 | Loss: 0.11866310983896255\n",
      "Epoch 39 | Batch 2 | Loss: 0.11505438387393951\n",
      "Epoch 39 | Batch 3 | Loss: 0.7046980261802673\n",
      "Epoch 39 | Batch 4 | Loss: 0.2615261673927307\n",
      "Epoch 39 | Batch 5 | Loss: 0.34085673093795776\n",
      "Epoch 39 | Batch 6 | Loss: 0.0948120653629303\n",
      "Epoch 39 | Batch 7 | Loss: 0.40413716435432434\n",
      "Epoch 39 | Batch 8 | Loss: 0.5738213658332825\n",
      "Epoch 39 | Batch 9 | Loss: 0.11714775860309601\n",
      "Epoch 39 | Batch 10 | Loss: 0.29061537981033325\n",
      "Epoch 39 | Batch 11 | Loss: 0.7433663010597229\n",
      "Epoch 39 | Batch 12 | Loss: 0.47063836455345154\n",
      "Epoch 39 | Batch 13 | Loss: 0.14261193573474884\n",
      "Epoch 39 | Batch 14 | Loss: 0.3132380247116089\n",
      "Epoch 39 | Batch 15 | Loss: 0.3014725148677826\n",
      "Epoch 39 | Batch 16 | Loss: 0.2665778696537018\n",
      "Epoch 39 | Batch 17 | Loss: 0.9577270746231079\n",
      "Epoch 39 | Batch 18 | Loss: 0.23495155572891235\n",
      "Epoch 39 | Batch 19 | Loss: 0.3449021279811859\n",
      "Epoch 39 | Batch 20 | Loss: 0.6125182509422302\n",
      "Epoch 39 | Batch 21 | Loss: 0.4332326054573059\n",
      "Epoch 39 | Batch 22 | Loss: 0.728079080581665\n",
      "Epoch 39 | Batch 23 | Loss: 0.13469651341438293\n",
      "Epoch 39 | Batch 24 | Loss: 0.39157116413116455\n",
      "Epoch 39 | Batch 25 | Loss: 0.443214476108551\n",
      "Epoch 39 | Batch 26 | Loss: 0.27123719453811646\n",
      "Epoch 39 | Batch 27 | Loss: 0.39185717701911926\n",
      "Epoch 39 | Batch 28 | Loss: 0.4094923734664917\n",
      "Epoch 39 | Batch 29 | Loss: 0.4176459312438965\n",
      "Epoch 39 | Batch 30 | Loss: 0.5194442272186279\n",
      "Epoch 39 | Batch 31 | Loss: 0.36087533831596375\n",
      "Epoch 39 | Batch 32 | Loss: 0.18648210167884827\n",
      "Epoch 39 | Batch 33 | Loss: 0.3986704349517822\n",
      "Epoch 39 | Batch 34 | Loss: 0.3809632360935211\n",
      "Epoch 39 | Batch 35 | Loss: 0.5604043006896973\n",
      "Epoch 39 | Batch 36 | Loss: 0.5008541345596313\n",
      "Epoch 39 | Batch 37 | Loss: 0.10588148981332779\n",
      "Epoch 39 | Batch 38 | Loss: 0.21027858555316925\n",
      "Epoch 39 | Batch 39 | Loss: 0.5348930358886719\n",
      "Epoch 39 | Batch 40 | Loss: 0.177642360329628\n",
      "Epoch 39 | Batch 41 | Loss: 0.24292299151420593\n",
      "Epoch 39 | Batch 42 | Loss: 0.28476572036743164\n",
      "Epoch 39 | Batch 43 | Loss: 0.37282848358154297\n",
      "Epoch 39 | Batch 44 | Loss: 0.3466201722621918\n",
      "Epoch 39 | Batch 45 | Loss: 0.5840628147125244\n",
      "Epoch 39 | Batch 46 | Loss: 0.40084609389305115\n",
      "Epoch 39 | Batch 47 | Loss: 0.5369770526885986\n",
      "Epoch 39 | Batch 48 | Loss: 0.26896533370018005\n",
      "Epoch 39 | Batch 49 | Loss: 0.18118149042129517\n",
      "Epoch 39 | Batch 50 | Loss: 0.4241601228713989\n",
      "Epoch 39 | Batch 51 | Loss: 0.3387610912322998\n",
      "Epoch 39 | Batch 52 | Loss: 0.2620261609554291\n",
      "Epoch 39 | Batch 53 | Loss: 0.17237171530723572\n",
      "Epoch 39 | Batch 54 | Loss: 0.14239299297332764\n",
      "Epoch 39 | Batch 55 | Loss: 0.7402559518814087\n",
      "Epoch 39 | Batch 56 | Loss: 0.6019958853721619\n",
      "Epoch 39 | Batch 57 | Loss: 0.10870793461799622\n",
      "Epoch 39 | Batch 58 | Loss: 0.4812750518321991\n",
      "Epoch 39 | Batch 59 | Loss: 0.29210326075553894\n",
      "Epoch 39 | Batch 60 | Loss: 0.4272485673427582\n",
      "Epoch 39 | Batch 61 | Loss: 0.22531810402870178\n",
      "Epoch 39 | Batch 62 | Loss: 0.2570750117301941\n",
      "Epoch 39 | Batch 63 | Loss: 0.3660256266593933\n",
      "Epoch 39 | Batch 64 | Loss: 0.467586874961853\n",
      "Epoch 39 | Batch 65 | Loss: 0.8186716437339783\n",
      "Epoch 39 | Batch 66 | Loss: 0.6188085675239563\n",
      "Epoch 39 | Batch 67 | Loss: 0.09741105139255524\n",
      "Epoch 39 | Batch 68 | Loss: 0.7328715324401855\n",
      "Epoch 39 | Batch 69 | Loss: 0.15366873145103455\n",
      "Epoch 39 | Batch 70 | Loss: 0.21001845598220825\n",
      "Epoch 39 | Batch 71 | Loss: 0.5030031800270081\n",
      "Epoch 39 | Batch 72 | Loss: 0.932831883430481\n",
      "Epoch 39 | Batch 73 | Loss: 0.13658082485198975\n",
      "Epoch 39 | Batch 74 | Loss: 0.18697431683540344\n",
      "Epoch 39 | Batch 75 | Loss: 0.42345932126045227\n",
      "Epoch 39 | Batch 76 | Loss: 0.415717214345932\n",
      "Epoch 39 | Batch 77 | Loss: 0.34533753991127014\n",
      "Epoch 39 | Batch 78 | Loss: 0.3919590711593628\n",
      "Epoch 39 | Batch 79 | Loss: 0.18457943201065063\n",
      "Epoch 39 | Batch 80 | Loss: 0.268572598695755\n",
      "Epoch 39 | Batch 81 | Loss: 0.44024932384490967\n",
      "Epoch 39 | Batch 82 | Loss: 0.4018423855304718\n",
      "Epoch 39 | Batch 83 | Loss: 0.15744394063949585\n",
      "Epoch 39 | Batch 84 | Loss: 0.3026208281517029\n",
      "Epoch 39 | Batch 85 | Loss: 0.5681964159011841\n",
      "Epoch 39 | Batch 86 | Loss: 0.2716304063796997\n",
      "Epoch 39 | Batch 87 | Loss: 0.24364610016345978\n",
      "Epoch 39 | Batch 88 | Loss: 0.5127468109130859\n",
      "Epoch 39 | Batch 89 | Loss: 0.09793451428413391\n",
      "Epoch 39 | Batch 90 | Loss: 1.2436060905456543\n",
      "Epoch 40 | Batch 1 | Loss: 0.2478608787059784\n",
      "Epoch 40 | Batch 2 | Loss: 0.3063194155693054\n",
      "Epoch 40 | Batch 3 | Loss: 0.35119354724884033\n",
      "Epoch 40 | Batch 4 | Loss: 0.830765962600708\n",
      "Epoch 40 | Batch 5 | Loss: 0.19301781058311462\n",
      "Epoch 40 | Batch 6 | Loss: 0.41584455966949463\n",
      "Epoch 40 | Batch 7 | Loss: 0.24779805541038513\n",
      "Epoch 40 | Batch 8 | Loss: 0.22578684985637665\n",
      "Epoch 40 | Batch 9 | Loss: 0.23308759927749634\n",
      "Epoch 40 | Batch 10 | Loss: 0.347031831741333\n",
      "Epoch 40 | Batch 11 | Loss: 0.47655415534973145\n",
      "Epoch 40 | Batch 12 | Loss: 0.37610292434692383\n",
      "Epoch 40 | Batch 13 | Loss: 0.2077634334564209\n",
      "Epoch 40 | Batch 14 | Loss: 0.1641530692577362\n",
      "Epoch 40 | Batch 15 | Loss: 0.37294748425483704\n",
      "Epoch 40 | Batch 16 | Loss: 0.3562554121017456\n",
      "Epoch 40 | Batch 17 | Loss: 0.18197381496429443\n",
      "Epoch 40 | Batch 18 | Loss: 0.14504943788051605\n",
      "Epoch 40 | Batch 19 | Loss: 0.6991418600082397\n",
      "Epoch 40 | Batch 20 | Loss: 0.5115746259689331\n",
      "Epoch 40 | Batch 21 | Loss: 0.3665759563446045\n",
      "Epoch 40 | Batch 22 | Loss: 0.20980125665664673\n",
      "Epoch 40 | Batch 23 | Loss: 0.5105811357498169\n",
      "Epoch 40 | Batch 24 | Loss: 0.25580114126205444\n",
      "Epoch 40 | Batch 25 | Loss: 0.24561211466789246\n",
      "Epoch 40 | Batch 26 | Loss: 0.4916144907474518\n",
      "Epoch 40 | Batch 27 | Loss: 0.16606521606445312\n",
      "Epoch 40 | Batch 28 | Loss: 0.15541492402553558\n",
      "Epoch 40 | Batch 29 | Loss: 0.17547549307346344\n",
      "Epoch 40 | Batch 30 | Loss: 0.2559397220611572\n",
      "Epoch 40 | Batch 31 | Loss: 0.14322209358215332\n",
      "Epoch 40 | Batch 32 | Loss: 0.1481046974658966\n",
      "Epoch 40 | Batch 33 | Loss: 0.2741892337799072\n",
      "Epoch 40 | Batch 34 | Loss: 0.23540334403514862\n",
      "Epoch 40 | Batch 35 | Loss: 0.5302212238311768\n",
      "Epoch 40 | Batch 36 | Loss: 0.47706085443496704\n",
      "Epoch 40 | Batch 37 | Loss: 0.546331524848938\n",
      "Epoch 40 | Batch 38 | Loss: 0.786336362361908\n",
      "Epoch 40 | Batch 39 | Loss: 0.5283470749855042\n",
      "Epoch 40 | Batch 40 | Loss: 0.2559499740600586\n",
      "Epoch 40 | Batch 41 | Loss: 0.058493517339229584\n",
      "Epoch 40 | Batch 42 | Loss: 0.300275057554245\n",
      "Epoch 40 | Batch 43 | Loss: 0.4285942018032074\n",
      "Epoch 40 | Batch 44 | Loss: 0.4964572489261627\n",
      "Epoch 40 | Batch 45 | Loss: 0.5333055257797241\n",
      "Epoch 40 | Batch 46 | Loss: 0.44445914030075073\n",
      "Epoch 40 | Batch 47 | Loss: 0.16734439134597778\n",
      "Epoch 40 | Batch 48 | Loss: 0.3437524437904358\n",
      "Epoch 40 | Batch 49 | Loss: 0.4591123163700104\n",
      "Epoch 40 | Batch 50 | Loss: 0.21080036461353302\n",
      "Epoch 40 | Batch 51 | Loss: 0.5996111631393433\n",
      "Epoch 40 | Batch 52 | Loss: 0.21679610013961792\n",
      "Epoch 40 | Batch 53 | Loss: 0.10429980605840683\n",
      "Epoch 40 | Batch 54 | Loss: 0.5383367538452148\n",
      "Epoch 40 | Batch 55 | Loss: 1.0879908800125122\n",
      "Epoch 40 | Batch 56 | Loss: 0.7089194059371948\n",
      "Epoch 40 | Batch 57 | Loss: 0.7372069954872131\n",
      "Epoch 40 | Batch 58 | Loss: 0.26896318793296814\n",
      "Epoch 40 | Batch 59 | Loss: 0.6922435164451599\n",
      "Epoch 40 | Batch 60 | Loss: 0.42682084441185\n",
      "Epoch 40 | Batch 61 | Loss: 0.2733606696128845\n",
      "Epoch 40 | Batch 62 | Loss: 0.437037855386734\n",
      "Epoch 40 | Batch 63 | Loss: 0.6466009020805359\n",
      "Epoch 40 | Batch 64 | Loss: 0.06456619501113892\n",
      "Epoch 40 | Batch 65 | Loss: 0.5045884847640991\n",
      "Epoch 40 | Batch 66 | Loss: 0.2834432125091553\n",
      "Epoch 40 | Batch 67 | Loss: 0.3769586384296417\n",
      "Epoch 40 | Batch 68 | Loss: 0.43892860412597656\n",
      "Epoch 40 | Batch 69 | Loss: 0.13505679368972778\n",
      "Epoch 40 | Batch 70 | Loss: 0.2744295597076416\n",
      "Epoch 40 | Batch 71 | Loss: 0.20344142615795135\n",
      "Epoch 40 | Batch 72 | Loss: 0.20339155197143555\n",
      "Epoch 40 | Batch 73 | Loss: 0.5667152404785156\n",
      "Epoch 40 | Batch 74 | Loss: 0.28444331884384155\n",
      "Epoch 40 | Batch 75 | Loss: 0.5441709756851196\n",
      "Epoch 40 | Batch 76 | Loss: 0.23607137799263\n",
      "Epoch 40 | Batch 77 | Loss: 0.35061901807785034\n",
      "Epoch 40 | Batch 78 | Loss: 0.4628937542438507\n",
      "Epoch 40 | Batch 79 | Loss: 0.2930554449558258\n",
      "Epoch 40 | Batch 80 | Loss: 0.33486995100975037\n",
      "Epoch 40 | Batch 81 | Loss: 0.1947064995765686\n",
      "Epoch 40 | Batch 82 | Loss: 0.30922526121139526\n",
      "Epoch 40 | Batch 83 | Loss: 0.6676839590072632\n",
      "Epoch 40 | Batch 84 | Loss: 0.7573642134666443\n",
      "Epoch 40 | Batch 85 | Loss: 0.6206652522087097\n",
      "Epoch 40 | Batch 86 | Loss: 0.2183002382516861\n",
      "Epoch 40 | Batch 87 | Loss: 0.35884857177734375\n",
      "Epoch 40 | Batch 88 | Loss: 0.09657350182533264\n",
      "Epoch 40 | Batch 89 | Loss: 0.6230475902557373\n",
      "Epoch 40 | Batch 90 | Loss: 0.6817453503608704\n",
      "Epoch 41 | Batch 1 | Loss: 0.08507949113845825\n",
      "Epoch 41 | Batch 2 | Loss: 0.4231095314025879\n",
      "Epoch 41 | Batch 3 | Loss: 0.41410568356513977\n",
      "Epoch 41 | Batch 4 | Loss: 0.37635183334350586\n",
      "Epoch 41 | Batch 5 | Loss: 0.41216784715652466\n",
      "Epoch 41 | Batch 6 | Loss: 0.5001862049102783\n",
      "Epoch 41 | Batch 7 | Loss: 0.7092196941375732\n",
      "Epoch 41 | Batch 8 | Loss: 0.47246208786964417\n",
      "Epoch 41 | Batch 9 | Loss: 0.5289992094039917\n",
      "Epoch 41 | Batch 10 | Loss: 0.2217121571302414\n",
      "Epoch 41 | Batch 11 | Loss: 0.648716151714325\n",
      "Epoch 41 | Batch 12 | Loss: 0.22962582111358643\n",
      "Epoch 41 | Batch 13 | Loss: 0.3611597716808319\n",
      "Epoch 41 | Batch 14 | Loss: 0.2691861391067505\n",
      "Epoch 41 | Batch 15 | Loss: 0.6226783990859985\n",
      "Epoch 41 | Batch 16 | Loss: 0.14534640312194824\n",
      "Epoch 41 | Batch 17 | Loss: 0.676015317440033\n",
      "Epoch 41 | Batch 18 | Loss: 0.3604210615158081\n",
      "Epoch 41 | Batch 19 | Loss: 0.31589773297309875\n",
      "Epoch 41 | Batch 20 | Loss: 0.5666216611862183\n",
      "Epoch 41 | Batch 21 | Loss: 0.10107527673244476\n",
      "Epoch 41 | Batch 22 | Loss: 0.5911333560943604\n",
      "Epoch 41 | Batch 23 | Loss: 0.6139523983001709\n",
      "Epoch 41 | Batch 24 | Loss: 0.2703819274902344\n",
      "Epoch 41 | Batch 25 | Loss: 0.4353256821632385\n",
      "Epoch 41 | Batch 26 | Loss: 0.24678559601306915\n",
      "Epoch 41 | Batch 27 | Loss: 0.2222157120704651\n",
      "Epoch 41 | Batch 28 | Loss: 0.4980713725090027\n",
      "Epoch 41 | Batch 29 | Loss: 0.4998295307159424\n",
      "Epoch 41 | Batch 30 | Loss: 0.2803844213485718\n",
      "Epoch 41 | Batch 31 | Loss: 0.16756391525268555\n",
      "Epoch 41 | Batch 32 | Loss: 0.4566890001296997\n",
      "Epoch 41 | Batch 33 | Loss: 0.33723363280296326\n",
      "Epoch 41 | Batch 34 | Loss: 0.5855746865272522\n",
      "Epoch 41 | Batch 35 | Loss: 0.35932403802871704\n",
      "Epoch 41 | Batch 36 | Loss: 0.22182689607143402\n",
      "Epoch 41 | Batch 37 | Loss: 0.25319400429725647\n",
      "Epoch 41 | Batch 38 | Loss: 0.14358673989772797\n",
      "Epoch 41 | Batch 39 | Loss: 0.20557790994644165\n",
      "Epoch 41 | Batch 40 | Loss: 0.3476150631904602\n",
      "Epoch 41 | Batch 41 | Loss: 0.13073496520519257\n",
      "Epoch 41 | Batch 42 | Loss: 0.3398666977882385\n",
      "Epoch 41 | Batch 43 | Loss: 0.14837482571601868\n",
      "Epoch 41 | Batch 44 | Loss: 0.32433244585990906\n",
      "Epoch 41 | Batch 45 | Loss: 0.924437940120697\n",
      "Epoch 41 | Batch 46 | Loss: 0.48798415064811707\n",
      "Epoch 41 | Batch 47 | Loss: 0.3596881031990051\n",
      "Epoch 41 | Batch 48 | Loss: 0.5007632970809937\n",
      "Epoch 41 | Batch 49 | Loss: 0.3316209018230438\n",
      "Epoch 41 | Batch 50 | Loss: 0.26291918754577637\n",
      "Epoch 41 | Batch 51 | Loss: 0.5340461730957031\n",
      "Epoch 41 | Batch 52 | Loss: 0.5188232064247131\n",
      "Epoch 41 | Batch 53 | Loss: 0.6481409072875977\n",
      "Epoch 41 | Batch 54 | Loss: 0.4078250527381897\n",
      "Epoch 41 | Batch 55 | Loss: 0.3137640357017517\n",
      "Epoch 41 | Batch 56 | Loss: 0.3665432035923004\n",
      "Epoch 41 | Batch 57 | Loss: 0.7637642621994019\n",
      "Epoch 41 | Batch 58 | Loss: 0.4965502619743347\n",
      "Epoch 41 | Batch 59 | Loss: 0.5851365327835083\n",
      "Epoch 41 | Batch 60 | Loss: 0.5029440522193909\n",
      "Epoch 41 | Batch 61 | Loss: 0.18316681683063507\n",
      "Epoch 41 | Batch 62 | Loss: 0.4586890637874603\n",
      "Epoch 41 | Batch 63 | Loss: 0.6917551755905151\n",
      "Epoch 41 | Batch 64 | Loss: 0.23785194754600525\n",
      "Epoch 41 | Batch 65 | Loss: 0.1228131651878357\n",
      "Epoch 41 | Batch 66 | Loss: 0.17444847524166107\n",
      "Epoch 41 | Batch 67 | Loss: 0.5150505304336548\n",
      "Epoch 41 | Batch 68 | Loss: 0.2956075966358185\n",
      "Epoch 41 | Batch 69 | Loss: 0.326122522354126\n",
      "Epoch 41 | Batch 70 | Loss: 0.3108842372894287\n",
      "Epoch 41 | Batch 71 | Loss: 0.6206318140029907\n",
      "Epoch 41 | Batch 72 | Loss: 0.18100860714912415\n",
      "Epoch 41 | Batch 73 | Loss: 0.34918883442878723\n",
      "Epoch 41 | Batch 74 | Loss: 0.1847010999917984\n",
      "Epoch 41 | Batch 75 | Loss: 0.24221786856651306\n",
      "Epoch 41 | Batch 76 | Loss: 0.331754207611084\n",
      "Epoch 41 | Batch 77 | Loss: 0.4450686573982239\n",
      "Epoch 41 | Batch 78 | Loss: 0.3144434690475464\n",
      "Epoch 41 | Batch 79 | Loss: 0.10203689336776733\n",
      "Epoch 41 | Batch 80 | Loss: 0.1204422265291214\n",
      "Epoch 41 | Batch 81 | Loss: 0.08377239853143692\n",
      "Epoch 41 | Batch 82 | Loss: 0.5935312509536743\n",
      "Epoch 41 | Batch 83 | Loss: 0.15291212499141693\n",
      "Epoch 41 | Batch 84 | Loss: 0.43265026807785034\n",
      "Epoch 41 | Batch 85 | Loss: 0.5410430431365967\n",
      "Epoch 41 | Batch 86 | Loss: 0.15276962518692017\n",
      "Epoch 41 | Batch 87 | Loss: 0.2841414511203766\n",
      "Epoch 41 | Batch 88 | Loss: 0.6800845861434937\n",
      "Epoch 41 | Batch 89 | Loss: 0.19143426418304443\n",
      "Epoch 41 | Batch 90 | Loss: 0.6446313858032227\n",
      "Epoch 42 | Batch 1 | Loss: 0.35974839329719543\n",
      "Epoch 42 | Batch 2 | Loss: 0.3406749665737152\n",
      "Epoch 42 | Batch 3 | Loss: 0.33931660652160645\n",
      "Epoch 42 | Batch 4 | Loss: 0.46174412965774536\n",
      "Epoch 42 | Batch 5 | Loss: 0.33331871032714844\n",
      "Epoch 42 | Batch 6 | Loss: 0.6528223752975464\n",
      "Epoch 42 | Batch 7 | Loss: 0.19004419445991516\n",
      "Epoch 42 | Batch 8 | Loss: 0.2237854152917862\n",
      "Epoch 42 | Batch 9 | Loss: 0.8515510559082031\n",
      "Epoch 42 | Batch 10 | Loss: 0.5582793951034546\n",
      "Epoch 42 | Batch 11 | Loss: 0.14483880996704102\n",
      "Epoch 42 | Batch 12 | Loss: 0.44576117396354675\n",
      "Epoch 42 | Batch 13 | Loss: 1.0750939846038818\n",
      "Epoch 42 | Batch 14 | Loss: 0.30283430218696594\n",
      "Epoch 42 | Batch 15 | Loss: 0.21275971829891205\n",
      "Epoch 42 | Batch 16 | Loss: 0.15458346903324127\n",
      "Epoch 42 | Batch 17 | Loss: 0.724174439907074\n",
      "Epoch 42 | Batch 18 | Loss: 0.33959251642227173\n",
      "Epoch 42 | Batch 19 | Loss: 0.1529715657234192\n",
      "Epoch 42 | Batch 20 | Loss: 0.19232705235481262\n",
      "Epoch 42 | Batch 21 | Loss: 0.3980056643486023\n",
      "Epoch 42 | Batch 22 | Loss: 0.14807343482971191\n",
      "Epoch 42 | Batch 23 | Loss: 0.3812277913093567\n",
      "Epoch 42 | Batch 24 | Loss: 0.28284990787506104\n",
      "Epoch 42 | Batch 25 | Loss: 0.5623679161071777\n",
      "Epoch 42 | Batch 26 | Loss: 0.4283420443534851\n",
      "Epoch 42 | Batch 27 | Loss: 0.4185432195663452\n",
      "Epoch 42 | Batch 28 | Loss: 0.31858527660369873\n",
      "Epoch 42 | Batch 29 | Loss: 0.30315637588500977\n",
      "Epoch 42 | Batch 30 | Loss: 0.35093435645103455\n",
      "Epoch 42 | Batch 31 | Loss: 0.3740539252758026\n",
      "Epoch 42 | Batch 32 | Loss: 0.5255213379859924\n",
      "Epoch 42 | Batch 33 | Loss: 0.2731255888938904\n",
      "Epoch 42 | Batch 34 | Loss: 0.08228135108947754\n",
      "Epoch 42 | Batch 35 | Loss: 0.5869104862213135\n",
      "Epoch 42 | Batch 36 | Loss: 0.6272075176239014\n",
      "Epoch 42 | Batch 37 | Loss: 0.29522258043289185\n",
      "Epoch 42 | Batch 38 | Loss: 0.365488201379776\n",
      "Epoch 42 | Batch 39 | Loss: 0.7273749709129333\n",
      "Epoch 42 | Batch 40 | Loss: 0.2043018341064453\n",
      "Epoch 42 | Batch 41 | Loss: 0.4058428108692169\n",
      "Epoch 42 | Batch 42 | Loss: 0.1721932739019394\n",
      "Epoch 42 | Batch 43 | Loss: 0.39549720287323\n",
      "Epoch 42 | Batch 44 | Loss: 0.36273643374443054\n",
      "Epoch 42 | Batch 45 | Loss: 0.18558207154273987\n",
      "Epoch 42 | Batch 46 | Loss: 0.18306756019592285\n",
      "Epoch 42 | Batch 47 | Loss: 0.24751609563827515\n",
      "Epoch 42 | Batch 48 | Loss: 0.2949948310852051\n",
      "Epoch 42 | Batch 49 | Loss: 0.4440023601055145\n",
      "Epoch 42 | Batch 50 | Loss: 0.5110830664634705\n",
      "Epoch 42 | Batch 51 | Loss: 0.24399931728839874\n",
      "Epoch 42 | Batch 52 | Loss: 0.16539785265922546\n",
      "Epoch 42 | Batch 53 | Loss: 0.2717570662498474\n",
      "Epoch 42 | Batch 54 | Loss: 0.2875158488750458\n",
      "Epoch 42 | Batch 55 | Loss: 0.4911140203475952\n",
      "Epoch 42 | Batch 56 | Loss: 0.21482069790363312\n",
      "Epoch 42 | Batch 57 | Loss: 0.12297512590885162\n",
      "Epoch 42 | Batch 58 | Loss: 0.29467394948005676\n",
      "Epoch 42 | Batch 59 | Loss: 0.6951460838317871\n",
      "Epoch 42 | Batch 60 | Loss: 0.1238531544804573\n",
      "Epoch 42 | Batch 61 | Loss: 0.3004860281944275\n",
      "Epoch 42 | Batch 62 | Loss: 0.2971233129501343\n",
      "Epoch 42 | Batch 63 | Loss: 0.38468605279922485\n",
      "Epoch 42 | Batch 64 | Loss: 0.6570243239402771\n",
      "Epoch 42 | Batch 65 | Loss: 0.09912806749343872\n",
      "Epoch 42 | Batch 66 | Loss: 0.3766340911388397\n",
      "Epoch 42 | Batch 67 | Loss: 0.6480019092559814\n",
      "Epoch 42 | Batch 68 | Loss: 0.603344738483429\n",
      "Epoch 42 | Batch 69 | Loss: 0.4241829514503479\n",
      "Epoch 42 | Batch 70 | Loss: 0.1530369222164154\n",
      "Epoch 42 | Batch 71 | Loss: 0.3870796859264374\n",
      "Epoch 42 | Batch 72 | Loss: 0.08793491125106812\n",
      "Epoch 42 | Batch 73 | Loss: 0.4670692980289459\n",
      "Epoch 42 | Batch 74 | Loss: 0.31541794538497925\n",
      "Epoch 42 | Batch 75 | Loss: 0.09733467549085617\n",
      "Epoch 42 | Batch 76 | Loss: 0.37830251455307007\n",
      "Epoch 42 | Batch 77 | Loss: 0.30658772587776184\n",
      "Epoch 42 | Batch 78 | Loss: 0.06481640785932541\n",
      "Epoch 42 | Batch 79 | Loss: 0.4744495749473572\n",
      "Epoch 42 | Batch 80 | Loss: 0.18187075853347778\n",
      "Epoch 42 | Batch 81 | Loss: 0.6041005849838257\n",
      "Epoch 42 | Batch 82 | Loss: 0.6892248392105103\n",
      "Epoch 42 | Batch 83 | Loss: 0.5471482276916504\n",
      "Epoch 42 | Batch 84 | Loss: 0.5573840737342834\n",
      "Epoch 42 | Batch 85 | Loss: 0.573236346244812\n",
      "Epoch 42 | Batch 86 | Loss: 0.35395169258117676\n",
      "Epoch 42 | Batch 87 | Loss: 0.14534881711006165\n",
      "Epoch 42 | Batch 88 | Loss: 0.42130082845687866\n",
      "Epoch 42 | Batch 89 | Loss: 1.2645864486694336\n",
      "Epoch 42 | Batch 90 | Loss: 0.6840428709983826\n",
      "Epoch 43 | Batch 1 | Loss: 0.40159502625465393\n",
      "Epoch 43 | Batch 2 | Loss: 0.6836885809898376\n",
      "Epoch 43 | Batch 3 | Loss: 0.1720736026763916\n",
      "Epoch 43 | Batch 4 | Loss: 0.2540757358074188\n",
      "Epoch 43 | Batch 5 | Loss: 0.5391892194747925\n",
      "Epoch 43 | Batch 6 | Loss: 0.35536637902259827\n",
      "Epoch 43 | Batch 7 | Loss: 0.23094424605369568\n",
      "Epoch 43 | Batch 8 | Loss: 0.14077739417552948\n",
      "Epoch 43 | Batch 9 | Loss: 0.16242384910583496\n",
      "Epoch 43 | Batch 10 | Loss: 0.618578314781189\n",
      "Epoch 43 | Batch 11 | Loss: 0.23841695487499237\n",
      "Epoch 43 | Batch 12 | Loss: 0.15829923748970032\n",
      "Epoch 43 | Batch 13 | Loss: 0.4122152030467987\n",
      "Epoch 43 | Batch 14 | Loss: 0.37855952978134155\n",
      "Epoch 43 | Batch 15 | Loss: 0.652079701423645\n",
      "Epoch 43 | Batch 16 | Loss: 0.38548657298088074\n",
      "Epoch 43 | Batch 17 | Loss: 0.24618162214756012\n",
      "Epoch 43 | Batch 18 | Loss: 0.4049545228481293\n",
      "Epoch 43 | Batch 19 | Loss: 0.4925936460494995\n",
      "Epoch 43 | Batch 20 | Loss: 0.17636051774024963\n",
      "Epoch 43 | Batch 21 | Loss: 0.8932316303253174\n",
      "Epoch 43 | Batch 22 | Loss: 0.41879093647003174\n",
      "Epoch 43 | Batch 23 | Loss: 0.18908581137657166\n",
      "Epoch 43 | Batch 24 | Loss: 0.7320010662078857\n",
      "Epoch 43 | Batch 25 | Loss: 0.21208234131336212\n",
      "Epoch 43 | Batch 26 | Loss: 0.5261930823326111\n",
      "Epoch 43 | Batch 27 | Loss: 0.6422127485275269\n",
      "Epoch 43 | Batch 28 | Loss: 0.41555914282798767\n",
      "Epoch 43 | Batch 29 | Loss: 0.3455200493335724\n",
      "Epoch 43 | Batch 30 | Loss: 0.09922345727682114\n",
      "Epoch 43 | Batch 31 | Loss: 0.2529847025871277\n",
      "Epoch 43 | Batch 32 | Loss: 0.11678367853164673\n",
      "Epoch 43 | Batch 33 | Loss: 0.2841961681842804\n",
      "Epoch 43 | Batch 34 | Loss: 0.3725242018699646\n",
      "Epoch 43 | Batch 35 | Loss: 0.26795780658721924\n",
      "Epoch 43 | Batch 36 | Loss: 0.2810811996459961\n",
      "Epoch 43 | Batch 37 | Loss: 0.5219359397888184\n",
      "Epoch 43 | Batch 38 | Loss: 0.41577577590942383\n",
      "Epoch 43 | Batch 39 | Loss: 0.20072105526924133\n",
      "Epoch 43 | Batch 40 | Loss: 0.39757370948791504\n",
      "Epoch 43 | Batch 41 | Loss: 0.08761128783226013\n",
      "Epoch 43 | Batch 42 | Loss: 0.30562835931777954\n",
      "Epoch 43 | Batch 43 | Loss: 0.5344212651252747\n",
      "Epoch 43 | Batch 44 | Loss: 1.167132019996643\n",
      "Epoch 43 | Batch 45 | Loss: 0.22993047535419464\n",
      "Epoch 43 | Batch 46 | Loss: 0.26554176211357117\n",
      "Epoch 43 | Batch 47 | Loss: 0.6806179285049438\n",
      "Epoch 43 | Batch 48 | Loss: 0.1303110122680664\n",
      "Epoch 43 | Batch 49 | Loss: 0.31280359625816345\n",
      "Epoch 43 | Batch 50 | Loss: 0.7048056125640869\n",
      "Epoch 43 | Batch 51 | Loss: 0.3132917284965515\n",
      "Epoch 43 | Batch 52 | Loss: 0.20117352902889252\n",
      "Epoch 43 | Batch 53 | Loss: 1.2292323112487793\n",
      "Epoch 43 | Batch 54 | Loss: 0.05442638695240021\n",
      "Epoch 43 | Batch 55 | Loss: 0.34629902243614197\n",
      "Epoch 43 | Batch 56 | Loss: 0.2092779278755188\n",
      "Epoch 43 | Batch 57 | Loss: 0.29599183797836304\n",
      "Epoch 43 | Batch 58 | Loss: 0.1926727294921875\n",
      "Epoch 43 | Batch 59 | Loss: 0.1636248081922531\n",
      "Epoch 43 | Batch 60 | Loss: 0.4324577748775482\n",
      "Epoch 43 | Batch 61 | Loss: 0.16144371032714844\n",
      "Epoch 43 | Batch 62 | Loss: 0.5806753635406494\n",
      "Epoch 43 | Batch 63 | Loss: 0.10857072472572327\n",
      "Epoch 43 | Batch 64 | Loss: 0.4140874147415161\n",
      "Epoch 43 | Batch 65 | Loss: 0.48200055956840515\n",
      "Epoch 43 | Batch 66 | Loss: 0.19071537256240845\n",
      "Epoch 43 | Batch 67 | Loss: 0.2740287780761719\n",
      "Epoch 43 | Batch 68 | Loss: 0.6578719615936279\n",
      "Epoch 43 | Batch 69 | Loss: 0.524215817451477\n",
      "Epoch 43 | Batch 70 | Loss: 0.3196870684623718\n",
      "Epoch 43 | Batch 71 | Loss: 0.36187905073165894\n",
      "Epoch 43 | Batch 72 | Loss: 0.12097321450710297\n",
      "Epoch 43 | Batch 73 | Loss: 0.2763243615627289\n",
      "Epoch 43 | Batch 74 | Loss: 0.6920033097267151\n",
      "Epoch 43 | Batch 75 | Loss: 0.49060460925102234\n",
      "Epoch 43 | Batch 76 | Loss: 0.25711745023727417\n",
      "Epoch 43 | Batch 77 | Loss: 0.4130394458770752\n",
      "Epoch 43 | Batch 78 | Loss: 0.11025561392307281\n",
      "Epoch 43 | Batch 79 | Loss: 0.0632665753364563\n",
      "Epoch 43 | Batch 80 | Loss: 0.5479479432106018\n",
      "Epoch 43 | Batch 81 | Loss: 0.26515012979507446\n",
      "Epoch 43 | Batch 82 | Loss: 1.0332598686218262\n",
      "Epoch 43 | Batch 83 | Loss: 0.3210379481315613\n",
      "Epoch 43 | Batch 84 | Loss: 0.4006214439868927\n",
      "Epoch 43 | Batch 85 | Loss: 0.24267828464508057\n",
      "Epoch 43 | Batch 86 | Loss: 0.3306386470794678\n",
      "Epoch 43 | Batch 87 | Loss: 0.6267513036727905\n",
      "Epoch 43 | Batch 88 | Loss: 0.09530945122241974\n",
      "Epoch 43 | Batch 89 | Loss: 0.5296976566314697\n",
      "Epoch 43 | Batch 90 | Loss: 0.41343989968299866\n",
      "Epoch 44 | Batch 1 | Loss: 0.21704450249671936\n",
      "Epoch 44 | Batch 2 | Loss: 0.1732749491930008\n",
      "Epoch 44 | Batch 3 | Loss: 0.7056249976158142\n",
      "Epoch 44 | Batch 4 | Loss: 0.2781156301498413\n",
      "Epoch 44 | Batch 5 | Loss: 0.3562054932117462\n",
      "Epoch 44 | Batch 6 | Loss: 0.749452531337738\n",
      "Epoch 44 | Batch 7 | Loss: 0.20324015617370605\n",
      "Epoch 44 | Batch 8 | Loss: 0.3802146315574646\n",
      "Epoch 44 | Batch 9 | Loss: 0.3120763599872589\n",
      "Epoch 44 | Batch 10 | Loss: 0.3617967963218689\n",
      "Epoch 44 | Batch 11 | Loss: 0.4171450138092041\n",
      "Epoch 44 | Batch 12 | Loss: 0.710979700088501\n",
      "Epoch 44 | Batch 13 | Loss: 0.6398557424545288\n",
      "Epoch 44 | Batch 14 | Loss: 0.11984492093324661\n",
      "Epoch 44 | Batch 15 | Loss: 0.32736504077911377\n",
      "Epoch 44 | Batch 16 | Loss: 0.4946933388710022\n",
      "Epoch 44 | Batch 17 | Loss: 0.4405971169471741\n",
      "Epoch 44 | Batch 18 | Loss: 0.4540356397628784\n",
      "Epoch 44 | Batch 19 | Loss: 0.39316025376319885\n",
      "Epoch 44 | Batch 20 | Loss: 0.6464076042175293\n",
      "Epoch 44 | Batch 21 | Loss: 0.2879653573036194\n",
      "Epoch 44 | Batch 22 | Loss: 0.2886483073234558\n",
      "Epoch 44 | Batch 23 | Loss: 0.7717719078063965\n",
      "Epoch 44 | Batch 24 | Loss: 0.44862809777259827\n",
      "Epoch 44 | Batch 25 | Loss: 0.30807408690452576\n",
      "Epoch 44 | Batch 26 | Loss: 0.44263219833374023\n",
      "Epoch 44 | Batch 27 | Loss: 0.24400463700294495\n",
      "Epoch 44 | Batch 28 | Loss: 0.10954080522060394\n",
      "Epoch 44 | Batch 29 | Loss: 0.36335334181785583\n",
      "Epoch 44 | Batch 30 | Loss: 0.813637375831604\n",
      "Epoch 44 | Batch 31 | Loss: 0.1445857286453247\n",
      "Epoch 44 | Batch 32 | Loss: 0.15679991245269775\n",
      "Epoch 44 | Batch 33 | Loss: 0.19780196249485016\n",
      "Epoch 44 | Batch 34 | Loss: 0.7886667847633362\n",
      "Epoch 44 | Batch 35 | Loss: 0.21772414445877075\n",
      "Epoch 44 | Batch 36 | Loss: 0.09156321734189987\n",
      "Epoch 44 | Batch 37 | Loss: 0.06341211497783661\n",
      "Epoch 44 | Batch 38 | Loss: 0.4308449625968933\n",
      "Epoch 44 | Batch 39 | Loss: 0.2097039669752121\n",
      "Epoch 44 | Batch 40 | Loss: 0.2558673024177551\n",
      "Epoch 44 | Batch 41 | Loss: 0.6091997623443604\n",
      "Epoch 44 | Batch 42 | Loss: 0.264089971780777\n",
      "Epoch 44 | Batch 43 | Loss: 0.2815764546394348\n",
      "Epoch 44 | Batch 44 | Loss: 0.5865020751953125\n",
      "Epoch 44 | Batch 45 | Loss: 0.13173767924308777\n",
      "Epoch 44 | Batch 46 | Loss: 0.49800416827201843\n",
      "Epoch 44 | Batch 47 | Loss: 0.26545897126197815\n",
      "Epoch 44 | Batch 48 | Loss: 0.507215678691864\n",
      "Epoch 44 | Batch 49 | Loss: 0.15639162063598633\n",
      "Epoch 44 | Batch 50 | Loss: 0.19658446311950684\n",
      "Epoch 44 | Batch 51 | Loss: 0.7022581100463867\n",
      "Epoch 44 | Batch 52 | Loss: 0.38945090770721436\n",
      "Epoch 44 | Batch 53 | Loss: 0.35717135667800903\n",
      "Epoch 44 | Batch 54 | Loss: 0.26287034153938293\n",
      "Epoch 44 | Batch 55 | Loss: 0.37983348965644836\n",
      "Epoch 44 | Batch 56 | Loss: 0.12893268465995789\n",
      "Epoch 44 | Batch 57 | Loss: 0.17620816826820374\n",
      "Epoch 44 | Batch 58 | Loss: 0.5172696113586426\n",
      "Epoch 44 | Batch 59 | Loss: 0.4254559278488159\n",
      "Epoch 44 | Batch 60 | Loss: 0.4837547242641449\n",
      "Epoch 44 | Batch 61 | Loss: 0.38091087341308594\n",
      "Epoch 44 | Batch 62 | Loss: 0.5412319898605347\n",
      "Epoch 44 | Batch 63 | Loss: 0.17252831161022186\n",
      "Epoch 44 | Batch 64 | Loss: 0.191736102104187\n",
      "Epoch 44 | Batch 65 | Loss: 0.4188498556613922\n",
      "Epoch 44 | Batch 66 | Loss: 0.37393805384635925\n",
      "Epoch 44 | Batch 67 | Loss: 0.40399494767189026\n",
      "Epoch 44 | Batch 68 | Loss: 0.1630193293094635\n",
      "Epoch 44 | Batch 69 | Loss: 0.1625770628452301\n",
      "Epoch 44 | Batch 70 | Loss: 0.1930711567401886\n",
      "Epoch 44 | Batch 71 | Loss: 0.561718225479126\n",
      "Epoch 44 | Batch 72 | Loss: 0.25955215096473694\n",
      "Epoch 44 | Batch 73 | Loss: 0.38948026299476624\n",
      "Epoch 44 | Batch 74 | Loss: 0.5376865863800049\n",
      "Epoch 44 | Batch 75 | Loss: 0.1478714942932129\n",
      "Epoch 44 | Batch 76 | Loss: 0.21307234466075897\n",
      "Epoch 44 | Batch 77 | Loss: 0.5536978244781494\n",
      "Epoch 44 | Batch 78 | Loss: 0.5932028889656067\n",
      "Epoch 44 | Batch 79 | Loss: 0.6319071054458618\n",
      "Epoch 44 | Batch 80 | Loss: 0.29608839750289917\n",
      "Epoch 44 | Batch 81 | Loss: 0.3369055986404419\n",
      "Epoch 44 | Batch 82 | Loss: 0.22650769352912903\n",
      "Epoch 44 | Batch 83 | Loss: 0.761725902557373\n",
      "Epoch 44 | Batch 84 | Loss: 0.34935927391052246\n",
      "Epoch 44 | Batch 85 | Loss: 0.6395836472511292\n",
      "Epoch 44 | Batch 86 | Loss: 0.1604939103126526\n",
      "Epoch 44 | Batch 87 | Loss: 0.31255728006362915\n",
      "Epoch 44 | Batch 88 | Loss: 0.4857853651046753\n",
      "Epoch 44 | Batch 89 | Loss: 0.48223844170570374\n",
      "Epoch 44 | Batch 90 | Loss: 0.1515602171421051\n",
      "Epoch 45 | Batch 1 | Loss: 0.23142991960048676\n",
      "Epoch 45 | Batch 2 | Loss: 0.6506834626197815\n",
      "Epoch 45 | Batch 3 | Loss: 0.43726223707199097\n",
      "Epoch 45 | Batch 4 | Loss: 0.5095667839050293\n",
      "Epoch 45 | Batch 5 | Loss: 0.4967941343784332\n",
      "Epoch 45 | Batch 6 | Loss: 0.35842108726501465\n",
      "Epoch 45 | Batch 7 | Loss: 0.305235892534256\n",
      "Epoch 45 | Batch 8 | Loss: 0.43215683102607727\n",
      "Epoch 45 | Batch 9 | Loss: 0.14539751410484314\n",
      "Epoch 45 | Batch 10 | Loss: 0.10969799757003784\n",
      "Epoch 45 | Batch 11 | Loss: 0.4264105558395386\n",
      "Epoch 45 | Batch 12 | Loss: 0.4037356674671173\n",
      "Epoch 45 | Batch 13 | Loss: 0.3776814341545105\n",
      "Epoch 45 | Batch 14 | Loss: 0.4766088128089905\n",
      "Epoch 45 | Batch 15 | Loss: 0.5644428730010986\n",
      "Epoch 45 | Batch 16 | Loss: 0.21182823181152344\n",
      "Epoch 45 | Batch 17 | Loss: 0.5227307081222534\n",
      "Epoch 45 | Batch 18 | Loss: 0.14466673135757446\n",
      "Epoch 45 | Batch 19 | Loss: 0.3490952253341675\n",
      "Epoch 45 | Batch 20 | Loss: 0.4076367914676666\n",
      "Epoch 45 | Batch 21 | Loss: 0.1053854301571846\n",
      "Epoch 45 | Batch 22 | Loss: 0.24335968494415283\n",
      "Epoch 45 | Batch 23 | Loss: 0.12456433475017548\n",
      "Epoch 45 | Batch 24 | Loss: 0.8870252370834351\n",
      "Epoch 45 | Batch 25 | Loss: 0.3114868998527527\n",
      "Epoch 45 | Batch 26 | Loss: 0.41321688890457153\n",
      "Epoch 45 | Batch 27 | Loss: 0.19309833645820618\n",
      "Epoch 45 | Batch 28 | Loss: 0.2882578372955322\n",
      "Epoch 45 | Batch 29 | Loss: 0.32217463850975037\n",
      "Epoch 45 | Batch 30 | Loss: 0.4650592803955078\n",
      "Epoch 45 | Batch 31 | Loss: 0.23091071844100952\n",
      "Epoch 45 | Batch 32 | Loss: 0.16443288326263428\n",
      "Epoch 45 | Batch 33 | Loss: 0.6643244028091431\n",
      "Epoch 45 | Batch 34 | Loss: 0.12297187745571136\n",
      "Epoch 45 | Batch 35 | Loss: 0.651656448841095\n",
      "Epoch 45 | Batch 36 | Loss: 0.3187797963619232\n",
      "Epoch 45 | Batch 37 | Loss: 0.3708367943763733\n",
      "Epoch 45 | Batch 38 | Loss: 0.08445868641138077\n",
      "Epoch 45 | Batch 39 | Loss: 0.49421098828315735\n",
      "Epoch 45 | Batch 40 | Loss: 0.6401647329330444\n",
      "Epoch 45 | Batch 41 | Loss: 0.6121500730514526\n",
      "Epoch 45 | Batch 42 | Loss: 0.35658758878707886\n",
      "Epoch 45 | Batch 43 | Loss: 0.1288512647151947\n",
      "Epoch 45 | Batch 44 | Loss: 0.29939180612564087\n",
      "Epoch 45 | Batch 45 | Loss: 0.44079339504241943\n",
      "Epoch 45 | Batch 46 | Loss: 0.4045562446117401\n",
      "Epoch 45 | Batch 47 | Loss: 0.20147442817687988\n",
      "Epoch 45 | Batch 48 | Loss: 0.589505136013031\n",
      "Epoch 45 | Batch 49 | Loss: 0.291787713766098\n",
      "Epoch 45 | Batch 50 | Loss: 0.39140570163726807\n",
      "Epoch 45 | Batch 51 | Loss: 0.20476312935352325\n",
      "Epoch 45 | Batch 52 | Loss: 0.2457142174243927\n",
      "Epoch 45 | Batch 53 | Loss: 0.11675312370061874\n",
      "Epoch 45 | Batch 54 | Loss: 0.2814265191555023\n",
      "Epoch 45 | Batch 55 | Loss: 0.9099955558776855\n",
      "Epoch 45 | Batch 56 | Loss: 0.6102951169013977\n",
      "Epoch 45 | Batch 57 | Loss: 0.2735596299171448\n",
      "Epoch 45 | Batch 58 | Loss: 0.2768792510032654\n",
      "Epoch 45 | Batch 59 | Loss: 0.6300981044769287\n",
      "Epoch 45 | Batch 60 | Loss: 0.23493480682373047\n",
      "Epoch 45 | Batch 61 | Loss: 0.13006651401519775\n",
      "Epoch 45 | Batch 62 | Loss: 0.612575888633728\n",
      "Epoch 45 | Batch 63 | Loss: 0.6750196814537048\n",
      "Epoch 45 | Batch 64 | Loss: 0.13699427247047424\n",
      "Epoch 45 | Batch 65 | Loss: 0.22996532917022705\n",
      "Epoch 45 | Batch 66 | Loss: 0.3085063099861145\n",
      "Epoch 45 | Batch 67 | Loss: 0.25013071298599243\n",
      "Epoch 45 | Batch 68 | Loss: 0.6257343292236328\n",
      "Epoch 45 | Batch 69 | Loss: 0.37049925327301025\n",
      "Epoch 45 | Batch 70 | Loss: 0.19817033410072327\n",
      "Epoch 45 | Batch 71 | Loss: 0.21277111768722534\n",
      "Epoch 45 | Batch 72 | Loss: 0.2564510107040405\n",
      "Epoch 45 | Batch 73 | Loss: 0.7269925475120544\n",
      "Epoch 45 | Batch 74 | Loss: 0.11508538573980331\n",
      "Epoch 45 | Batch 75 | Loss: 0.350174218416214\n",
      "Epoch 45 | Batch 76 | Loss: 0.3261987864971161\n",
      "Epoch 45 | Batch 77 | Loss: 0.4406868517398834\n",
      "Epoch 45 | Batch 78 | Loss: 0.3067944347858429\n",
      "Epoch 45 | Batch 79 | Loss: 0.6181265711784363\n",
      "Epoch 45 | Batch 80 | Loss: 0.25775402784347534\n",
      "Epoch 45 | Batch 81 | Loss: 0.40891382098197937\n",
      "Epoch 45 | Batch 82 | Loss: 0.22647376358509064\n",
      "Epoch 45 | Batch 83 | Loss: 0.4953162968158722\n",
      "Epoch 45 | Batch 84 | Loss: 0.2604721188545227\n",
      "Epoch 45 | Batch 85 | Loss: 0.41851282119750977\n",
      "Epoch 45 | Batch 86 | Loss: 0.08505088835954666\n",
      "Epoch 45 | Batch 87 | Loss: 0.4496977925300598\n",
      "Epoch 45 | Batch 88 | Loss: 0.8191534280776978\n",
      "Epoch 45 | Batch 89 | Loss: 0.4669915437698364\n",
      "Epoch 45 | Batch 90 | Loss: 0.16265681385993958\n",
      "Epoch 46 | Batch 1 | Loss: 0.4980103671550751\n",
      "Epoch 46 | Batch 2 | Loss: 0.13477684557437897\n",
      "Epoch 46 | Batch 3 | Loss: 0.0867580771446228\n",
      "Epoch 46 | Batch 4 | Loss: 0.4406503140926361\n",
      "Epoch 46 | Batch 5 | Loss: 0.17542871832847595\n",
      "Epoch 46 | Batch 6 | Loss: 0.2643599510192871\n",
      "Epoch 46 | Batch 7 | Loss: 0.44147661328315735\n",
      "Epoch 46 | Batch 8 | Loss: 0.5482924580574036\n",
      "Epoch 46 | Batch 9 | Loss: 0.2397909313440323\n",
      "Epoch 46 | Batch 10 | Loss: 0.16669590771198273\n",
      "Epoch 46 | Batch 11 | Loss: 0.19336248934268951\n",
      "Epoch 46 | Batch 12 | Loss: 0.4464113116264343\n",
      "Epoch 46 | Batch 13 | Loss: 0.25314512848854065\n",
      "Epoch 46 | Batch 14 | Loss: 0.4532797336578369\n",
      "Epoch 46 | Batch 15 | Loss: 0.4797278046607971\n",
      "Epoch 46 | Batch 16 | Loss: 0.15288613736629486\n",
      "Epoch 46 | Batch 17 | Loss: 0.21086585521697998\n",
      "Epoch 46 | Batch 18 | Loss: 0.0510052852332592\n",
      "Epoch 46 | Batch 19 | Loss: 0.15733300149440765\n",
      "Epoch 46 | Batch 20 | Loss: 0.637700617313385\n",
      "Epoch 46 | Batch 21 | Loss: 0.14055171608924866\n",
      "Epoch 46 | Batch 22 | Loss: 0.3764367699623108\n",
      "Epoch 46 | Batch 23 | Loss: 0.14540663361549377\n",
      "Epoch 46 | Batch 24 | Loss: 0.17615607380867004\n",
      "Epoch 46 | Batch 25 | Loss: 0.5247300267219543\n",
      "Epoch 46 | Batch 26 | Loss: 0.36622726917266846\n",
      "Epoch 46 | Batch 27 | Loss: 0.6824555397033691\n",
      "Epoch 46 | Batch 28 | Loss: 0.14770717918872833\n",
      "Epoch 46 | Batch 29 | Loss: 0.07326753437519073\n",
      "Epoch 46 | Batch 30 | Loss: 0.4882478415966034\n",
      "Epoch 46 | Batch 31 | Loss: 0.352117121219635\n",
      "Epoch 46 | Batch 32 | Loss: 0.5005699992179871\n",
      "Epoch 46 | Batch 33 | Loss: 0.1769239753484726\n",
      "Epoch 46 | Batch 34 | Loss: 0.4984174966812134\n",
      "Epoch 46 | Batch 35 | Loss: 0.09761069715023041\n",
      "Epoch 46 | Batch 36 | Loss: 0.9106125831604004\n",
      "Epoch 46 | Batch 37 | Loss: 0.5104181170463562\n",
      "Epoch 46 | Batch 38 | Loss: 0.2070002257823944\n",
      "Epoch 46 | Batch 39 | Loss: 0.08936992287635803\n",
      "Epoch 46 | Batch 40 | Loss: 0.1617804914712906\n",
      "Epoch 46 | Batch 41 | Loss: 0.19275861978530884\n",
      "Epoch 46 | Batch 42 | Loss: 0.7303897738456726\n",
      "Epoch 46 | Batch 43 | Loss: 0.5811529755592346\n",
      "Epoch 46 | Batch 44 | Loss: 0.40362828969955444\n",
      "Epoch 46 | Batch 45 | Loss: 0.30326300859451294\n",
      "Epoch 46 | Batch 46 | Loss: 0.6228762269020081\n",
      "Epoch 46 | Batch 47 | Loss: 0.13992717862129211\n",
      "Epoch 46 | Batch 48 | Loss: 0.3191671371459961\n",
      "Epoch 46 | Batch 49 | Loss: 0.11598870158195496\n",
      "Epoch 46 | Batch 50 | Loss: 0.37362802028656006\n",
      "Epoch 46 | Batch 51 | Loss: 0.6870837211608887\n",
      "Epoch 46 | Batch 52 | Loss: 0.2739204168319702\n",
      "Epoch 46 | Batch 53 | Loss: 0.20799997448921204\n",
      "Epoch 46 | Batch 54 | Loss: 0.9013910293579102\n",
      "Epoch 46 | Batch 55 | Loss: 0.7233867049217224\n",
      "Epoch 46 | Batch 56 | Loss: 0.190992534160614\n",
      "Epoch 46 | Batch 57 | Loss: 0.6317651867866516\n",
      "Epoch 46 | Batch 58 | Loss: 0.16600099205970764\n",
      "Epoch 46 | Batch 59 | Loss: 0.32213157415390015\n",
      "Epoch 46 | Batch 60 | Loss: 0.3585841655731201\n",
      "Epoch 46 | Batch 61 | Loss: 0.433932900428772\n",
      "Epoch 46 | Batch 62 | Loss: 0.2716963291168213\n",
      "Epoch 46 | Batch 63 | Loss: 0.09180152416229248\n",
      "Epoch 46 | Batch 64 | Loss: 0.22012552618980408\n",
      "Epoch 46 | Batch 65 | Loss: 0.3429296612739563\n",
      "Epoch 46 | Batch 66 | Loss: 0.35366037487983704\n",
      "Epoch 46 | Batch 67 | Loss: 0.3346744477748871\n",
      "Epoch 46 | Batch 68 | Loss: 0.270834743976593\n",
      "Epoch 46 | Batch 69 | Loss: 0.6076996326446533\n",
      "Epoch 46 | Batch 70 | Loss: 0.2164122760295868\n",
      "Epoch 46 | Batch 71 | Loss: 0.9404631853103638\n",
      "Epoch 46 | Batch 72 | Loss: 0.45387810468673706\n",
      "Epoch 46 | Batch 73 | Loss: 0.5329896807670593\n",
      "Epoch 46 | Batch 74 | Loss: 0.2282935082912445\n",
      "Epoch 46 | Batch 75 | Loss: 0.5496315360069275\n",
      "Epoch 46 | Batch 76 | Loss: 0.07223771512508392\n",
      "Epoch 46 | Batch 77 | Loss: 0.6691147685050964\n",
      "Epoch 46 | Batch 78 | Loss: 0.13804250955581665\n",
      "Epoch 46 | Batch 79 | Loss: 0.41995078325271606\n",
      "Epoch 46 | Batch 80 | Loss: 0.7962360382080078\n",
      "Epoch 46 | Batch 81 | Loss: 0.14085227251052856\n",
      "Epoch 46 | Batch 82 | Loss: 0.5193897485733032\n",
      "Epoch 46 | Batch 83 | Loss: 0.3848183751106262\n",
      "Epoch 46 | Batch 84 | Loss: 0.38601723313331604\n",
      "Epoch 46 | Batch 85 | Loss: 0.8449395895004272\n",
      "Epoch 46 | Batch 86 | Loss: 0.4120723605155945\n",
      "Epoch 46 | Batch 87 | Loss: 0.41955137252807617\n",
      "Epoch 46 | Batch 88 | Loss: 0.2918786108493805\n",
      "Epoch 46 | Batch 89 | Loss: 0.3570553958415985\n",
      "Epoch 46 | Batch 90 | Loss: 1.802720546722412\n",
      "Epoch 47 | Batch 1 | Loss: 0.31229400634765625\n",
      "Epoch 47 | Batch 2 | Loss: 0.5151159763336182\n",
      "Epoch 47 | Batch 3 | Loss: 0.3871731162071228\n",
      "Epoch 47 | Batch 4 | Loss: 0.4656845033168793\n",
      "Epoch 47 | Batch 5 | Loss: 0.3183368146419525\n",
      "Epoch 47 | Batch 6 | Loss: 0.2984681725502014\n",
      "Epoch 47 | Batch 7 | Loss: 0.6861838102340698\n",
      "Epoch 47 | Batch 8 | Loss: 0.306294709444046\n",
      "Epoch 47 | Batch 9 | Loss: 0.36194223165512085\n",
      "Epoch 47 | Batch 10 | Loss: 0.21570217609405518\n",
      "Epoch 47 | Batch 11 | Loss: 0.44135782122612\n",
      "Epoch 47 | Batch 12 | Loss: 0.31707578897476196\n",
      "Epoch 47 | Batch 13 | Loss: 0.5609210133552551\n",
      "Epoch 47 | Batch 14 | Loss: 0.5458508729934692\n",
      "Epoch 47 | Batch 15 | Loss: 0.34066733717918396\n",
      "Epoch 47 | Batch 16 | Loss: 0.18486520648002625\n",
      "Epoch 47 | Batch 17 | Loss: 0.6550127267837524\n",
      "Epoch 47 | Batch 18 | Loss: 0.3326026201248169\n",
      "Epoch 47 | Batch 19 | Loss: 0.5743699073791504\n",
      "Epoch 47 | Batch 20 | Loss: 0.23444890975952148\n",
      "Epoch 47 | Batch 21 | Loss: 0.33694320917129517\n",
      "Epoch 47 | Batch 22 | Loss: 0.26367583870887756\n",
      "Epoch 47 | Batch 23 | Loss: 0.2383880317211151\n",
      "Epoch 47 | Batch 24 | Loss: 0.27922865748405457\n",
      "Epoch 47 | Batch 25 | Loss: 0.18013493716716766\n",
      "Epoch 47 | Batch 26 | Loss: 0.19122198224067688\n",
      "Epoch 47 | Batch 27 | Loss: 0.4150921404361725\n",
      "Epoch 47 | Batch 28 | Loss: 0.403835654258728\n",
      "Epoch 47 | Batch 29 | Loss: 0.37125125527381897\n",
      "Epoch 47 | Batch 30 | Loss: 0.3872963786125183\n",
      "Epoch 47 | Batch 31 | Loss: 0.4832291603088379\n",
      "Epoch 47 | Batch 32 | Loss: 0.2343825250864029\n",
      "Epoch 47 | Batch 33 | Loss: 0.18403702974319458\n",
      "Epoch 47 | Batch 34 | Loss: 0.3577645421028137\n",
      "Epoch 47 | Batch 35 | Loss: 0.6094211339950562\n",
      "Epoch 47 | Batch 36 | Loss: 0.8394501209259033\n",
      "Epoch 47 | Batch 37 | Loss: 0.28845298290252686\n",
      "Epoch 47 | Batch 38 | Loss: 0.38191884756088257\n",
      "Epoch 47 | Batch 39 | Loss: 0.35759007930755615\n",
      "Epoch 47 | Batch 40 | Loss: 0.46515390276908875\n",
      "Epoch 47 | Batch 41 | Loss: 0.27471137046813965\n",
      "Epoch 47 | Batch 42 | Loss: 0.6273168325424194\n",
      "Epoch 47 | Batch 43 | Loss: 0.4094946086406708\n",
      "Epoch 47 | Batch 44 | Loss: 0.7579071521759033\n",
      "Epoch 47 | Batch 45 | Loss: 0.3720966577529907\n",
      "Epoch 47 | Batch 46 | Loss: 0.3777230978012085\n",
      "Epoch 47 | Batch 47 | Loss: 0.1228371411561966\n",
      "Epoch 47 | Batch 48 | Loss: 0.09950387477874756\n",
      "Epoch 47 | Batch 49 | Loss: 0.18568304181098938\n",
      "Epoch 47 | Batch 50 | Loss: 0.2540723979473114\n",
      "Epoch 47 | Batch 51 | Loss: 0.2196667492389679\n",
      "Epoch 47 | Batch 52 | Loss: 0.28377416729927063\n",
      "Epoch 47 | Batch 53 | Loss: 0.5626159906387329\n",
      "Epoch 47 | Batch 54 | Loss: 0.21726372838020325\n",
      "Epoch 47 | Batch 55 | Loss: 0.38007014989852905\n",
      "Epoch 47 | Batch 56 | Loss: 0.5070635080337524\n",
      "Epoch 47 | Batch 57 | Loss: 0.3814695179462433\n",
      "Epoch 47 | Batch 58 | Loss: 0.31620463728904724\n",
      "Epoch 47 | Batch 59 | Loss: 0.16166925430297852\n",
      "Epoch 47 | Batch 60 | Loss: 0.1697075068950653\n",
      "Epoch 47 | Batch 61 | Loss: 0.43395745754241943\n",
      "Epoch 47 | Batch 62 | Loss: 0.21033763885498047\n",
      "Epoch 47 | Batch 63 | Loss: 0.1606983244419098\n",
      "Epoch 47 | Batch 64 | Loss: 0.2138240486383438\n",
      "Epoch 47 | Batch 65 | Loss: 0.2586204409599304\n",
      "Epoch 47 | Batch 66 | Loss: 0.4494002163410187\n",
      "Epoch 47 | Batch 67 | Loss: 0.07477129250764847\n",
      "Epoch 47 | Batch 68 | Loss: 0.3859216570854187\n",
      "Epoch 47 | Batch 69 | Loss: 0.6147136092185974\n",
      "Epoch 47 | Batch 70 | Loss: 0.4289012551307678\n",
      "Epoch 47 | Batch 71 | Loss: 0.31011876463890076\n",
      "Epoch 47 | Batch 72 | Loss: 0.46217793226242065\n",
      "Epoch 47 | Batch 73 | Loss: 0.37252792716026306\n",
      "Epoch 47 | Batch 74 | Loss: 0.1117786094546318\n",
      "Epoch 47 | Batch 75 | Loss: 0.27013081312179565\n",
      "Epoch 47 | Batch 76 | Loss: 0.23007503151893616\n",
      "Epoch 47 | Batch 77 | Loss: 0.48938557505607605\n",
      "Epoch 47 | Batch 78 | Loss: 0.9044315814971924\n",
      "Epoch 47 | Batch 79 | Loss: 0.22045373916625977\n",
      "Epoch 47 | Batch 80 | Loss: 0.5186280012130737\n",
      "Epoch 47 | Batch 81 | Loss: 0.2632255554199219\n",
      "Epoch 47 | Batch 82 | Loss: 0.7825668454170227\n",
      "Epoch 47 | Batch 83 | Loss: 0.7243809700012207\n",
      "Epoch 47 | Batch 84 | Loss: 0.11588098108768463\n",
      "Epoch 47 | Batch 85 | Loss: 0.16472932696342468\n",
      "Epoch 47 | Batch 86 | Loss: 0.4151040017604828\n",
      "Epoch 47 | Batch 87 | Loss: 0.16256076097488403\n",
      "Epoch 47 | Batch 88 | Loss: 0.6669906377792358\n",
      "Epoch 47 | Batch 89 | Loss: 0.153733491897583\n",
      "Epoch 47 | Batch 90 | Loss: 0.8447906970977783\n",
      "Epoch 48 | Batch 1 | Loss: 0.3326387107372284\n",
      "Epoch 48 | Batch 2 | Loss: 0.38603371381759644\n",
      "Epoch 48 | Batch 3 | Loss: 0.43227678537368774\n",
      "Epoch 48 | Batch 4 | Loss: 0.13001245260238647\n",
      "Epoch 48 | Batch 5 | Loss: 0.5239274501800537\n",
      "Epoch 48 | Batch 6 | Loss: 0.5893767476081848\n",
      "Epoch 48 | Batch 7 | Loss: 0.15059058368206024\n",
      "Epoch 48 | Batch 8 | Loss: 0.8226714134216309\n",
      "Epoch 48 | Batch 9 | Loss: 0.43750083446502686\n",
      "Epoch 48 | Batch 10 | Loss: 0.4217437207698822\n",
      "Epoch 48 | Batch 11 | Loss: 0.4771854281425476\n",
      "Epoch 48 | Batch 12 | Loss: 0.08791553974151611\n",
      "Epoch 48 | Batch 13 | Loss: 0.25972312688827515\n",
      "Epoch 48 | Batch 14 | Loss: 0.3856334984302521\n",
      "Epoch 48 | Batch 15 | Loss: 0.3148566484451294\n",
      "Epoch 48 | Batch 16 | Loss: 0.18235935270786285\n",
      "Epoch 48 | Batch 17 | Loss: 0.5228419899940491\n",
      "Epoch 48 | Batch 18 | Loss: 0.7164468765258789\n",
      "Epoch 48 | Batch 19 | Loss: 0.7621526718139648\n",
      "Epoch 48 | Batch 20 | Loss: 0.18610721826553345\n",
      "Epoch 48 | Batch 21 | Loss: 0.5101261138916016\n",
      "Epoch 48 | Batch 22 | Loss: 0.22816874086856842\n",
      "Epoch 48 | Batch 23 | Loss: 0.18721167743206024\n",
      "Epoch 48 | Batch 24 | Loss: 0.7238460779190063\n",
      "Epoch 48 | Batch 25 | Loss: 0.4492000639438629\n",
      "Epoch 48 | Batch 26 | Loss: 0.13705399632453918\n",
      "Epoch 48 | Batch 27 | Loss: 0.5245033502578735\n",
      "Epoch 48 | Batch 28 | Loss: 0.4303871989250183\n",
      "Epoch 48 | Batch 29 | Loss: 0.24863409996032715\n",
      "Epoch 48 | Batch 30 | Loss: 0.12716013193130493\n",
      "Epoch 48 | Batch 31 | Loss: 0.21866931021213531\n",
      "Epoch 48 | Batch 32 | Loss: 0.454531192779541\n",
      "Epoch 48 | Batch 33 | Loss: 0.34261953830718994\n",
      "Epoch 48 | Batch 34 | Loss: 0.3172779083251953\n",
      "Epoch 48 | Batch 35 | Loss: 0.3088374137878418\n",
      "Epoch 48 | Batch 36 | Loss: 0.4593883752822876\n",
      "Epoch 48 | Batch 37 | Loss: 0.23499411344528198\n",
      "Epoch 48 | Batch 38 | Loss: 0.19505885243415833\n",
      "Epoch 48 | Batch 39 | Loss: 0.30200767517089844\n",
      "Epoch 48 | Batch 40 | Loss: 0.34811827540397644\n",
      "Epoch 48 | Batch 41 | Loss: 0.30598583817481995\n",
      "Epoch 48 | Batch 42 | Loss: 0.30009543895721436\n",
      "Epoch 48 | Batch 43 | Loss: 0.3607287108898163\n",
      "Epoch 48 | Batch 44 | Loss: 0.44582027196884155\n",
      "Epoch 48 | Batch 45 | Loss: 0.2076427936553955\n",
      "Epoch 48 | Batch 46 | Loss: 0.6972876787185669\n",
      "Epoch 48 | Batch 47 | Loss: 0.52374666929245\n",
      "Epoch 48 | Batch 48 | Loss: 0.05233539640903473\n",
      "Epoch 48 | Batch 49 | Loss: 0.7154616713523865\n",
      "Epoch 48 | Batch 50 | Loss: 0.5377022624015808\n",
      "Epoch 48 | Batch 51 | Loss: 0.58636075258255\n",
      "Epoch 48 | Batch 52 | Loss: 0.20714546740055084\n",
      "Epoch 48 | Batch 53 | Loss: 0.32213324308395386\n",
      "Epoch 48 | Batch 54 | Loss: 0.4865195155143738\n",
      "Epoch 48 | Batch 55 | Loss: 0.5923136472702026\n",
      "Epoch 48 | Batch 56 | Loss: 0.1463175117969513\n",
      "Epoch 48 | Batch 57 | Loss: 0.21821029484272003\n",
      "Epoch 48 | Batch 58 | Loss: 0.28016942739486694\n",
      "Epoch 48 | Batch 59 | Loss: 0.2592872977256775\n",
      "Epoch 48 | Batch 60 | Loss: 0.41622018814086914\n",
      "Epoch 48 | Batch 61 | Loss: 0.6275246143341064\n",
      "Epoch 48 | Batch 62 | Loss: 0.43228620290756226\n",
      "Epoch 48 | Batch 63 | Loss: 0.18641525506973267\n",
      "Epoch 48 | Batch 64 | Loss: 0.2511035203933716\n",
      "Epoch 48 | Batch 65 | Loss: 0.2036387026309967\n",
      "Epoch 48 | Batch 66 | Loss: 0.12215728312730789\n",
      "Epoch 48 | Batch 67 | Loss: 0.33616119623184204\n",
      "Epoch 48 | Batch 68 | Loss: 0.43065115809440613\n",
      "Epoch 48 | Batch 69 | Loss: 0.4281772971153259\n",
      "Epoch 48 | Batch 70 | Loss: 0.2592030465602875\n",
      "Epoch 48 | Batch 71 | Loss: 0.907044529914856\n",
      "Epoch 48 | Batch 72 | Loss: 0.4798537492752075\n",
      "Epoch 48 | Batch 73 | Loss: 0.165365070104599\n",
      "Epoch 48 | Batch 74 | Loss: 0.22639572620391846\n",
      "Epoch 48 | Batch 75 | Loss: 0.2558777928352356\n",
      "Epoch 48 | Batch 76 | Loss: 0.7837998867034912\n",
      "Epoch 48 | Batch 77 | Loss: 0.2277640402317047\n",
      "Epoch 48 | Batch 78 | Loss: 0.2218739241361618\n",
      "Epoch 48 | Batch 79 | Loss: 0.11294890195131302\n",
      "Epoch 48 | Batch 80 | Loss: 0.3022708594799042\n",
      "Epoch 48 | Batch 81 | Loss: 0.5648036003112793\n",
      "Epoch 48 | Batch 82 | Loss: 0.3700862228870392\n",
      "Epoch 48 | Batch 83 | Loss: 0.3010845184326172\n",
      "Epoch 48 | Batch 84 | Loss: 0.125711590051651\n",
      "Epoch 48 | Batch 85 | Loss: 0.6713749766349792\n",
      "Epoch 48 | Batch 86 | Loss: 0.4390009045600891\n",
      "Epoch 48 | Batch 87 | Loss: 0.24667853116989136\n",
      "Epoch 48 | Batch 88 | Loss: 0.4099029004573822\n",
      "Epoch 48 | Batch 89 | Loss: 0.3958103060722351\n",
      "Epoch 48 | Batch 90 | Loss: 0.13220791518688202\n",
      "Epoch 49 | Batch 1 | Loss: 0.40999650955200195\n",
      "Epoch 49 | Batch 2 | Loss: 0.40685319900512695\n",
      "Epoch 49 | Batch 3 | Loss: 0.4250331223011017\n",
      "Epoch 49 | Batch 4 | Loss: 0.330045223236084\n",
      "Epoch 49 | Batch 5 | Loss: 0.12827590107917786\n",
      "Epoch 49 | Batch 6 | Loss: 0.3055262863636017\n",
      "Epoch 49 | Batch 7 | Loss: 0.4751008152961731\n",
      "Epoch 49 | Batch 8 | Loss: 0.5674053430557251\n",
      "Epoch 49 | Batch 9 | Loss: 0.10992483049631119\n",
      "Epoch 49 | Batch 10 | Loss: 0.207548588514328\n",
      "Epoch 49 | Batch 11 | Loss: 0.3823574483394623\n",
      "Epoch 49 | Batch 12 | Loss: 0.3646978735923767\n",
      "Epoch 49 | Batch 13 | Loss: 0.625064492225647\n",
      "Epoch 49 | Batch 14 | Loss: 0.44167542457580566\n",
      "Epoch 49 | Batch 15 | Loss: 0.40802574157714844\n",
      "Epoch 49 | Batch 16 | Loss: 0.6115724444389343\n",
      "Epoch 49 | Batch 17 | Loss: 0.3896586298942566\n",
      "Epoch 49 | Batch 18 | Loss: 0.32179903984069824\n",
      "Epoch 49 | Batch 19 | Loss: 0.10963307321071625\n",
      "Epoch 49 | Batch 20 | Loss: 0.12079937756061554\n",
      "Epoch 49 | Batch 21 | Loss: 0.10860149562358856\n",
      "Epoch 49 | Batch 22 | Loss: 0.15257751941680908\n",
      "Epoch 49 | Batch 23 | Loss: 0.4058162569999695\n",
      "Epoch 49 | Batch 24 | Loss: 0.15554913878440857\n",
      "Epoch 49 | Batch 25 | Loss: 0.105824314057827\n",
      "Epoch 49 | Batch 26 | Loss: 0.3065628409385681\n",
      "Epoch 49 | Batch 27 | Loss: 0.3931959569454193\n",
      "Epoch 49 | Batch 28 | Loss: 0.36820530891418457\n",
      "Epoch 49 | Batch 29 | Loss: 0.08398127555847168\n",
      "Epoch 49 | Batch 30 | Loss: 0.5457484722137451\n",
      "Epoch 49 | Batch 31 | Loss: 0.4581936001777649\n",
      "Epoch 49 | Batch 32 | Loss: 0.593017578125\n",
      "Epoch 49 | Batch 33 | Loss: 0.36045530438423157\n",
      "Epoch 49 | Batch 34 | Loss: 0.553580641746521\n",
      "Epoch 49 | Batch 35 | Loss: 0.517148494720459\n",
      "Epoch 49 | Batch 36 | Loss: 0.36321064829826355\n",
      "Epoch 49 | Batch 37 | Loss: 0.6793304681777954\n",
      "Epoch 49 | Batch 38 | Loss: 0.10721921920776367\n",
      "Epoch 49 | Batch 39 | Loss: 0.18788596987724304\n",
      "Epoch 49 | Batch 40 | Loss: 0.2915937602519989\n",
      "Epoch 49 | Batch 41 | Loss: 0.2916261553764343\n",
      "Epoch 49 | Batch 42 | Loss: 0.3981507122516632\n",
      "Epoch 49 | Batch 43 | Loss: 0.10069607198238373\n",
      "Epoch 49 | Batch 44 | Loss: 0.3998592495918274\n",
      "Epoch 49 | Batch 45 | Loss: 0.3616351783275604\n",
      "Epoch 49 | Batch 46 | Loss: 0.08851899951696396\n",
      "Epoch 49 | Batch 47 | Loss: 0.665200412273407\n",
      "Epoch 49 | Batch 48 | Loss: 0.20739181339740753\n",
      "Epoch 49 | Batch 49 | Loss: 0.44491904973983765\n",
      "Epoch 49 | Batch 50 | Loss: 0.6190234422683716\n",
      "Epoch 49 | Batch 51 | Loss: 0.8519165515899658\n",
      "Epoch 49 | Batch 52 | Loss: 0.3532155752182007\n",
      "Epoch 49 | Batch 53 | Loss: 0.32952743768692017\n",
      "Epoch 49 | Batch 54 | Loss: 0.5759413242340088\n",
      "Epoch 49 | Batch 55 | Loss: 0.14201559126377106\n",
      "Epoch 49 | Batch 56 | Loss: 0.4154927432537079\n",
      "Epoch 49 | Batch 57 | Loss: 0.538601279258728\n",
      "Epoch 49 | Batch 58 | Loss: 0.30430394411087036\n",
      "Epoch 49 | Batch 59 | Loss: 0.3037288188934326\n",
      "Epoch 49 | Batch 60 | Loss: 0.2629370093345642\n",
      "Epoch 49 | Batch 61 | Loss: 0.45832622051239014\n",
      "Epoch 49 | Batch 62 | Loss: 0.4292522668838501\n",
      "Epoch 49 | Batch 63 | Loss: 0.10754431784152985\n",
      "Epoch 49 | Batch 64 | Loss: 0.18264001607894897\n",
      "Epoch 49 | Batch 65 | Loss: 0.13801339268684387\n",
      "Epoch 49 | Batch 66 | Loss: 0.4595523178577423\n",
      "Epoch 49 | Batch 67 | Loss: 0.10153692960739136\n",
      "Epoch 49 | Batch 68 | Loss: 1.0170773267745972\n",
      "Epoch 49 | Batch 69 | Loss: 0.6715937256813049\n",
      "Epoch 49 | Batch 70 | Loss: 0.33760660886764526\n",
      "Epoch 49 | Batch 71 | Loss: 0.4815017580986023\n",
      "Epoch 49 | Batch 72 | Loss: 0.3210260272026062\n",
      "Epoch 49 | Batch 73 | Loss: 0.35018426179885864\n",
      "Epoch 49 | Batch 74 | Loss: 0.5406231880187988\n",
      "Epoch 49 | Batch 75 | Loss: 0.4570963680744171\n",
      "Epoch 49 | Batch 76 | Loss: 0.5784795880317688\n",
      "Epoch 49 | Batch 77 | Loss: 0.8544660806655884\n",
      "Epoch 49 | Batch 78 | Loss: 0.1645011156797409\n",
      "Epoch 49 | Batch 79 | Loss: 0.2258070707321167\n",
      "Epoch 49 | Batch 80 | Loss: 0.3168731927871704\n",
      "Epoch 49 | Batch 81 | Loss: 0.3414278030395508\n",
      "Epoch 49 | Batch 82 | Loss: 0.3952218294143677\n",
      "Epoch 49 | Batch 83 | Loss: 0.376821905374527\n",
      "Epoch 49 | Batch 84 | Loss: 0.195973739027977\n",
      "Epoch 49 | Batch 85 | Loss: 0.4181220233440399\n",
      "Epoch 49 | Batch 86 | Loss: 0.07340333610773087\n",
      "Epoch 49 | Batch 87 | Loss: 0.5710160732269287\n",
      "Epoch 49 | Batch 88 | Loss: 0.34964337944984436\n",
      "Epoch 49 | Batch 89 | Loss: 0.267803817987442\n",
      "Epoch 49 | Batch 90 | Loss: 0.17131011188030243\n",
      "Epoch 50 | Batch 1 | Loss: 0.2573491036891937\n",
      "Epoch 50 | Batch 2 | Loss: 0.5177202820777893\n",
      "Epoch 50 | Batch 3 | Loss: 1.178679347038269\n",
      "Epoch 50 | Batch 4 | Loss: 0.21573486924171448\n",
      "Epoch 50 | Batch 5 | Loss: 0.5602076649665833\n",
      "Epoch 50 | Batch 6 | Loss: 0.3369275629520416\n",
      "Epoch 50 | Batch 7 | Loss: 0.6630913019180298\n",
      "Epoch 50 | Batch 8 | Loss: 0.2684890627861023\n",
      "Epoch 50 | Batch 9 | Loss: 0.2704877257347107\n",
      "Epoch 50 | Batch 10 | Loss: 0.23440246284008026\n",
      "Epoch 50 | Batch 11 | Loss: 0.13268615305423737\n",
      "Epoch 50 | Batch 12 | Loss: 0.49918803572654724\n",
      "Epoch 50 | Batch 13 | Loss: 0.42737701535224915\n",
      "Epoch 50 | Batch 14 | Loss: 0.33372244238853455\n",
      "Epoch 50 | Batch 15 | Loss: 0.40854161977767944\n",
      "Epoch 50 | Batch 16 | Loss: 0.5115536451339722\n",
      "Epoch 50 | Batch 17 | Loss: 0.11641388386487961\n",
      "Epoch 50 | Batch 18 | Loss: 0.5130218267440796\n",
      "Epoch 50 | Batch 19 | Loss: 0.17179104685783386\n",
      "Epoch 50 | Batch 20 | Loss: 0.14571154117584229\n",
      "Epoch 50 | Batch 21 | Loss: 0.35923531651496887\n",
      "Epoch 50 | Batch 22 | Loss: 0.456338107585907\n",
      "Epoch 50 | Batch 23 | Loss: 0.18498247861862183\n",
      "Epoch 50 | Batch 24 | Loss: 0.16689345240592957\n",
      "Epoch 50 | Batch 25 | Loss: 0.2840791344642639\n",
      "Epoch 50 | Batch 26 | Loss: 0.4182000160217285\n",
      "Epoch 50 | Batch 27 | Loss: 0.19342544674873352\n",
      "Epoch 50 | Batch 28 | Loss: 0.157577782869339\n",
      "Epoch 50 | Batch 29 | Loss: 0.3821696639060974\n",
      "Epoch 50 | Batch 30 | Loss: 0.15822488069534302\n",
      "Epoch 50 | Batch 31 | Loss: 0.17796453833580017\n",
      "Epoch 50 | Batch 32 | Loss: 0.2922251224517822\n",
      "Epoch 50 | Batch 33 | Loss: 0.5661489963531494\n",
      "Epoch 50 | Batch 34 | Loss: 0.32245180010795593\n",
      "Epoch 50 | Batch 35 | Loss: 0.5314319133758545\n",
      "Epoch 50 | Batch 36 | Loss: 0.10590534657239914\n",
      "Epoch 50 | Batch 37 | Loss: 0.2767428457736969\n",
      "Epoch 50 | Batch 38 | Loss: 0.1970539093017578\n",
      "Epoch 50 | Batch 39 | Loss: 0.3724684715270996\n",
      "Epoch 50 | Batch 40 | Loss: 0.25724655389785767\n",
      "Epoch 50 | Batch 41 | Loss: 0.6830666065216064\n",
      "Epoch 50 | Batch 42 | Loss: 0.44986069202423096\n",
      "Epoch 50 | Batch 43 | Loss: 0.2959821820259094\n",
      "Epoch 50 | Batch 44 | Loss: 0.5053104162216187\n",
      "Epoch 50 | Batch 45 | Loss: 0.3087020516395569\n",
      "Epoch 50 | Batch 46 | Loss: 0.6861428022384644\n",
      "Epoch 50 | Batch 47 | Loss: 0.5759140849113464\n",
      "Epoch 50 | Batch 48 | Loss: 0.133539617061615\n",
      "Epoch 50 | Batch 49 | Loss: 0.17126718163490295\n",
      "Epoch 50 | Batch 50 | Loss: 0.3638588786125183\n",
      "Epoch 50 | Batch 51 | Loss: 0.8098639249801636\n",
      "Epoch 50 | Batch 52 | Loss: 0.39994749426841736\n",
      "Epoch 50 | Batch 53 | Loss: 0.6044840812683105\n",
      "Epoch 50 | Batch 54 | Loss: 0.1914823204278946\n",
      "Epoch 50 | Batch 55 | Loss: 0.5594033598899841\n",
      "Epoch 50 | Batch 56 | Loss: 0.4490966498851776\n",
      "Epoch 50 | Batch 57 | Loss: 0.07111331075429916\n",
      "Epoch 50 | Batch 58 | Loss: 0.3189477026462555\n",
      "Epoch 50 | Batch 59 | Loss: 0.28764045238494873\n",
      "Epoch 50 | Batch 60 | Loss: 0.9634501934051514\n",
      "Epoch 50 | Batch 61 | Loss: 0.6019377708435059\n",
      "Epoch 50 | Batch 62 | Loss: 0.41764748096466064\n",
      "Epoch 50 | Batch 63 | Loss: 0.3130144476890564\n",
      "Epoch 50 | Batch 64 | Loss: 0.531291127204895\n",
      "Epoch 50 | Batch 65 | Loss: 0.4429095983505249\n",
      "Epoch 50 | Batch 66 | Loss: 0.1684490442276001\n",
      "Epoch 50 | Batch 67 | Loss: 0.3142017722129822\n",
      "Epoch 50 | Batch 68 | Loss: 0.24452941119670868\n",
      "Epoch 50 | Batch 69 | Loss: 0.44525325298309326\n",
      "Epoch 50 | Batch 70 | Loss: 0.5213779211044312\n",
      "Epoch 50 | Batch 71 | Loss: 0.3513489067554474\n",
      "Epoch 50 | Batch 72 | Loss: 0.3461012840270996\n",
      "Epoch 50 | Batch 73 | Loss: 0.48392564058303833\n",
      "Epoch 50 | Batch 74 | Loss: 0.31127071380615234\n",
      "Epoch 50 | Batch 75 | Loss: 0.4462146759033203\n",
      "Epoch 50 | Batch 76 | Loss: 0.23420189321041107\n",
      "Epoch 50 | Batch 77 | Loss: 0.1453755497932434\n",
      "Epoch 50 | Batch 78 | Loss: 0.1802414208650589\n",
      "Epoch 50 | Batch 79 | Loss: 0.25277605652809143\n",
      "Epoch 50 | Batch 80 | Loss: 0.2691586911678314\n",
      "Epoch 50 | Batch 81 | Loss: 0.7792324423789978\n",
      "Epoch 50 | Batch 82 | Loss: 0.24331167340278625\n",
      "Epoch 50 | Batch 83 | Loss: 0.3227875828742981\n",
      "Epoch 50 | Batch 84 | Loss: 0.13564524054527283\n",
      "Epoch 50 | Batch 85 | Loss: 0.13793113827705383\n",
      "Epoch 50 | Batch 86 | Loss: 0.30907562375068665\n",
      "Epoch 50 | Batch 87 | Loss: 0.6655373573303223\n",
      "Epoch 50 | Batch 88 | Loss: 0.11951444298028946\n",
      "Epoch 50 | Batch 89 | Loss: 0.9501311779022217\n",
      "Epoch 50 | Batch 90 | Loss: 8.035871360334568e-06\n",
      "Epoch 51 | Batch 1 | Loss: 0.0660061240196228\n",
      "Epoch 51 | Batch 2 | Loss: 0.3388786017894745\n",
      "Epoch 51 | Batch 3 | Loss: 0.21696241199970245\n",
      "Epoch 51 | Batch 4 | Loss: 0.4400949478149414\n",
      "Epoch 51 | Batch 5 | Loss: 0.23601391911506653\n",
      "Epoch 51 | Batch 6 | Loss: 0.4599902331829071\n",
      "Epoch 51 | Batch 7 | Loss: 0.3678025007247925\n",
      "Epoch 51 | Batch 8 | Loss: 0.30462393164634705\n",
      "Epoch 51 | Batch 9 | Loss: 0.7536019086837769\n",
      "Epoch 51 | Batch 10 | Loss: 0.3662892282009125\n",
      "Epoch 51 | Batch 11 | Loss: 0.4013238549232483\n",
      "Epoch 51 | Batch 12 | Loss: 0.24772459268569946\n",
      "Epoch 51 | Batch 13 | Loss: 0.06598764657974243\n",
      "Epoch 51 | Batch 14 | Loss: 0.46684396266937256\n",
      "Epoch 51 | Batch 15 | Loss: 0.665617823600769\n",
      "Epoch 51 | Batch 16 | Loss: 0.3840728998184204\n",
      "Epoch 51 | Batch 17 | Loss: 0.22132471203804016\n",
      "Epoch 51 | Batch 18 | Loss: 0.956352174282074\n",
      "Epoch 51 | Batch 19 | Loss: 0.39938488602638245\n",
      "Epoch 51 | Batch 20 | Loss: 0.6361236572265625\n",
      "Epoch 51 | Batch 21 | Loss: 0.3087891936302185\n",
      "Epoch 51 | Batch 22 | Loss: 0.1001465767621994\n",
      "Epoch 51 | Batch 23 | Loss: 0.25559213757514954\n",
      "Epoch 51 | Batch 24 | Loss: 0.25637713074684143\n",
      "Epoch 51 | Batch 25 | Loss: 0.5697261691093445\n",
      "Epoch 51 | Batch 26 | Loss: 0.47266045212745667\n",
      "Epoch 51 | Batch 27 | Loss: 0.27909889817237854\n",
      "Epoch 51 | Batch 28 | Loss: 0.2573774456977844\n",
      "Epoch 51 | Batch 29 | Loss: 0.8960488438606262\n",
      "Epoch 51 | Batch 30 | Loss: 0.3690246045589447\n",
      "Epoch 51 | Batch 31 | Loss: 0.22221475839614868\n",
      "Epoch 51 | Batch 32 | Loss: 0.4359963536262512\n",
      "Epoch 51 | Batch 33 | Loss: 0.6669394969940186\n",
      "Epoch 51 | Batch 34 | Loss: 0.23456156253814697\n",
      "Epoch 51 | Batch 35 | Loss: 0.6280573606491089\n",
      "Epoch 51 | Batch 36 | Loss: 0.2061132937669754\n",
      "Epoch 51 | Batch 37 | Loss: 0.732257604598999\n",
      "Epoch 51 | Batch 38 | Loss: 0.6781152486801147\n",
      "Epoch 51 | Batch 39 | Loss: 0.2553287744522095\n",
      "Epoch 51 | Batch 40 | Loss: 0.6092530488967896\n",
      "Epoch 51 | Batch 41 | Loss: 0.5269826650619507\n",
      "Epoch 51 | Batch 42 | Loss: 0.21067920327186584\n",
      "Epoch 51 | Batch 43 | Loss: 0.1903996765613556\n",
      "Epoch 51 | Batch 44 | Loss: 0.7241917848587036\n",
      "Epoch 51 | Batch 45 | Loss: 0.5207613110542297\n",
      "Epoch 51 | Batch 46 | Loss: 0.2938234806060791\n",
      "Epoch 51 | Batch 47 | Loss: 0.15748131275177002\n",
      "Epoch 51 | Batch 48 | Loss: 0.15551018714904785\n",
      "Epoch 51 | Batch 49 | Loss: 0.30030447244644165\n",
      "Epoch 51 | Batch 50 | Loss: 0.48350781202316284\n",
      "Epoch 51 | Batch 51 | Loss: 0.24358844757080078\n",
      "Epoch 51 | Batch 52 | Loss: 0.16459238529205322\n",
      "Epoch 51 | Batch 53 | Loss: 0.24314367771148682\n",
      "Epoch 51 | Batch 54 | Loss: 0.46194326877593994\n",
      "Epoch 51 | Batch 55 | Loss: 0.5399957895278931\n",
      "Epoch 51 | Batch 56 | Loss: 0.39979755878448486\n",
      "Epoch 51 | Batch 57 | Loss: 0.2043767124414444\n",
      "Epoch 51 | Batch 58 | Loss: 0.17553409934043884\n",
      "Epoch 51 | Batch 59 | Loss: 0.43293169140815735\n",
      "Epoch 51 | Batch 60 | Loss: 0.30353373289108276\n",
      "Epoch 51 | Batch 61 | Loss: 0.7099286913871765\n",
      "Epoch 51 | Batch 62 | Loss: 0.2301044464111328\n",
      "Epoch 51 | Batch 63 | Loss: 0.39461061358451843\n",
      "Epoch 51 | Batch 64 | Loss: 0.5616004467010498\n",
      "Epoch 51 | Batch 65 | Loss: 0.6204395890235901\n",
      "Epoch 51 | Batch 66 | Loss: 0.27730780839920044\n",
      "Epoch 51 | Batch 67 | Loss: 0.14472723007202148\n",
      "Epoch 51 | Batch 68 | Loss: 0.222225621342659\n",
      "Epoch 51 | Batch 69 | Loss: 0.4384118914604187\n",
      "Epoch 51 | Batch 70 | Loss: 0.07810573279857635\n",
      "Epoch 51 | Batch 71 | Loss: 0.20482514798641205\n",
      "Epoch 51 | Batch 72 | Loss: 0.38938355445861816\n",
      "Epoch 51 | Batch 73 | Loss: 0.16869398951530457\n",
      "Epoch 51 | Batch 74 | Loss: 0.3768043518066406\n",
      "Epoch 51 | Batch 75 | Loss: 0.6958054304122925\n",
      "Epoch 51 | Batch 76 | Loss: 0.31547388434410095\n",
      "Epoch 51 | Batch 77 | Loss: 0.6809755563735962\n",
      "Epoch 51 | Batch 78 | Loss: 0.3473186194896698\n",
      "Epoch 51 | Batch 79 | Loss: 0.20419836044311523\n",
      "Epoch 51 | Batch 80 | Loss: 0.17125961184501648\n",
      "Epoch 51 | Batch 81 | Loss: 0.11182437837123871\n",
      "Epoch 51 | Batch 82 | Loss: 0.5927693247795105\n",
      "Epoch 51 | Batch 83 | Loss: 0.3925667405128479\n",
      "Epoch 51 | Batch 84 | Loss: 0.08662398904561996\n",
      "Epoch 51 | Batch 85 | Loss: 0.5669105052947998\n",
      "Epoch 51 | Batch 86 | Loss: 0.17348778247833252\n",
      "Epoch 51 | Batch 87 | Loss: 0.08804360777139664\n",
      "Epoch 51 | Batch 88 | Loss: 0.42992454767227173\n",
      "Epoch 51 | Batch 89 | Loss: 0.20840111374855042\n",
      "Epoch 51 | Batch 90 | Loss: 0.69712895154953\n",
      "Epoch 52 | Batch 1 | Loss: 0.41573935747146606\n",
      "Epoch 52 | Batch 2 | Loss: 0.21141831576824188\n",
      "Epoch 52 | Batch 3 | Loss: 0.28941020369529724\n",
      "Epoch 52 | Batch 4 | Loss: 0.1560705304145813\n",
      "Epoch 52 | Batch 5 | Loss: 0.34746426343917847\n",
      "Epoch 52 | Batch 6 | Loss: 0.3222225606441498\n",
      "Epoch 52 | Batch 7 | Loss: 0.2814147472381592\n",
      "Epoch 52 | Batch 8 | Loss: 0.0968143492937088\n",
      "Epoch 52 | Batch 9 | Loss: 0.4780975580215454\n",
      "Epoch 52 | Batch 10 | Loss: 0.20193925499916077\n",
      "Epoch 52 | Batch 11 | Loss: 0.41704338788986206\n",
      "Epoch 52 | Batch 12 | Loss: 0.24423491954803467\n",
      "Epoch 52 | Batch 13 | Loss: 0.4983288049697876\n",
      "Epoch 52 | Batch 14 | Loss: 0.5957685708999634\n",
      "Epoch 52 | Batch 15 | Loss: 0.13409677147865295\n",
      "Epoch 52 | Batch 16 | Loss: 0.09470877051353455\n",
      "Epoch 52 | Batch 17 | Loss: 0.6315338015556335\n",
      "Epoch 52 | Batch 18 | Loss: 0.27805036306381226\n",
      "Epoch 52 | Batch 19 | Loss: 0.31321364641189575\n",
      "Epoch 52 | Batch 20 | Loss: 0.3362607955932617\n",
      "Epoch 52 | Batch 21 | Loss: 0.45980384945869446\n",
      "Epoch 52 | Batch 22 | Loss: 0.4433991312980652\n",
      "Epoch 52 | Batch 23 | Loss: 0.5675732493400574\n",
      "Epoch 52 | Batch 24 | Loss: 0.37897783517837524\n",
      "Epoch 52 | Batch 25 | Loss: 0.17031057178974152\n",
      "Epoch 52 | Batch 26 | Loss: 0.1836356669664383\n",
      "Epoch 52 | Batch 27 | Loss: 0.747015118598938\n",
      "Epoch 52 | Batch 28 | Loss: 0.40311649441719055\n",
      "Epoch 52 | Batch 29 | Loss: 0.498613178730011\n",
      "Epoch 52 | Batch 30 | Loss: 0.7013599872589111\n",
      "Epoch 52 | Batch 31 | Loss: 0.1827557384967804\n",
      "Epoch 52 | Batch 32 | Loss: 0.08409814536571503\n",
      "Epoch 52 | Batch 33 | Loss: 0.12194177508354187\n",
      "Epoch 52 | Batch 34 | Loss: 0.3888357877731323\n",
      "Epoch 52 | Batch 35 | Loss: 0.41542530059814453\n",
      "Epoch 52 | Batch 36 | Loss: 0.7811087369918823\n",
      "Epoch 52 | Batch 37 | Loss: 0.4649626314640045\n",
      "Epoch 52 | Batch 38 | Loss: 0.34080493450164795\n",
      "Epoch 52 | Batch 39 | Loss: 0.30483144521713257\n",
      "Epoch 52 | Batch 40 | Loss: 0.8035221099853516\n",
      "Epoch 52 | Batch 41 | Loss: 0.4338415861129761\n",
      "Epoch 52 | Batch 42 | Loss: 0.23513416945934296\n",
      "Epoch 52 | Batch 43 | Loss: 0.43340110778808594\n",
      "Epoch 52 | Batch 44 | Loss: 0.1004180908203125\n",
      "Epoch 52 | Batch 45 | Loss: 0.6177506446838379\n",
      "Epoch 52 | Batch 46 | Loss: 0.4181516766548157\n",
      "Epoch 52 | Batch 47 | Loss: 0.7008117437362671\n",
      "Epoch 52 | Batch 48 | Loss: 0.14532345533370972\n",
      "Epoch 52 | Batch 49 | Loss: 0.7792037129402161\n",
      "Epoch 52 | Batch 50 | Loss: 0.33591464161872864\n",
      "Epoch 52 | Batch 51 | Loss: 0.26232045888900757\n",
      "Epoch 52 | Batch 52 | Loss: 0.45600587129592896\n",
      "Epoch 52 | Batch 53 | Loss: 0.6609485149383545\n",
      "Epoch 52 | Batch 54 | Loss: 0.4020683765411377\n",
      "Epoch 52 | Batch 55 | Loss: 0.19617441296577454\n",
      "Epoch 52 | Batch 56 | Loss: 0.18826356530189514\n",
      "Epoch 52 | Batch 57 | Loss: 0.42381712794303894\n",
      "Epoch 52 | Batch 58 | Loss: 0.4469297528266907\n",
      "Epoch 52 | Batch 59 | Loss: 0.2949334979057312\n",
      "Epoch 52 | Batch 60 | Loss: 0.18935146927833557\n",
      "Epoch 52 | Batch 61 | Loss: 0.1976107656955719\n",
      "Epoch 52 | Batch 62 | Loss: 0.4098894000053406\n",
      "Epoch 52 | Batch 63 | Loss: 0.784256100654602\n",
      "Epoch 52 | Batch 64 | Loss: 0.3943464457988739\n",
      "Epoch 52 | Batch 65 | Loss: 0.4709567427635193\n",
      "Epoch 52 | Batch 66 | Loss: 0.23084665834903717\n",
      "Epoch 52 | Batch 67 | Loss: 0.4835420548915863\n",
      "Epoch 52 | Batch 68 | Loss: 0.26123499870300293\n",
      "Epoch 52 | Batch 69 | Loss: 0.5498219728469849\n",
      "Epoch 52 | Batch 70 | Loss: 0.34255534410476685\n",
      "Epoch 52 | Batch 71 | Loss: 0.4309450387954712\n",
      "Epoch 52 | Batch 72 | Loss: 0.12496969103813171\n",
      "Epoch 52 | Batch 73 | Loss: 0.3124667704105377\n",
      "Epoch 52 | Batch 74 | Loss: 0.42648813128471375\n",
      "Epoch 52 | Batch 75 | Loss: 0.19695258140563965\n",
      "Epoch 52 | Batch 76 | Loss: 0.11617864668369293\n",
      "Epoch 52 | Batch 77 | Loss: 0.1976245939731598\n",
      "Epoch 52 | Batch 78 | Loss: 0.8972486257553101\n",
      "Epoch 52 | Batch 79 | Loss: 0.4608531594276428\n",
      "Epoch 52 | Batch 80 | Loss: 0.3084644079208374\n",
      "Epoch 52 | Batch 81 | Loss: 0.12774766981601715\n",
      "Epoch 52 | Batch 82 | Loss: 0.12919223308563232\n",
      "Epoch 52 | Batch 83 | Loss: 0.279190331697464\n",
      "Epoch 52 | Batch 84 | Loss: 0.5215231776237488\n",
      "Epoch 52 | Batch 85 | Loss: 0.20825129747390747\n",
      "Epoch 52 | Batch 86 | Loss: 0.4527535140514374\n",
      "Epoch 52 | Batch 87 | Loss: 0.5083543658256531\n",
      "Epoch 52 | Batch 88 | Loss: 0.3902651369571686\n",
      "Epoch 52 | Batch 89 | Loss: 0.5470865368843079\n",
      "Epoch 52 | Batch 90 | Loss: 0.12396099418401718\n",
      "Epoch 53 | Batch 1 | Loss: 0.10516926646232605\n",
      "Epoch 53 | Batch 2 | Loss: 0.7825419306755066\n",
      "Epoch 53 | Batch 3 | Loss: 0.41055843234062195\n",
      "Epoch 53 | Batch 4 | Loss: 0.47350800037384033\n",
      "Epoch 53 | Batch 5 | Loss: 0.20474210381507874\n",
      "Epoch 53 | Batch 6 | Loss: 0.4899689853191376\n",
      "Epoch 53 | Batch 7 | Loss: 0.26193666458129883\n",
      "Epoch 53 | Batch 8 | Loss: 0.35823288559913635\n",
      "Epoch 53 | Batch 9 | Loss: 0.31003937125205994\n",
      "Epoch 53 | Batch 10 | Loss: 0.9984728693962097\n",
      "Epoch 53 | Batch 11 | Loss: 0.22426855564117432\n",
      "Epoch 53 | Batch 12 | Loss: 0.20020917057991028\n",
      "Epoch 53 | Batch 13 | Loss: 0.1836826354265213\n",
      "Epoch 53 | Batch 14 | Loss: 0.5977325439453125\n",
      "Epoch 53 | Batch 15 | Loss: 0.2144632190465927\n",
      "Epoch 53 | Batch 16 | Loss: 0.4499207139015198\n",
      "Epoch 53 | Batch 17 | Loss: 0.3078213036060333\n",
      "Epoch 53 | Batch 18 | Loss: 0.3525024950504303\n",
      "Epoch 53 | Batch 19 | Loss: 0.24593700468540192\n",
      "Epoch 53 | Batch 20 | Loss: 0.17120787501335144\n",
      "Epoch 53 | Batch 21 | Loss: 0.3526056408882141\n",
      "Epoch 53 | Batch 22 | Loss: 0.37191182374954224\n",
      "Epoch 53 | Batch 23 | Loss: 0.2391340136528015\n",
      "Epoch 53 | Batch 24 | Loss: 0.18731442093849182\n",
      "Epoch 53 | Batch 25 | Loss: 0.4592316746711731\n",
      "Epoch 53 | Batch 26 | Loss: 0.26732271909713745\n",
      "Epoch 53 | Batch 27 | Loss: 0.7218661308288574\n",
      "Epoch 53 | Batch 28 | Loss: 0.13127759099006653\n",
      "Epoch 53 | Batch 29 | Loss: 0.13713641464710236\n",
      "Epoch 53 | Batch 30 | Loss: 0.7994239330291748\n",
      "Epoch 53 | Batch 31 | Loss: 0.4672868549823761\n",
      "Epoch 53 | Batch 32 | Loss: 0.16214343905448914\n",
      "Epoch 53 | Batch 33 | Loss: 0.5989484786987305\n",
      "Epoch 53 | Batch 34 | Loss: 0.061943165957927704\n",
      "Epoch 53 | Batch 35 | Loss: 0.518029510974884\n",
      "Epoch 53 | Batch 36 | Loss: 0.4864780306816101\n",
      "Epoch 53 | Batch 37 | Loss: 0.21235477924346924\n",
      "Epoch 53 | Batch 38 | Loss: 0.10936138778924942\n",
      "Epoch 53 | Batch 39 | Loss: 0.3601619601249695\n",
      "Epoch 53 | Batch 40 | Loss: 0.17359663546085358\n",
      "Epoch 53 | Batch 41 | Loss: 0.4404509961605072\n",
      "Epoch 53 | Batch 42 | Loss: 0.2946961224079132\n",
      "Epoch 53 | Batch 43 | Loss: 0.1273379921913147\n",
      "Epoch 53 | Batch 44 | Loss: 0.46068599820137024\n",
      "Epoch 53 | Batch 45 | Loss: 0.3200731575489044\n",
      "Epoch 53 | Batch 46 | Loss: 0.9268683195114136\n",
      "Epoch 53 | Batch 47 | Loss: 0.2103969156742096\n",
      "Epoch 53 | Batch 48 | Loss: 0.34456300735473633\n",
      "Epoch 53 | Batch 49 | Loss: 0.42096108198165894\n",
      "Epoch 53 | Batch 50 | Loss: 0.3731539845466614\n",
      "Epoch 53 | Batch 51 | Loss: 0.4463043510913849\n",
      "Epoch 53 | Batch 52 | Loss: 0.6919401288032532\n",
      "Epoch 53 | Batch 53 | Loss: 0.21881994605064392\n",
      "Epoch 53 | Batch 54 | Loss: 0.1262720376253128\n",
      "Epoch 53 | Batch 55 | Loss: 0.24507614970207214\n",
      "Epoch 53 | Batch 56 | Loss: 0.6821967363357544\n",
      "Epoch 53 | Batch 57 | Loss: 0.6133583784103394\n",
      "Epoch 53 | Batch 58 | Loss: 0.24688178300857544\n",
      "Epoch 53 | Batch 59 | Loss: 0.6703580021858215\n",
      "Epoch 53 | Batch 60 | Loss: 0.2524241805076599\n",
      "Epoch 53 | Batch 61 | Loss: 0.7030172348022461\n",
      "Epoch 53 | Batch 62 | Loss: 0.44046369194984436\n",
      "Epoch 53 | Batch 63 | Loss: 0.43475988507270813\n",
      "Epoch 53 | Batch 64 | Loss: 0.13015612959861755\n",
      "Epoch 53 | Batch 65 | Loss: 0.40179872512817383\n",
      "Epoch 53 | Batch 66 | Loss: 0.33378976583480835\n",
      "Epoch 53 | Batch 67 | Loss: 0.3348296880722046\n",
      "Epoch 53 | Batch 68 | Loss: 0.5081865787506104\n",
      "Epoch 53 | Batch 69 | Loss: 0.18549507856369019\n",
      "Epoch 53 | Batch 70 | Loss: 0.2391548454761505\n",
      "Epoch 53 | Batch 71 | Loss: 0.7756308913230896\n",
      "Epoch 53 | Batch 72 | Loss: 0.3731042146682739\n",
      "Epoch 53 | Batch 73 | Loss: 0.20813488960266113\n",
      "Epoch 53 | Batch 74 | Loss: 0.30860137939453125\n",
      "Epoch 53 | Batch 75 | Loss: 0.47429054975509644\n",
      "Epoch 53 | Batch 76 | Loss: 0.5473402738571167\n",
      "Epoch 53 | Batch 77 | Loss: 0.2180156409740448\n",
      "Epoch 53 | Batch 78 | Loss: 0.5668127536773682\n",
      "Epoch 53 | Batch 79 | Loss: 0.2725023031234741\n",
      "Epoch 53 | Batch 80 | Loss: 0.4895937144756317\n",
      "Epoch 53 | Batch 81 | Loss: 0.13214868307113647\n",
      "Epoch 53 | Batch 82 | Loss: 0.2838627099990845\n",
      "Epoch 53 | Batch 83 | Loss: 0.12910175323486328\n",
      "Epoch 53 | Batch 84 | Loss: 0.710114598274231\n",
      "Epoch 53 | Batch 85 | Loss: 0.28448763489723206\n",
      "Epoch 53 | Batch 86 | Loss: 0.4055875241756439\n",
      "Epoch 53 | Batch 87 | Loss: 0.4188899099826813\n",
      "Epoch 53 | Batch 88 | Loss: 0.5019362568855286\n",
      "Epoch 53 | Batch 89 | Loss: 0.2169352024793625\n",
      "Epoch 53 | Batch 90 | Loss: 0.1348371058702469\n",
      "Epoch 54 | Batch 1 | Loss: 0.24883852899074554\n",
      "Epoch 54 | Batch 2 | Loss: 0.29186487197875977\n",
      "Epoch 54 | Batch 3 | Loss: 0.6365588903427124\n",
      "Epoch 54 | Batch 4 | Loss: 0.4770280718803406\n",
      "Epoch 54 | Batch 5 | Loss: 0.3835704028606415\n",
      "Epoch 54 | Batch 6 | Loss: 0.5749219655990601\n",
      "Epoch 54 | Batch 7 | Loss: 0.6688396334648132\n",
      "Epoch 54 | Batch 8 | Loss: 0.7092525959014893\n",
      "Epoch 54 | Batch 9 | Loss: 0.3965047299861908\n",
      "Epoch 54 | Batch 10 | Loss: 0.12815162539482117\n",
      "Epoch 54 | Batch 11 | Loss: 0.6284068822860718\n",
      "Epoch 54 | Batch 12 | Loss: 0.3971095085144043\n",
      "Epoch 54 | Batch 13 | Loss: 0.5760239362716675\n",
      "Epoch 54 | Batch 14 | Loss: 0.4226851463317871\n",
      "Epoch 54 | Batch 15 | Loss: 0.13445469737052917\n",
      "Epoch 54 | Batch 16 | Loss: 0.6507257223129272\n",
      "Epoch 54 | Batch 17 | Loss: 0.6226316690444946\n",
      "Epoch 54 | Batch 18 | Loss: 0.3677249848842621\n",
      "Epoch 54 | Batch 19 | Loss: 0.30806684494018555\n",
      "Epoch 54 | Batch 20 | Loss: 0.5620371103286743\n",
      "Epoch 54 | Batch 21 | Loss: 0.11556755006313324\n",
      "Epoch 54 | Batch 22 | Loss: 0.3407347798347473\n",
      "Epoch 54 | Batch 23 | Loss: 0.4615001082420349\n",
      "Epoch 54 | Batch 24 | Loss: 0.2264060378074646\n",
      "Epoch 54 | Batch 25 | Loss: 0.23478445410728455\n",
      "Epoch 54 | Batch 26 | Loss: 0.1205596849322319\n",
      "Epoch 54 | Batch 27 | Loss: 0.43463772535324097\n",
      "Epoch 54 | Batch 28 | Loss: 0.6287657022476196\n",
      "Epoch 54 | Batch 29 | Loss: 0.16027432680130005\n",
      "Epoch 54 | Batch 30 | Loss: 0.2685932517051697\n",
      "Epoch 54 | Batch 31 | Loss: 0.4630401134490967\n",
      "Epoch 54 | Batch 32 | Loss: 0.10720530152320862\n",
      "Epoch 54 | Batch 33 | Loss: 0.3551897704601288\n",
      "Epoch 54 | Batch 34 | Loss: 0.33757078647613525\n",
      "Epoch 54 | Batch 35 | Loss: 0.3691490590572357\n",
      "Epoch 54 | Batch 36 | Loss: 0.5507916212081909\n",
      "Epoch 54 | Batch 37 | Loss: 0.6450417041778564\n",
      "Epoch 54 | Batch 38 | Loss: 0.05359261855483055\n",
      "Epoch 54 | Batch 39 | Loss: 0.2114594280719757\n",
      "Epoch 54 | Batch 40 | Loss: 0.8440001010894775\n",
      "Epoch 54 | Batch 41 | Loss: 0.4026939272880554\n",
      "Epoch 54 | Batch 42 | Loss: 0.13414496183395386\n",
      "Epoch 54 | Batch 43 | Loss: 0.22753779590129852\n",
      "Epoch 54 | Batch 44 | Loss: 0.40070343017578125\n",
      "Epoch 54 | Batch 45 | Loss: 0.2454276978969574\n",
      "Epoch 54 | Batch 46 | Loss: 0.2907523512840271\n",
      "Epoch 54 | Batch 47 | Loss: 0.215535968542099\n",
      "Epoch 54 | Batch 48 | Loss: 0.3073274493217468\n",
      "Epoch 54 | Batch 49 | Loss: 0.4274924099445343\n",
      "Epoch 54 | Batch 50 | Loss: 0.45248284935951233\n",
      "Epoch 54 | Batch 51 | Loss: 0.22010411322116852\n",
      "Epoch 54 | Batch 52 | Loss: 0.49758386611938477\n",
      "Epoch 54 | Batch 53 | Loss: 0.4284760355949402\n",
      "Epoch 54 | Batch 54 | Loss: 0.23600035905838013\n",
      "Epoch 54 | Batch 55 | Loss: 0.251127153635025\n",
      "Epoch 54 | Batch 56 | Loss: 0.34086841344833374\n",
      "Epoch 54 | Batch 57 | Loss: 0.3976629376411438\n",
      "Epoch 54 | Batch 58 | Loss: 0.11132529377937317\n",
      "Epoch 54 | Batch 59 | Loss: 0.37002086639404297\n",
      "Epoch 54 | Batch 60 | Loss: 0.1716756671667099\n",
      "Epoch 54 | Batch 61 | Loss: 0.29268166422843933\n",
      "Epoch 54 | Batch 62 | Loss: 0.6374012231826782\n",
      "Epoch 54 | Batch 63 | Loss: 0.3677684962749481\n",
      "Epoch 54 | Batch 64 | Loss: 0.8947862982749939\n",
      "Epoch 54 | Batch 65 | Loss: 0.17182937264442444\n",
      "Epoch 54 | Batch 66 | Loss: 0.3203733563423157\n",
      "Epoch 54 | Batch 67 | Loss: 0.35819101333618164\n",
      "Epoch 54 | Batch 68 | Loss: 0.40443649888038635\n",
      "Epoch 54 | Batch 69 | Loss: 0.35470449924468994\n",
      "Epoch 54 | Batch 70 | Loss: 0.09201282262802124\n",
      "Epoch 54 | Batch 71 | Loss: 0.37977489829063416\n",
      "Epoch 54 | Batch 72 | Loss: 0.3740500211715698\n",
      "Epoch 54 | Batch 73 | Loss: 0.39225640892982483\n",
      "Epoch 54 | Batch 74 | Loss: 0.4162771999835968\n",
      "Epoch 54 | Batch 75 | Loss: 0.6425414085388184\n",
      "Epoch 54 | Batch 76 | Loss: 0.3983447551727295\n",
      "Epoch 54 | Batch 77 | Loss: 0.3258748948574066\n",
      "Epoch 54 | Batch 78 | Loss: 0.070317342877388\n",
      "Epoch 54 | Batch 79 | Loss: 0.5366969108581543\n",
      "Epoch 54 | Batch 80 | Loss: 0.17003659904003143\n",
      "Epoch 54 | Batch 81 | Loss: 0.13174743950366974\n",
      "Epoch 54 | Batch 82 | Loss: 0.6220115423202515\n",
      "Epoch 54 | Batch 83 | Loss: 0.26847246289253235\n",
      "Epoch 54 | Batch 84 | Loss: 0.12273004651069641\n",
      "Epoch 54 | Batch 85 | Loss: 0.2209760546684265\n",
      "Epoch 54 | Batch 86 | Loss: 0.999574601650238\n",
      "Epoch 54 | Batch 87 | Loss: 0.44632473587989807\n",
      "Epoch 54 | Batch 88 | Loss: 0.22833046317100525\n",
      "Epoch 54 | Batch 89 | Loss: 0.4694770574569702\n",
      "Epoch 54 | Batch 90 | Loss: 4.5174150727689266e-05\n",
      "Epoch 55 | Batch 1 | Loss: 0.5092598795890808\n",
      "Epoch 55 | Batch 2 | Loss: 0.4454856514930725\n",
      "Epoch 55 | Batch 3 | Loss: 0.6046212911605835\n",
      "Epoch 55 | Batch 4 | Loss: 0.5360851287841797\n",
      "Epoch 55 | Batch 5 | Loss: 0.6262499094009399\n",
      "Epoch 55 | Batch 6 | Loss: 0.3439030647277832\n",
      "Epoch 55 | Batch 7 | Loss: 0.41241031885147095\n",
      "Epoch 55 | Batch 8 | Loss: 0.4652882218360901\n",
      "Epoch 55 | Batch 9 | Loss: 0.6354042291641235\n",
      "Epoch 55 | Batch 10 | Loss: 0.25270265340805054\n",
      "Epoch 55 | Batch 11 | Loss: 0.40679043531417847\n",
      "Epoch 55 | Batch 12 | Loss: 0.407130628824234\n",
      "Epoch 55 | Batch 13 | Loss: 0.5483925342559814\n",
      "Epoch 55 | Batch 14 | Loss: 0.4010307192802429\n",
      "Epoch 55 | Batch 15 | Loss: 0.8882171511650085\n",
      "Epoch 55 | Batch 16 | Loss: 0.09318200498819351\n",
      "Epoch 55 | Batch 17 | Loss: 0.41849589347839355\n",
      "Epoch 55 | Batch 18 | Loss: 0.2918294668197632\n",
      "Epoch 55 | Batch 19 | Loss: 0.17118458449840546\n",
      "Epoch 55 | Batch 20 | Loss: 0.10325619578361511\n",
      "Epoch 55 | Batch 21 | Loss: 0.3292512595653534\n",
      "Epoch 55 | Batch 22 | Loss: 0.6096917986869812\n",
      "Epoch 55 | Batch 23 | Loss: 0.34383851289749146\n",
      "Epoch 55 | Batch 24 | Loss: 0.3324212431907654\n",
      "Epoch 55 | Batch 25 | Loss: 0.4351201057434082\n",
      "Epoch 55 | Batch 26 | Loss: 0.2353246957063675\n",
      "Epoch 55 | Batch 27 | Loss: 0.8004018068313599\n",
      "Epoch 55 | Batch 28 | Loss: 0.08707591891288757\n",
      "Epoch 55 | Batch 29 | Loss: 0.3601379096508026\n",
      "Epoch 55 | Batch 30 | Loss: 0.1292831003665924\n",
      "Epoch 55 | Batch 31 | Loss: 0.4240047335624695\n",
      "Epoch 55 | Batch 32 | Loss: 0.24373868107795715\n",
      "Epoch 55 | Batch 33 | Loss: 0.40613237023353577\n",
      "Epoch 55 | Batch 34 | Loss: 0.1573798805475235\n",
      "Epoch 55 | Batch 35 | Loss: 0.20053431391716003\n",
      "Epoch 55 | Batch 36 | Loss: 0.3033604025840759\n",
      "Epoch 55 | Batch 37 | Loss: 0.5742334127426147\n",
      "Epoch 55 | Batch 38 | Loss: 0.09324903786182404\n",
      "Epoch 55 | Batch 39 | Loss: 0.3660327196121216\n",
      "Epoch 55 | Batch 40 | Loss: 0.43630942702293396\n",
      "Epoch 55 | Batch 41 | Loss: 0.17842064797878265\n",
      "Epoch 55 | Batch 42 | Loss: 0.6010721325874329\n",
      "Epoch 55 | Batch 43 | Loss: 0.41859787702560425\n",
      "Epoch 55 | Batch 44 | Loss: 0.3541561961174011\n",
      "Epoch 55 | Batch 45 | Loss: 0.49769413471221924\n",
      "Epoch 55 | Batch 46 | Loss: 0.5730037689208984\n",
      "Epoch 55 | Batch 47 | Loss: 0.42675966024398804\n",
      "Epoch 55 | Batch 48 | Loss: 0.2788906395435333\n",
      "Epoch 55 | Batch 49 | Loss: 0.17267818748950958\n",
      "Epoch 55 | Batch 50 | Loss: 0.3905514180660248\n",
      "Epoch 55 | Batch 51 | Loss: 0.35219696164131165\n",
      "Epoch 55 | Batch 52 | Loss: 0.18170906603336334\n",
      "Epoch 55 | Batch 53 | Loss: 0.5429097414016724\n",
      "Epoch 55 | Batch 54 | Loss: 0.3529210686683655\n",
      "Epoch 55 | Batch 55 | Loss: 0.37597113847732544\n",
      "Epoch 55 | Batch 56 | Loss: 0.22144247591495514\n",
      "Epoch 55 | Batch 57 | Loss: 0.4476642310619354\n",
      "Epoch 55 | Batch 58 | Loss: 0.31238076090812683\n",
      "Epoch 55 | Batch 59 | Loss: 0.25041502714157104\n",
      "Epoch 55 | Batch 60 | Loss: 0.9363455772399902\n",
      "Epoch 55 | Batch 61 | Loss: 0.26291921734809875\n",
      "Epoch 55 | Batch 62 | Loss: 0.7550607323646545\n",
      "Epoch 55 | Batch 63 | Loss: 0.6294415593147278\n",
      "Epoch 55 | Batch 64 | Loss: 0.35524290800094604\n",
      "Epoch 55 | Batch 65 | Loss: 0.06819821894168854\n",
      "Epoch 55 | Batch 66 | Loss: 0.43358510732650757\n",
      "Epoch 55 | Batch 67 | Loss: 0.5089526176452637\n",
      "Epoch 55 | Batch 68 | Loss: 0.2693881690502167\n",
      "Epoch 55 | Batch 69 | Loss: 0.2908889353275299\n",
      "Epoch 55 | Batch 70 | Loss: 0.37699052691459656\n",
      "Epoch 55 | Batch 71 | Loss: 0.2124280482530594\n",
      "Epoch 55 | Batch 72 | Loss: 0.21491043269634247\n",
      "Epoch 55 | Batch 73 | Loss: 0.3168467879295349\n",
      "Epoch 55 | Batch 74 | Loss: 0.22846285998821259\n",
      "Epoch 55 | Batch 75 | Loss: 0.7431715726852417\n",
      "Epoch 55 | Batch 76 | Loss: 0.43839141726493835\n",
      "Epoch 55 | Batch 77 | Loss: 0.5292367935180664\n",
      "Epoch 55 | Batch 78 | Loss: 0.14231877028942108\n",
      "Epoch 55 | Batch 79 | Loss: 0.2875143885612488\n",
      "Epoch 55 | Batch 80 | Loss: 0.14214082062244415\n",
      "Epoch 55 | Batch 81 | Loss: 0.25707900524139404\n",
      "Epoch 55 | Batch 82 | Loss: 0.32553011178970337\n",
      "Epoch 55 | Batch 83 | Loss: 0.12252236902713776\n",
      "Epoch 55 | Batch 84 | Loss: 0.05542977899312973\n",
      "Epoch 55 | Batch 85 | Loss: 0.42746275663375854\n",
      "Epoch 55 | Batch 86 | Loss: 0.17699182033538818\n",
      "Epoch 55 | Batch 87 | Loss: 0.11947041004896164\n",
      "Epoch 55 | Batch 88 | Loss: 0.15232419967651367\n",
      "Epoch 55 | Batch 89 | Loss: 0.4845844507217407\n",
      "Epoch 55 | Batch 90 | Loss: 0.06941793859004974\n",
      "Epoch 56 | Batch 1 | Loss: 0.6100989580154419\n",
      "Epoch 56 | Batch 2 | Loss: 0.4813697934150696\n",
      "Epoch 56 | Batch 3 | Loss: 0.12016777694225311\n",
      "Epoch 56 | Batch 4 | Loss: 0.5752456188201904\n",
      "Epoch 56 | Batch 5 | Loss: 0.6800941228866577\n",
      "Epoch 56 | Batch 6 | Loss: 0.583524227142334\n",
      "Epoch 56 | Batch 7 | Loss: 0.2522915005683899\n",
      "Epoch 56 | Batch 8 | Loss: 0.04046696797013283\n",
      "Epoch 56 | Batch 9 | Loss: 0.5821025967597961\n",
      "Epoch 56 | Batch 10 | Loss: 0.40854063630104065\n",
      "Epoch 56 | Batch 11 | Loss: 0.6708434224128723\n",
      "Epoch 56 | Batch 12 | Loss: 0.577634871006012\n",
      "Epoch 56 | Batch 13 | Loss: 0.3059688210487366\n",
      "Epoch 56 | Batch 14 | Loss: 0.28724902868270874\n",
      "Epoch 56 | Batch 15 | Loss: 0.41026681661605835\n",
      "Epoch 56 | Batch 16 | Loss: 0.47654497623443604\n",
      "Epoch 56 | Batch 17 | Loss: 0.34892386198043823\n",
      "Epoch 56 | Batch 18 | Loss: 0.5237597823143005\n",
      "Epoch 56 | Batch 19 | Loss: 0.37243402004241943\n",
      "Epoch 56 | Batch 20 | Loss: 0.4806731939315796\n",
      "Epoch 56 | Batch 21 | Loss: 0.16969946026802063\n",
      "Epoch 56 | Batch 22 | Loss: 0.345533549785614\n",
      "Epoch 56 | Batch 23 | Loss: 0.476033478975296\n",
      "Epoch 56 | Batch 24 | Loss: 0.2364913374185562\n",
      "Epoch 56 | Batch 25 | Loss: 0.43472856283187866\n",
      "Epoch 56 | Batch 26 | Loss: 0.38965922594070435\n",
      "Epoch 56 | Batch 27 | Loss: 0.3388961851596832\n",
      "Epoch 56 | Batch 28 | Loss: 0.14219127595424652\n",
      "Epoch 56 | Batch 29 | Loss: 0.6385027170181274\n",
      "Epoch 56 | Batch 30 | Loss: 0.2813912034034729\n",
      "Epoch 56 | Batch 31 | Loss: 0.6028286218643188\n",
      "Epoch 56 | Batch 32 | Loss: 0.7359288334846497\n",
      "Epoch 56 | Batch 33 | Loss: 0.8306115865707397\n",
      "Epoch 56 | Batch 34 | Loss: 0.07617659866809845\n",
      "Epoch 56 | Batch 35 | Loss: 0.3547205626964569\n",
      "Epoch 56 | Batch 36 | Loss: 0.3656448721885681\n",
      "Epoch 56 | Batch 37 | Loss: 0.30334365367889404\n",
      "Epoch 56 | Batch 38 | Loss: 0.2932475209236145\n",
      "Epoch 56 | Batch 39 | Loss: 0.5837560892105103\n",
      "Epoch 56 | Batch 40 | Loss: 0.519411027431488\n",
      "Epoch 56 | Batch 41 | Loss: 0.0971568375825882\n",
      "Epoch 56 | Batch 42 | Loss: 0.38651493191719055\n",
      "Epoch 56 | Batch 43 | Loss: 0.6237250566482544\n",
      "Epoch 56 | Batch 44 | Loss: 0.31982624530792236\n",
      "Epoch 56 | Batch 45 | Loss: 0.24821776151657104\n",
      "Epoch 56 | Batch 46 | Loss: 0.5558881759643555\n",
      "Epoch 56 | Batch 47 | Loss: 0.2235347479581833\n",
      "Epoch 56 | Batch 48 | Loss: 0.20594698190689087\n",
      "Epoch 56 | Batch 49 | Loss: 0.4204961061477661\n",
      "Epoch 56 | Batch 50 | Loss: 0.27136683464050293\n",
      "Epoch 56 | Batch 51 | Loss: 0.3049386739730835\n",
      "Epoch 56 | Batch 52 | Loss: 0.12392136454582214\n",
      "Epoch 56 | Batch 53 | Loss: 0.46173611283302307\n",
      "Epoch 56 | Batch 54 | Loss: 0.6645923852920532\n",
      "Epoch 56 | Batch 55 | Loss: 0.4925927519798279\n",
      "Epoch 56 | Batch 56 | Loss: 0.25535473227500916\n",
      "Epoch 56 | Batch 57 | Loss: 0.7985776662826538\n",
      "Epoch 56 | Batch 58 | Loss: 0.18256936967372894\n",
      "Epoch 56 | Batch 59 | Loss: 0.32505670189857483\n",
      "Epoch 56 | Batch 60 | Loss: 0.4194656312465668\n",
      "Epoch 56 | Batch 61 | Loss: 0.37846487760543823\n",
      "Epoch 56 | Batch 62 | Loss: 0.2718445360660553\n",
      "Epoch 56 | Batch 63 | Loss: 0.21983174979686737\n",
      "Epoch 56 | Batch 64 | Loss: 0.10577942430973053\n",
      "Epoch 56 | Batch 65 | Loss: 0.19664114713668823\n",
      "Epoch 56 | Batch 66 | Loss: 0.11568371951580048\n",
      "Epoch 56 | Batch 67 | Loss: 0.3816103935241699\n",
      "Epoch 56 | Batch 68 | Loss: 0.20631137490272522\n",
      "Epoch 56 | Batch 69 | Loss: 0.24188260734081268\n",
      "Epoch 56 | Batch 70 | Loss: 0.35968390107154846\n",
      "Epoch 56 | Batch 71 | Loss: 0.6613660454750061\n",
      "Epoch 56 | Batch 72 | Loss: 0.5106784701347351\n",
      "Epoch 56 | Batch 73 | Loss: 0.3370401859283447\n",
      "Epoch 56 | Batch 74 | Loss: 0.2932484447956085\n",
      "Epoch 56 | Batch 75 | Loss: 0.5462136268615723\n",
      "Epoch 56 | Batch 76 | Loss: 0.22996613383293152\n",
      "Epoch 56 | Batch 77 | Loss: 0.10224557667970657\n",
      "Epoch 56 | Batch 78 | Loss: 0.29685652256011963\n",
      "Epoch 56 | Batch 79 | Loss: 0.32392749190330505\n",
      "Epoch 56 | Batch 80 | Loss: 0.21012303233146667\n",
      "Epoch 56 | Batch 81 | Loss: 0.3951677680015564\n",
      "Epoch 56 | Batch 82 | Loss: 0.1562371701002121\n",
      "Epoch 56 | Batch 83 | Loss: 0.487499475479126\n",
      "Epoch 56 | Batch 84 | Loss: 0.38398998975753784\n",
      "Epoch 56 | Batch 85 | Loss: 0.24757565557956696\n",
      "Epoch 56 | Batch 86 | Loss: 0.4157416522502899\n",
      "Epoch 56 | Batch 87 | Loss: 0.2722684442996979\n",
      "Epoch 56 | Batch 88 | Loss: 0.3322529196739197\n",
      "Epoch 56 | Batch 89 | Loss: 0.3550780713558197\n",
      "Epoch 56 | Batch 90 | Loss: 0.026260286569595337\n",
      "Epoch 57 | Batch 1 | Loss: 0.26330775022506714\n",
      "Epoch 57 | Batch 2 | Loss: 0.3973541855812073\n",
      "Epoch 57 | Batch 3 | Loss: 0.12390640377998352\n",
      "Epoch 57 | Batch 4 | Loss: 0.5663199424743652\n",
      "Epoch 57 | Batch 5 | Loss: 0.23435387015342712\n",
      "Epoch 57 | Batch 6 | Loss: 0.17425033450126648\n",
      "Epoch 57 | Batch 7 | Loss: 0.3055856227874756\n",
      "Epoch 57 | Batch 8 | Loss: 0.20885735750198364\n",
      "Epoch 57 | Batch 9 | Loss: 0.3457302451133728\n",
      "Epoch 57 | Batch 10 | Loss: 0.4369978904724121\n",
      "Epoch 57 | Batch 11 | Loss: 0.5164647102355957\n",
      "Epoch 57 | Batch 12 | Loss: 0.4771941006183624\n",
      "Epoch 57 | Batch 13 | Loss: 0.10086233913898468\n",
      "Epoch 57 | Batch 14 | Loss: 0.10086965560913086\n",
      "Epoch 57 | Batch 15 | Loss: 0.10801071673631668\n",
      "Epoch 57 | Batch 16 | Loss: 0.09628146141767502\n",
      "Epoch 57 | Batch 17 | Loss: 0.30405285954475403\n",
      "Epoch 57 | Batch 18 | Loss: 1.0891778469085693\n",
      "Epoch 57 | Batch 19 | Loss: 0.915508508682251\n",
      "Epoch 57 | Batch 20 | Loss: 0.3823135793209076\n",
      "Epoch 57 | Batch 21 | Loss: 0.5696523189544678\n",
      "Epoch 57 | Batch 22 | Loss: 0.1487089842557907\n",
      "Epoch 57 | Batch 23 | Loss: 0.48835843801498413\n",
      "Epoch 57 | Batch 24 | Loss: 0.5421119928359985\n",
      "Epoch 57 | Batch 25 | Loss: 0.2754864990711212\n",
      "Epoch 57 | Batch 26 | Loss: 0.1172778457403183\n",
      "Epoch 57 | Batch 27 | Loss: 0.4037720561027527\n",
      "Epoch 57 | Batch 28 | Loss: 0.17279405891895294\n",
      "Epoch 57 | Batch 29 | Loss: 0.1697235256433487\n",
      "Epoch 57 | Batch 30 | Loss: 0.7177220582962036\n",
      "Epoch 57 | Batch 31 | Loss: 0.7038853764533997\n",
      "Epoch 57 | Batch 32 | Loss: 0.08569927513599396\n",
      "Epoch 57 | Batch 33 | Loss: 0.26866254210472107\n",
      "Epoch 57 | Batch 34 | Loss: 0.49915337562561035\n",
      "Epoch 57 | Batch 35 | Loss: 0.23153969645500183\n",
      "Epoch 57 | Batch 36 | Loss: 0.10022406280040741\n",
      "Epoch 57 | Batch 37 | Loss: 0.22687050700187683\n",
      "Epoch 57 | Batch 38 | Loss: 0.2897637188434601\n",
      "Epoch 57 | Batch 39 | Loss: 0.3090823292732239\n",
      "Epoch 57 | Batch 40 | Loss: 0.495361864566803\n",
      "Epoch 57 | Batch 41 | Loss: 0.40149328112602234\n",
      "Epoch 57 | Batch 42 | Loss: 0.41680237650871277\n",
      "Epoch 57 | Batch 43 | Loss: 0.4969864785671234\n",
      "Epoch 57 | Batch 44 | Loss: 0.40107545256614685\n",
      "Epoch 57 | Batch 45 | Loss: 0.8180237412452698\n",
      "Epoch 57 | Batch 46 | Loss: 1.2284862995147705\n",
      "Epoch 57 | Batch 47 | Loss: 0.40039780735969543\n",
      "Epoch 57 | Batch 48 | Loss: 0.35448113083839417\n",
      "Epoch 57 | Batch 49 | Loss: 0.20881536602973938\n",
      "Epoch 57 | Batch 50 | Loss: 0.48763754963874817\n",
      "Epoch 57 | Batch 51 | Loss: 0.2193070501089096\n",
      "Epoch 57 | Batch 52 | Loss: 0.5144606828689575\n",
      "Epoch 57 | Batch 53 | Loss: 0.3955489993095398\n",
      "Epoch 57 | Batch 54 | Loss: 0.06758749485015869\n",
      "Epoch 57 | Batch 55 | Loss: 0.11368882656097412\n",
      "Epoch 57 | Batch 56 | Loss: 0.1560979187488556\n",
      "Epoch 57 | Batch 57 | Loss: 0.07408562302589417\n",
      "Epoch 57 | Batch 58 | Loss: 0.29854056239128113\n",
      "Epoch 57 | Batch 59 | Loss: 0.4595143496990204\n",
      "Epoch 57 | Batch 60 | Loss: 0.48450207710266113\n",
      "Epoch 57 | Batch 61 | Loss: 0.24103838205337524\n",
      "Epoch 57 | Batch 62 | Loss: 0.399077445268631\n",
      "Epoch 57 | Batch 63 | Loss: 0.18712860345840454\n",
      "Epoch 57 | Batch 64 | Loss: 0.6404397487640381\n",
      "Epoch 57 | Batch 65 | Loss: 0.6696481704711914\n",
      "Epoch 57 | Batch 66 | Loss: 0.5109849572181702\n",
      "Epoch 57 | Batch 67 | Loss: 0.2216314673423767\n",
      "Epoch 57 | Batch 68 | Loss: 0.2703295350074768\n",
      "Epoch 57 | Batch 69 | Loss: 0.34002694487571716\n",
      "Epoch 57 | Batch 70 | Loss: 0.35424190759658813\n",
      "Epoch 57 | Batch 71 | Loss: 0.3514789640903473\n",
      "Epoch 57 | Batch 72 | Loss: 0.3877672255039215\n",
      "Epoch 57 | Batch 73 | Loss: 0.25418734550476074\n",
      "Epoch 57 | Batch 74 | Loss: 0.09122098982334137\n",
      "Epoch 57 | Batch 75 | Loss: 0.6187563538551331\n",
      "Epoch 57 | Batch 76 | Loss: 0.5094457864761353\n",
      "Epoch 57 | Batch 77 | Loss: 0.31618204712867737\n",
      "Epoch 57 | Batch 78 | Loss: 0.5157508254051208\n",
      "Epoch 57 | Batch 79 | Loss: 0.40477854013442993\n",
      "Epoch 57 | Batch 80 | Loss: 0.5691801309585571\n",
      "Epoch 57 | Batch 81 | Loss: 0.18008127808570862\n",
      "Epoch 57 | Batch 82 | Loss: 0.2946632206439972\n",
      "Epoch 57 | Batch 83 | Loss: 0.32537245750427246\n",
      "Epoch 57 | Batch 84 | Loss: 0.4564335346221924\n",
      "Epoch 57 | Batch 85 | Loss: 0.705832302570343\n",
      "Epoch 57 | Batch 86 | Loss: 0.10513544827699661\n",
      "Epoch 57 | Batch 87 | Loss: 0.17160487174987793\n",
      "Epoch 57 | Batch 88 | Loss: 0.4151844382286072\n",
      "Epoch 57 | Batch 89 | Loss: 0.4021154046058655\n",
      "Epoch 57 | Batch 90 | Loss: 0.15288227796554565\n",
      "Epoch 58 | Batch 1 | Loss: 0.3155501186847687\n",
      "Epoch 58 | Batch 2 | Loss: 0.35139885544776917\n",
      "Epoch 58 | Batch 3 | Loss: 0.29774293303489685\n",
      "Epoch 58 | Batch 4 | Loss: 0.28442442417144775\n",
      "Epoch 58 | Batch 5 | Loss: 0.23799481987953186\n",
      "Epoch 58 | Batch 6 | Loss: 0.3418731093406677\n",
      "Epoch 58 | Batch 7 | Loss: 0.4742576777935028\n",
      "Epoch 58 | Batch 8 | Loss: 0.4871562123298645\n",
      "Epoch 58 | Batch 9 | Loss: 0.3012928068637848\n",
      "Epoch 58 | Batch 10 | Loss: 0.3442249596118927\n",
      "Epoch 58 | Batch 11 | Loss: 0.10930712521076202\n",
      "Epoch 58 | Batch 12 | Loss: 0.2084951102733612\n",
      "Epoch 58 | Batch 13 | Loss: 0.2921049892902374\n",
      "Epoch 58 | Batch 14 | Loss: 0.14354830980300903\n",
      "Epoch 58 | Batch 15 | Loss: 0.0990614965558052\n",
      "Epoch 58 | Batch 16 | Loss: 0.47779032588005066\n",
      "Epoch 58 | Batch 17 | Loss: 0.704338788986206\n",
      "Epoch 58 | Batch 18 | Loss: 0.12732921540737152\n",
      "Epoch 58 | Batch 19 | Loss: 0.09179247170686722\n",
      "Epoch 58 | Batch 20 | Loss: 0.40963226556777954\n",
      "Epoch 58 | Batch 21 | Loss: 0.5050039887428284\n",
      "Epoch 58 | Batch 22 | Loss: 0.29050230979919434\n",
      "Epoch 58 | Batch 23 | Loss: 0.22199836373329163\n",
      "Epoch 58 | Batch 24 | Loss: 0.3747116029262543\n",
      "Epoch 58 | Batch 25 | Loss: 0.3020686209201813\n",
      "Epoch 58 | Batch 26 | Loss: 0.23506790399551392\n",
      "Epoch 58 | Batch 27 | Loss: 0.34249749779701233\n",
      "Epoch 58 | Batch 28 | Loss: 0.25984179973602295\n",
      "Epoch 58 | Batch 29 | Loss: 0.6452815532684326\n",
      "Epoch 58 | Batch 30 | Loss: 0.2838592529296875\n",
      "Epoch 58 | Batch 31 | Loss: 0.46097689867019653\n",
      "Epoch 58 | Batch 32 | Loss: 0.09942711889743805\n",
      "Epoch 58 | Batch 33 | Loss: 0.443579763174057\n",
      "Epoch 58 | Batch 34 | Loss: 0.7539283037185669\n",
      "Epoch 58 | Batch 35 | Loss: 0.4331818222999573\n",
      "Epoch 58 | Batch 36 | Loss: 0.6389412879943848\n",
      "Epoch 58 | Batch 37 | Loss: 0.5120773315429688\n",
      "Epoch 58 | Batch 38 | Loss: 0.6215251088142395\n",
      "Epoch 58 | Batch 39 | Loss: 0.4786461889743805\n",
      "Epoch 58 | Batch 40 | Loss: 0.1656041443347931\n",
      "Epoch 58 | Batch 41 | Loss: 0.3910149335861206\n",
      "Epoch 58 | Batch 42 | Loss: 0.4580135941505432\n",
      "Epoch 58 | Batch 43 | Loss: 0.5473229289054871\n",
      "Epoch 58 | Batch 44 | Loss: 0.4274255037307739\n",
      "Epoch 58 | Batch 45 | Loss: 0.7233054637908936\n",
      "Epoch 58 | Batch 46 | Loss: 0.3388493061065674\n",
      "Epoch 58 | Batch 47 | Loss: 0.5422431230545044\n",
      "Epoch 58 | Batch 48 | Loss: 0.20102739334106445\n",
      "Epoch 58 | Batch 49 | Loss: 0.2625133991241455\n",
      "Epoch 58 | Batch 50 | Loss: 0.5360589027404785\n",
      "Epoch 58 | Batch 51 | Loss: 0.3557518422603607\n",
      "Epoch 58 | Batch 52 | Loss: 0.11362874507904053\n",
      "Epoch 58 | Batch 53 | Loss: 0.2965755760669708\n",
      "Epoch 58 | Batch 54 | Loss: 0.7345374822616577\n",
      "Epoch 58 | Batch 55 | Loss: 0.47857820987701416\n",
      "Epoch 58 | Batch 56 | Loss: 0.2758716642856598\n",
      "Epoch 58 | Batch 57 | Loss: 0.3239354193210602\n",
      "Epoch 58 | Batch 58 | Loss: 0.4119677245616913\n",
      "Epoch 58 | Batch 59 | Loss: 0.30624884366989136\n",
      "Epoch 58 | Batch 60 | Loss: 0.657002329826355\n",
      "Epoch 58 | Batch 61 | Loss: 0.4114134907722473\n",
      "Epoch 58 | Batch 62 | Loss: 0.33839547634124756\n",
      "Epoch 58 | Batch 63 | Loss: 0.20591206848621368\n",
      "Epoch 58 | Batch 64 | Loss: 0.3223332464694977\n",
      "Epoch 58 | Batch 65 | Loss: 0.6131141185760498\n",
      "Epoch 58 | Batch 66 | Loss: 0.34485164284706116\n",
      "Epoch 58 | Batch 67 | Loss: 0.3552270233631134\n",
      "Epoch 58 | Batch 68 | Loss: 0.048088669776916504\n",
      "Epoch 58 | Batch 69 | Loss: 0.4850636124610901\n",
      "Epoch 58 | Batch 70 | Loss: 0.2658880352973938\n",
      "Epoch 58 | Batch 71 | Loss: 0.5543529987335205\n",
      "Epoch 58 | Batch 72 | Loss: 0.7113997936248779\n",
      "Epoch 58 | Batch 73 | Loss: 0.3669079542160034\n",
      "Epoch 58 | Batch 74 | Loss: 0.09501668810844421\n",
      "Epoch 58 | Batch 75 | Loss: 0.17122481763362885\n",
      "Epoch 58 | Batch 76 | Loss: 0.6541297435760498\n",
      "Epoch 58 | Batch 77 | Loss: 0.26294976472854614\n",
      "Epoch 58 | Batch 78 | Loss: 0.2371053844690323\n",
      "Epoch 58 | Batch 79 | Loss: 0.15069003403186798\n",
      "Epoch 58 | Batch 80 | Loss: 0.2824079990386963\n",
      "Epoch 58 | Batch 81 | Loss: 0.1918424665927887\n",
      "Epoch 58 | Batch 82 | Loss: 0.7829724550247192\n",
      "Epoch 58 | Batch 83 | Loss: 0.24407503008842468\n",
      "Epoch 58 | Batch 84 | Loss: 0.42543837428092957\n",
      "Epoch 58 | Batch 85 | Loss: 0.23626983165740967\n",
      "Epoch 58 | Batch 86 | Loss: 0.37236225605010986\n",
      "Epoch 58 | Batch 87 | Loss: 0.5199491381645203\n",
      "Epoch 58 | Batch 88 | Loss: 0.37777045369148254\n",
      "Epoch 58 | Batch 89 | Loss: 0.20011302828788757\n",
      "Epoch 58 | Batch 90 | Loss: 0.7918097376823425\n",
      "Epoch 59 | Batch 1 | Loss: 0.2517498731613159\n",
      "Epoch 59 | Batch 2 | Loss: 0.20908989012241364\n",
      "Epoch 59 | Batch 3 | Loss: 0.38951367139816284\n",
      "Epoch 59 | Batch 4 | Loss: 0.20419956743717194\n",
      "Epoch 59 | Batch 5 | Loss: 0.4658474624156952\n",
      "Epoch 59 | Batch 6 | Loss: 0.5897815823554993\n",
      "Epoch 59 | Batch 7 | Loss: 0.10172773152589798\n",
      "Epoch 59 | Batch 8 | Loss: 0.29250872135162354\n",
      "Epoch 59 | Batch 9 | Loss: 0.21284502744674683\n",
      "Epoch 59 | Batch 10 | Loss: 0.3911857008934021\n",
      "Epoch 59 | Batch 11 | Loss: 0.2190520167350769\n",
      "Epoch 59 | Batch 12 | Loss: 0.7110742330551147\n",
      "Epoch 59 | Batch 13 | Loss: 0.3728935420513153\n",
      "Epoch 59 | Batch 14 | Loss: 0.21045519411563873\n",
      "Epoch 59 | Batch 15 | Loss: 0.5168864727020264\n",
      "Epoch 59 | Batch 16 | Loss: 0.6232288479804993\n",
      "Epoch 59 | Batch 17 | Loss: 0.1874481737613678\n",
      "Epoch 59 | Batch 18 | Loss: 0.3735719919204712\n",
      "Epoch 59 | Batch 19 | Loss: 0.16191069781780243\n",
      "Epoch 59 | Batch 20 | Loss: 0.663390040397644\n",
      "Epoch 59 | Batch 21 | Loss: 0.40533697605133057\n",
      "Epoch 59 | Batch 22 | Loss: 0.09754770249128342\n",
      "Epoch 59 | Batch 23 | Loss: 0.14834749698638916\n",
      "Epoch 59 | Batch 24 | Loss: 0.5589824914932251\n",
      "Epoch 59 | Batch 25 | Loss: 0.3894778788089752\n",
      "Epoch 59 | Batch 26 | Loss: 0.43149590492248535\n",
      "Epoch 59 | Batch 27 | Loss: 0.307030588388443\n",
      "Epoch 59 | Batch 28 | Loss: 0.38951319456100464\n",
      "Epoch 59 | Batch 29 | Loss: 0.8235454559326172\n",
      "Epoch 59 | Batch 30 | Loss: 0.2037055492401123\n",
      "Epoch 59 | Batch 31 | Loss: 0.3987567722797394\n",
      "Epoch 59 | Batch 32 | Loss: 0.33582910895347595\n",
      "Epoch 59 | Batch 33 | Loss: 0.4993421137332916\n",
      "Epoch 59 | Batch 34 | Loss: 0.5936028361320496\n",
      "Epoch 59 | Batch 35 | Loss: 0.3062196969985962\n",
      "Epoch 59 | Batch 36 | Loss: 0.4759509563446045\n",
      "Epoch 59 | Batch 37 | Loss: 0.3332829475402832\n",
      "Epoch 59 | Batch 38 | Loss: 0.31352904438972473\n",
      "Epoch 59 | Batch 39 | Loss: 0.3392755389213562\n",
      "Epoch 59 | Batch 40 | Loss: 0.2310410737991333\n",
      "Epoch 59 | Batch 41 | Loss: 0.0916210487484932\n",
      "Epoch 59 | Batch 42 | Loss: 0.19238904118537903\n",
      "Epoch 59 | Batch 43 | Loss: 0.37794068455696106\n",
      "Epoch 59 | Batch 44 | Loss: 0.5861322283744812\n",
      "Epoch 59 | Batch 45 | Loss: 0.6906294226646423\n",
      "Epoch 59 | Batch 46 | Loss: 0.10065768659114838\n",
      "Epoch 59 | Batch 47 | Loss: 0.7133405208587646\n",
      "Epoch 59 | Batch 48 | Loss: 0.49538886547088623\n",
      "Epoch 59 | Batch 49 | Loss: 0.23137043416500092\n",
      "Epoch 59 | Batch 50 | Loss: 0.4066932499408722\n",
      "Epoch 59 | Batch 51 | Loss: 0.48975473642349243\n",
      "Epoch 59 | Batch 52 | Loss: 0.0835794135928154\n",
      "Epoch 59 | Batch 53 | Loss: 0.44998493790626526\n",
      "Epoch 59 | Batch 54 | Loss: 0.40838226675987244\n",
      "Epoch 59 | Batch 55 | Loss: 0.372597873210907\n",
      "Epoch 59 | Batch 56 | Loss: 0.7138912677764893\n",
      "Epoch 59 | Batch 57 | Loss: 0.5114833116531372\n",
      "Epoch 59 | Batch 58 | Loss: 0.18349020183086395\n",
      "Epoch 59 | Batch 59 | Loss: 0.22251100838184357\n",
      "Epoch 59 | Batch 60 | Loss: 0.1363341063261032\n",
      "Epoch 59 | Batch 61 | Loss: 0.5216244459152222\n",
      "Epoch 59 | Batch 62 | Loss: 0.13655026257038116\n",
      "Epoch 59 | Batch 63 | Loss: 0.13934609293937683\n",
      "Epoch 59 | Batch 64 | Loss: 0.6869775652885437\n",
      "Epoch 59 | Batch 65 | Loss: 0.7169683575630188\n",
      "Epoch 59 | Batch 66 | Loss: 0.5424680113792419\n",
      "Epoch 59 | Batch 67 | Loss: 0.49085795879364014\n",
      "Epoch 59 | Batch 68 | Loss: 0.14515113830566406\n",
      "Epoch 59 | Batch 69 | Loss: 0.5036640167236328\n",
      "Epoch 59 | Batch 70 | Loss: 0.3562920093536377\n",
      "Epoch 59 | Batch 71 | Loss: 0.2962567210197449\n",
      "Epoch 59 | Batch 72 | Loss: 0.341465026140213\n",
      "Epoch 59 | Batch 73 | Loss: 0.4859391152858734\n",
      "Epoch 59 | Batch 74 | Loss: 0.4302619397640228\n",
      "Epoch 59 | Batch 75 | Loss: 0.2200528383255005\n",
      "Epoch 59 | Batch 76 | Loss: 0.36316078901290894\n",
      "Epoch 59 | Batch 77 | Loss: 0.40746816992759705\n",
      "Epoch 59 | Batch 78 | Loss: 0.5777355432510376\n",
      "Epoch 59 | Batch 79 | Loss: 0.6897643804550171\n",
      "Epoch 59 | Batch 80 | Loss: 0.22244232892990112\n",
      "Epoch 59 | Batch 81 | Loss: 0.5968683958053589\n",
      "Epoch 59 | Batch 82 | Loss: 0.36964917182922363\n",
      "Epoch 59 | Batch 83 | Loss: 0.1543484628200531\n",
      "Epoch 59 | Batch 84 | Loss: 0.48766255378723145\n",
      "Epoch 59 | Batch 85 | Loss: 0.14823563396930695\n",
      "Epoch 59 | Batch 86 | Loss: 0.2623169720172882\n",
      "Epoch 59 | Batch 87 | Loss: 0.3101045787334442\n",
      "Epoch 59 | Batch 88 | Loss: 0.2468910962343216\n",
      "Epoch 59 | Batch 89 | Loss: 0.21682313084602356\n",
      "Epoch 59 | Batch 90 | Loss: 0.005208143033087254\n",
      "Epoch 60 | Batch 1 | Loss: 0.4390295445919037\n",
      "Epoch 60 | Batch 2 | Loss: 0.39830490946769714\n",
      "Epoch 60 | Batch 3 | Loss: 0.21869125962257385\n",
      "Epoch 60 | Batch 4 | Loss: 0.13622628152370453\n",
      "Epoch 60 | Batch 5 | Loss: 0.2523980736732483\n",
      "Epoch 60 | Batch 6 | Loss: 0.275358110666275\n",
      "Epoch 60 | Batch 7 | Loss: 0.16195032000541687\n",
      "Epoch 60 | Batch 8 | Loss: 0.675557017326355\n",
      "Epoch 60 | Batch 9 | Loss: 0.790708065032959\n",
      "Epoch 60 | Batch 10 | Loss: 0.5076426267623901\n",
      "Epoch 60 | Batch 11 | Loss: 0.4989687502384186\n",
      "Epoch 60 | Batch 12 | Loss: 0.46156197786331177\n",
      "Epoch 60 | Batch 13 | Loss: 0.19777345657348633\n",
      "Epoch 60 | Batch 14 | Loss: 0.591553807258606\n",
      "Epoch 60 | Batch 15 | Loss: 0.5418046712875366\n",
      "Epoch 60 | Batch 16 | Loss: 0.2681463360786438\n",
      "Epoch 60 | Batch 17 | Loss: 0.18934442102909088\n",
      "Epoch 60 | Batch 18 | Loss: 0.3558255434036255\n",
      "Epoch 60 | Batch 19 | Loss: 0.3674548864364624\n",
      "Epoch 60 | Batch 20 | Loss: 0.39025554060935974\n",
      "Epoch 60 | Batch 21 | Loss: 0.19825944304466248\n",
      "Epoch 60 | Batch 22 | Loss: 0.20553576946258545\n",
      "Epoch 60 | Batch 23 | Loss: 0.3681175708770752\n",
      "Epoch 60 | Batch 24 | Loss: 0.33092787861824036\n",
      "Epoch 60 | Batch 25 | Loss: 0.39659053087234497\n",
      "Epoch 60 | Batch 26 | Loss: 0.34800830483436584\n",
      "Epoch 60 | Batch 27 | Loss: 0.12291695177555084\n",
      "Epoch 60 | Batch 28 | Loss: 0.2528435289859772\n",
      "Epoch 60 | Batch 29 | Loss: 0.22219343483448029\n",
      "Epoch 60 | Batch 30 | Loss: 0.1543084681034088\n",
      "Epoch 60 | Batch 31 | Loss: 0.4314073324203491\n",
      "Epoch 60 | Batch 32 | Loss: 0.3779265582561493\n",
      "Epoch 60 | Batch 33 | Loss: 0.1085529699921608\n",
      "Epoch 60 | Batch 34 | Loss: 0.2660914361476898\n",
      "Epoch 60 | Batch 35 | Loss: 0.41590648889541626\n",
      "Epoch 60 | Batch 36 | Loss: 0.5554662346839905\n",
      "Epoch 60 | Batch 37 | Loss: 0.6555777192115784\n",
      "Epoch 60 | Batch 38 | Loss: 0.24491968750953674\n",
      "Epoch 60 | Batch 39 | Loss: 0.39540794491767883\n",
      "Epoch 60 | Batch 40 | Loss: 0.0910692811012268\n",
      "Epoch 60 | Batch 41 | Loss: 0.4268334209918976\n",
      "Epoch 60 | Batch 42 | Loss: 0.449272096157074\n",
      "Epoch 60 | Batch 43 | Loss: 0.10470526665449142\n",
      "Epoch 60 | Batch 44 | Loss: 0.12584596872329712\n",
      "Epoch 60 | Batch 45 | Loss: 1.1341676712036133\n",
      "Epoch 60 | Batch 46 | Loss: 0.45426687598228455\n",
      "Epoch 60 | Batch 47 | Loss: 0.15597456693649292\n",
      "Epoch 60 | Batch 48 | Loss: 0.36860477924346924\n",
      "Epoch 60 | Batch 49 | Loss: 0.20588985085487366\n",
      "Epoch 60 | Batch 50 | Loss: 0.6330733299255371\n",
      "Epoch 60 | Batch 51 | Loss: 0.5439077615737915\n",
      "Epoch 60 | Batch 52 | Loss: 0.39872050285339355\n",
      "Epoch 60 | Batch 53 | Loss: 0.3707100749015808\n",
      "Epoch 60 | Batch 54 | Loss: 1.0451951026916504\n",
      "Epoch 60 | Batch 55 | Loss: 0.29646971821784973\n",
      "Epoch 60 | Batch 56 | Loss: 0.10588687658309937\n",
      "Epoch 60 | Batch 57 | Loss: 0.4831061363220215\n",
      "Epoch 60 | Batch 58 | Loss: 0.20008531212806702\n",
      "Epoch 60 | Batch 59 | Loss: 0.20764456689357758\n",
      "Epoch 60 | Batch 60 | Loss: 0.46230775117874146\n",
      "Epoch 60 | Batch 61 | Loss: 0.19218890368938446\n",
      "Epoch 60 | Batch 62 | Loss: 0.2943979501724243\n",
      "Epoch 60 | Batch 63 | Loss: 0.7284496426582336\n",
      "Epoch 60 | Batch 64 | Loss: 0.6780816912651062\n",
      "Epoch 60 | Batch 65 | Loss: 0.6487425565719604\n",
      "Epoch 60 | Batch 66 | Loss: 0.26668983697891235\n",
      "Epoch 60 | Batch 67 | Loss: 0.39212802052497864\n",
      "Epoch 60 | Batch 68 | Loss: 0.16132895648479462\n",
      "Epoch 60 | Batch 69 | Loss: 0.2716841399669647\n",
      "Epoch 60 | Batch 70 | Loss: 0.44077932834625244\n",
      "Epoch 60 | Batch 71 | Loss: 0.4851108193397522\n",
      "Epoch 60 | Batch 72 | Loss: 0.42520907521247864\n",
      "Epoch 60 | Batch 73 | Loss: 0.3644271492958069\n",
      "Epoch 60 | Batch 74 | Loss: 0.22847937047481537\n",
      "Epoch 60 | Batch 75 | Loss: 0.20468953251838684\n",
      "Epoch 60 | Batch 76 | Loss: 0.4836667776107788\n",
      "Epoch 60 | Batch 77 | Loss: 0.6547545194625854\n",
      "Epoch 60 | Batch 78 | Loss: 0.20828020572662354\n",
      "Epoch 60 | Batch 79 | Loss: 0.3369550108909607\n",
      "Epoch 60 | Batch 80 | Loss: 0.2235628217458725\n",
      "Epoch 60 | Batch 81 | Loss: 0.08232786506414413\n",
      "Epoch 60 | Batch 82 | Loss: 0.27612537145614624\n",
      "Epoch 60 | Batch 83 | Loss: 0.34761613607406616\n",
      "Epoch 60 | Batch 84 | Loss: 0.2392684519290924\n",
      "Epoch 60 | Batch 85 | Loss: 0.2177165150642395\n",
      "Epoch 60 | Batch 86 | Loss: 0.8781088590621948\n",
      "Epoch 60 | Batch 87 | Loss: 0.5908643007278442\n",
      "Epoch 60 | Batch 88 | Loss: 0.22115488350391388\n",
      "Epoch 60 | Batch 89 | Loss: 0.2766236662864685\n",
      "Epoch 60 | Batch 90 | Loss: 1.2421364784240723\n",
      "Epoch 61 | Batch 1 | Loss: 0.7199792861938477\n",
      "Epoch 61 | Batch 2 | Loss: 0.29564836621284485\n",
      "Epoch 61 | Batch 3 | Loss: 0.3352776765823364\n",
      "Epoch 61 | Batch 4 | Loss: 0.42773789167404175\n",
      "Epoch 61 | Batch 5 | Loss: 0.4806184768676758\n",
      "Epoch 61 | Batch 6 | Loss: 0.484029620885849\n",
      "Epoch 61 | Batch 7 | Loss: 0.3333674371242523\n",
      "Epoch 61 | Batch 8 | Loss: 0.3141677975654602\n",
      "Epoch 61 | Batch 9 | Loss: 0.36728018522262573\n",
      "Epoch 61 | Batch 10 | Loss: 0.22638307511806488\n",
      "Epoch 61 | Batch 11 | Loss: 0.7418075203895569\n",
      "Epoch 61 | Batch 12 | Loss: 0.3272212743759155\n",
      "Epoch 61 | Batch 13 | Loss: 0.22266866266727448\n",
      "Epoch 61 | Batch 14 | Loss: 0.5598090291023254\n",
      "Epoch 61 | Batch 15 | Loss: 0.3771621882915497\n",
      "Epoch 61 | Batch 16 | Loss: 0.1633041799068451\n",
      "Epoch 61 | Batch 17 | Loss: 0.26672595739364624\n",
      "Epoch 61 | Batch 18 | Loss: 0.3112008571624756\n",
      "Epoch 61 | Batch 19 | Loss: 0.562656581401825\n",
      "Epoch 61 | Batch 20 | Loss: 0.48941922187805176\n",
      "Epoch 61 | Batch 21 | Loss: 0.0990387573838234\n",
      "Epoch 61 | Batch 22 | Loss: 0.5179722309112549\n",
      "Epoch 61 | Batch 23 | Loss: 0.3252786099910736\n",
      "Epoch 61 | Batch 24 | Loss: 0.18181638419628143\n",
      "Epoch 61 | Batch 25 | Loss: 0.20186054706573486\n",
      "Epoch 61 | Batch 26 | Loss: 0.351588636636734\n",
      "Epoch 61 | Batch 27 | Loss: 0.7374556660652161\n",
      "Epoch 61 | Batch 28 | Loss: 0.10944971442222595\n",
      "Epoch 61 | Batch 29 | Loss: 0.27744024991989136\n",
      "Epoch 61 | Batch 30 | Loss: 0.46597820520401\n",
      "Epoch 61 | Batch 31 | Loss: 0.0918782502412796\n",
      "Epoch 61 | Batch 32 | Loss: 0.7569819092750549\n",
      "Epoch 61 | Batch 33 | Loss: 0.8875316381454468\n",
      "Epoch 61 | Batch 34 | Loss: 0.6271108984947205\n",
      "Epoch 61 | Batch 35 | Loss: 0.557263970375061\n",
      "Epoch 61 | Batch 36 | Loss: 0.4450365900993347\n",
      "Epoch 61 | Batch 37 | Loss: 0.5036400556564331\n",
      "Epoch 61 | Batch 38 | Loss: 0.1451730728149414\n",
      "Epoch 61 | Batch 39 | Loss: 0.8937914371490479\n",
      "Epoch 61 | Batch 40 | Loss: 0.222922220826149\n",
      "Epoch 61 | Batch 41 | Loss: 0.31826460361480713\n",
      "Epoch 61 | Batch 42 | Loss: 0.33545905351638794\n",
      "Epoch 61 | Batch 43 | Loss: 0.2807626724243164\n",
      "Epoch 61 | Batch 44 | Loss: 0.16461536288261414\n",
      "Epoch 61 | Batch 45 | Loss: 0.3385026752948761\n",
      "Epoch 61 | Batch 46 | Loss: 0.22608673572540283\n",
      "Epoch 61 | Batch 47 | Loss: 0.580691397190094\n",
      "Epoch 61 | Batch 48 | Loss: 0.3735462427139282\n",
      "Epoch 61 | Batch 49 | Loss: 0.34527587890625\n",
      "Epoch 61 | Batch 50 | Loss: 0.1661832332611084\n",
      "Epoch 61 | Batch 51 | Loss: 0.37279725074768066\n",
      "Epoch 61 | Batch 52 | Loss: 0.1336803436279297\n",
      "Epoch 61 | Batch 53 | Loss: 0.6204355955123901\n",
      "Epoch 61 | Batch 54 | Loss: 0.32445240020751953\n",
      "Epoch 61 | Batch 55 | Loss: 0.14190448820590973\n",
      "Epoch 61 | Batch 56 | Loss: 0.4787024259567261\n",
      "Epoch 61 | Batch 57 | Loss: 0.46829840540885925\n",
      "Epoch 61 | Batch 58 | Loss: 0.5261390805244446\n",
      "Epoch 61 | Batch 59 | Loss: 0.5468493103981018\n",
      "Epoch 61 | Batch 60 | Loss: 0.475647896528244\n",
      "Epoch 61 | Batch 61 | Loss: 0.38284042477607727\n",
      "Epoch 61 | Batch 62 | Loss: 0.5387823581695557\n",
      "Epoch 61 | Batch 63 | Loss: 0.0629553496837616\n",
      "Epoch 61 | Batch 64 | Loss: 0.22949983179569244\n",
      "Epoch 61 | Batch 65 | Loss: 0.4836689233779907\n",
      "Epoch 61 | Batch 66 | Loss: 0.33524370193481445\n",
      "Epoch 61 | Batch 67 | Loss: 0.2232687771320343\n",
      "Epoch 61 | Batch 68 | Loss: 0.5499354600906372\n",
      "Epoch 61 | Batch 69 | Loss: 0.27978721261024475\n",
      "Epoch 61 | Batch 70 | Loss: 0.35801538825035095\n",
      "Epoch 61 | Batch 71 | Loss: 0.22845113277435303\n",
      "Epoch 61 | Batch 72 | Loss: 0.3702245056629181\n",
      "Epoch 61 | Batch 73 | Loss: 0.21526919305324554\n",
      "Epoch 61 | Batch 74 | Loss: 0.25565969944000244\n",
      "Epoch 61 | Batch 75 | Loss: 0.2618557810783386\n",
      "Epoch 61 | Batch 76 | Loss: 0.3131696879863739\n",
      "Epoch 61 | Batch 77 | Loss: 0.3232194483280182\n",
      "Epoch 61 | Batch 78 | Loss: 0.36609137058258057\n",
      "Epoch 61 | Batch 79 | Loss: 0.27178114652633667\n",
      "Epoch 61 | Batch 80 | Loss: 0.6629759669303894\n",
      "Epoch 61 | Batch 81 | Loss: 0.46378612518310547\n",
      "Epoch 61 | Batch 82 | Loss: 0.4164377748966217\n",
      "Epoch 61 | Batch 83 | Loss: 0.4949934482574463\n",
      "Epoch 61 | Batch 84 | Loss: 0.4075804352760315\n",
      "Epoch 61 | Batch 85 | Loss: 0.22632993757724762\n",
      "Epoch 61 | Batch 86 | Loss: 0.19409368932247162\n",
      "Epoch 61 | Batch 87 | Loss: 0.172468364238739\n",
      "Epoch 61 | Batch 88 | Loss: 0.4584108591079712\n",
      "Epoch 61 | Batch 89 | Loss: 0.14136482775211334\n",
      "Epoch 61 | Batch 90 | Loss: 0.012415873818099499\n",
      "Epoch 62 | Batch 1 | Loss: 0.3711169958114624\n",
      "Epoch 62 | Batch 2 | Loss: 0.17996424436569214\n",
      "Epoch 62 | Batch 3 | Loss: 0.2709270417690277\n",
      "Epoch 62 | Batch 4 | Loss: 0.5854307413101196\n",
      "Epoch 62 | Batch 5 | Loss: 0.25689244270324707\n",
      "Epoch 62 | Batch 6 | Loss: 0.8083012104034424\n",
      "Epoch 62 | Batch 7 | Loss: 0.40500181913375854\n",
      "Epoch 62 | Batch 8 | Loss: 0.3616742789745331\n",
      "Epoch 62 | Batch 9 | Loss: 0.22423087060451508\n",
      "Epoch 62 | Batch 10 | Loss: 0.12370315194129944\n",
      "Epoch 62 | Batch 11 | Loss: 0.25721922516822815\n",
      "Epoch 62 | Batch 12 | Loss: 0.5164175033569336\n",
      "Epoch 62 | Batch 13 | Loss: 0.279034286737442\n",
      "Epoch 62 | Batch 14 | Loss: 0.5705848932266235\n",
      "Epoch 62 | Batch 15 | Loss: 0.05689740553498268\n",
      "Epoch 62 | Batch 16 | Loss: 0.22406312823295593\n",
      "Epoch 62 | Batch 17 | Loss: 0.14767345786094666\n",
      "Epoch 62 | Batch 18 | Loss: 0.19503992795944214\n",
      "Epoch 62 | Batch 19 | Loss: 0.3197195827960968\n",
      "Epoch 62 | Batch 20 | Loss: 0.28085702657699585\n",
      "Epoch 62 | Batch 21 | Loss: 0.9819785952568054\n",
      "Epoch 62 | Batch 22 | Loss: 0.33420705795288086\n",
      "Epoch 62 | Batch 23 | Loss: 0.7372153401374817\n",
      "Epoch 62 | Batch 24 | Loss: 0.10558284819126129\n",
      "Epoch 62 | Batch 25 | Loss: 0.3050119876861572\n",
      "Epoch 62 | Batch 26 | Loss: 0.15849363803863525\n",
      "Epoch 62 | Batch 27 | Loss: 0.9557090997695923\n",
      "Epoch 62 | Batch 28 | Loss: 0.5510067939758301\n",
      "Epoch 62 | Batch 29 | Loss: 1.2228353023529053\n",
      "Epoch 62 | Batch 30 | Loss: 0.36687207221984863\n",
      "Epoch 62 | Batch 31 | Loss: 0.24931298196315765\n",
      "Epoch 62 | Batch 32 | Loss: 0.184417262673378\n",
      "Epoch 62 | Batch 33 | Loss: 0.4607085585594177\n",
      "Epoch 62 | Batch 34 | Loss: 0.09896497428417206\n",
      "Epoch 62 | Batch 35 | Loss: 0.2316027283668518\n",
      "Epoch 62 | Batch 36 | Loss: 0.5688658952713013\n",
      "Epoch 62 | Batch 37 | Loss: 0.4581540822982788\n",
      "Epoch 62 | Batch 38 | Loss: 0.3384336829185486\n",
      "Epoch 62 | Batch 39 | Loss: 0.3279052972793579\n",
      "Epoch 62 | Batch 40 | Loss: 0.965186357498169\n",
      "Epoch 62 | Batch 41 | Loss: 0.21353386342525482\n",
      "Epoch 62 | Batch 42 | Loss: 0.24469858407974243\n",
      "Epoch 62 | Batch 43 | Loss: 0.23947563767433167\n",
      "Epoch 62 | Batch 44 | Loss: 0.23754528164863586\n",
      "Epoch 62 | Batch 45 | Loss: 0.5821408033370972\n",
      "Epoch 62 | Batch 46 | Loss: 0.373801052570343\n",
      "Epoch 62 | Batch 47 | Loss: 0.17893153429031372\n",
      "Epoch 62 | Batch 48 | Loss: 0.1600906401872635\n",
      "Epoch 62 | Batch 49 | Loss: 0.18184871971607208\n",
      "Epoch 62 | Batch 50 | Loss: 0.20940984785556793\n",
      "Epoch 62 | Batch 51 | Loss: 0.6198771595954895\n",
      "Epoch 62 | Batch 52 | Loss: 0.3781866431236267\n",
      "Epoch 62 | Batch 53 | Loss: 0.07929342985153198\n",
      "Epoch 62 | Batch 54 | Loss: 0.5698105692863464\n",
      "Epoch 62 | Batch 55 | Loss: 0.4848315119743347\n",
      "Epoch 62 | Batch 56 | Loss: 0.4050586223602295\n",
      "Epoch 62 | Batch 57 | Loss: 0.2770620584487915\n",
      "Epoch 62 | Batch 58 | Loss: 0.1522899717092514\n",
      "Epoch 62 | Batch 59 | Loss: 0.25704264640808105\n",
      "Epoch 62 | Batch 60 | Loss: 0.28312432765960693\n",
      "Epoch 62 | Batch 61 | Loss: 0.14743079245090485\n",
      "Epoch 62 | Batch 62 | Loss: 0.1549481302499771\n",
      "Epoch 62 | Batch 63 | Loss: 0.4139639139175415\n",
      "Epoch 62 | Batch 64 | Loss: 0.6641372442245483\n",
      "Epoch 62 | Batch 65 | Loss: 0.275922954082489\n",
      "Epoch 62 | Batch 66 | Loss: 0.30447524785995483\n",
      "Epoch 62 | Batch 67 | Loss: 0.8794199228286743\n",
      "Epoch 62 | Batch 68 | Loss: 0.13246044516563416\n",
      "Epoch 62 | Batch 69 | Loss: 0.4017805755138397\n",
      "Epoch 62 | Batch 70 | Loss: 0.3501315712928772\n",
      "Epoch 62 | Batch 71 | Loss: 0.29044052958488464\n",
      "Epoch 62 | Batch 72 | Loss: 0.13598766922950745\n",
      "Epoch 62 | Batch 73 | Loss: 0.7050522565841675\n",
      "Epoch 62 | Batch 74 | Loss: 0.2115960717201233\n",
      "Epoch 62 | Batch 75 | Loss: 0.205106720328331\n",
      "Epoch 62 | Batch 76 | Loss: 0.19544272124767303\n",
      "Epoch 62 | Batch 77 | Loss: 1.0965101718902588\n",
      "Epoch 62 | Batch 78 | Loss: 0.22786912322044373\n",
      "Epoch 62 | Batch 79 | Loss: 0.14016175270080566\n",
      "Epoch 62 | Batch 80 | Loss: 0.37774914503097534\n",
      "Epoch 62 | Batch 81 | Loss: 0.29213064908981323\n",
      "Epoch 62 | Batch 82 | Loss: 0.1966208815574646\n",
      "Epoch 62 | Batch 83 | Loss: 0.22098425030708313\n",
      "Epoch 62 | Batch 84 | Loss: 0.7630707025527954\n",
      "Epoch 62 | Batch 85 | Loss: 0.18952630460262299\n",
      "Epoch 62 | Batch 86 | Loss: 0.2653220295906067\n",
      "Epoch 62 | Batch 87 | Loss: 0.28555652499198914\n",
      "Epoch 62 | Batch 88 | Loss: 0.385644793510437\n",
      "Epoch 62 | Batch 89 | Loss: 0.6577087640762329\n",
      "Epoch 62 | Batch 90 | Loss: 0.14526984095573425\n",
      "Epoch 63 | Batch 1 | Loss: 0.4580461084842682\n",
      "Epoch 63 | Batch 2 | Loss: 0.4958080053329468\n",
      "Epoch 63 | Batch 3 | Loss: 0.16965654492378235\n",
      "Epoch 63 | Batch 4 | Loss: 0.24593226611614227\n",
      "Epoch 63 | Batch 5 | Loss: 0.19418957829475403\n",
      "Epoch 63 | Batch 6 | Loss: 0.26337552070617676\n",
      "Epoch 63 | Batch 7 | Loss: 0.594678521156311\n",
      "Epoch 63 | Batch 8 | Loss: 0.4508124887943268\n",
      "Epoch 63 | Batch 9 | Loss: 0.24419942498207092\n",
      "Epoch 63 | Batch 10 | Loss: 0.10710395872592926\n",
      "Epoch 63 | Batch 11 | Loss: 0.7354111671447754\n",
      "Epoch 63 | Batch 12 | Loss: 0.4764372706413269\n",
      "Epoch 63 | Batch 13 | Loss: 0.35142645239830017\n",
      "Epoch 63 | Batch 14 | Loss: 0.24455666542053223\n",
      "Epoch 63 | Batch 15 | Loss: 0.25863397121429443\n",
      "Epoch 63 | Batch 16 | Loss: 0.22015222907066345\n",
      "Epoch 63 | Batch 17 | Loss: 0.35181570053100586\n",
      "Epoch 63 | Batch 18 | Loss: 0.2264510691165924\n",
      "Epoch 63 | Batch 19 | Loss: 0.7335577607154846\n",
      "Epoch 63 | Batch 20 | Loss: 0.23728212714195251\n",
      "Epoch 63 | Batch 21 | Loss: 0.7490622997283936\n",
      "Epoch 63 | Batch 22 | Loss: 0.5953171253204346\n",
      "Epoch 63 | Batch 23 | Loss: 0.32633182406425476\n",
      "Epoch 63 | Batch 24 | Loss: 0.39839863777160645\n",
      "Epoch 63 | Batch 25 | Loss: 0.28500795364379883\n",
      "Epoch 63 | Batch 26 | Loss: 0.34597277641296387\n",
      "Epoch 63 | Batch 27 | Loss: 0.4271045923233032\n",
      "Epoch 63 | Batch 28 | Loss: 0.5946854948997498\n",
      "Epoch 63 | Batch 29 | Loss: 0.7735649347305298\n",
      "Epoch 63 | Batch 30 | Loss: 0.5139429569244385\n",
      "Epoch 63 | Batch 31 | Loss: 0.22738021612167358\n",
      "Epoch 63 | Batch 32 | Loss: 0.23575231432914734\n",
      "Epoch 63 | Batch 33 | Loss: 0.19046521186828613\n",
      "Epoch 63 | Batch 34 | Loss: 0.39948999881744385\n",
      "Epoch 63 | Batch 35 | Loss: 0.17341060936450958\n",
      "Epoch 63 | Batch 36 | Loss: 0.30733954906463623\n",
      "Epoch 63 | Batch 37 | Loss: 0.4451948404312134\n",
      "Epoch 63 | Batch 38 | Loss: 1.0505954027175903\n",
      "Epoch 63 | Batch 39 | Loss: 0.1295023113489151\n",
      "Epoch 63 | Batch 40 | Loss: 0.4137072265148163\n",
      "Epoch 63 | Batch 41 | Loss: 0.30563801527023315\n",
      "Epoch 63 | Batch 42 | Loss: 0.4284750521183014\n",
      "Epoch 63 | Batch 43 | Loss: 0.22549405694007874\n",
      "Epoch 63 | Batch 44 | Loss: 0.40021610260009766\n",
      "Epoch 63 | Batch 45 | Loss: 0.4426664412021637\n",
      "Epoch 63 | Batch 46 | Loss: 0.29641976952552795\n",
      "Epoch 63 | Batch 47 | Loss: 0.4377976059913635\n",
      "Epoch 63 | Batch 48 | Loss: 0.2612978518009186\n",
      "Epoch 63 | Batch 49 | Loss: 0.08767210692167282\n",
      "Epoch 63 | Batch 50 | Loss: 0.25270015001296997\n",
      "Epoch 63 | Batch 51 | Loss: 0.1955060064792633\n",
      "Epoch 63 | Batch 52 | Loss: 0.08218876272439957\n",
      "Epoch 63 | Batch 53 | Loss: 0.14153817296028137\n",
      "Epoch 63 | Batch 54 | Loss: 0.35023239254951477\n",
      "Epoch 63 | Batch 55 | Loss: 0.5146430134773254\n",
      "Epoch 63 | Batch 56 | Loss: 0.3362317681312561\n",
      "Epoch 63 | Batch 57 | Loss: 0.6345527768135071\n",
      "Epoch 63 | Batch 58 | Loss: 0.2553810179233551\n",
      "Epoch 63 | Batch 59 | Loss: 0.5301128029823303\n",
      "Epoch 63 | Batch 60 | Loss: 0.9470616579055786\n",
      "Epoch 63 | Batch 61 | Loss: 0.08612856268882751\n",
      "Epoch 63 | Batch 62 | Loss: 0.30824631452560425\n",
      "Epoch 63 | Batch 63 | Loss: 0.4655292332172394\n",
      "Epoch 63 | Batch 64 | Loss: 0.19984816014766693\n",
      "Epoch 63 | Batch 65 | Loss: 0.23770341277122498\n",
      "Epoch 63 | Batch 66 | Loss: 0.18894632160663605\n",
      "Epoch 63 | Batch 67 | Loss: 0.6673257946968079\n",
      "Epoch 63 | Batch 68 | Loss: 0.5638347864151001\n",
      "Epoch 63 | Batch 69 | Loss: 0.2933763265609741\n",
      "Epoch 63 | Batch 70 | Loss: 0.1951962113380432\n",
      "Epoch 63 | Batch 71 | Loss: 0.24654655158519745\n",
      "Epoch 63 | Batch 72 | Loss: 0.550689160823822\n",
      "Epoch 63 | Batch 73 | Loss: 0.7424567937850952\n",
      "Epoch 63 | Batch 74 | Loss: 0.11327122151851654\n",
      "Epoch 63 | Batch 75 | Loss: 0.28845229744911194\n",
      "Epoch 63 | Batch 76 | Loss: 0.3121275305747986\n",
      "Epoch 63 | Batch 77 | Loss: 0.659302830696106\n",
      "Epoch 63 | Batch 78 | Loss: 0.2028968334197998\n",
      "Epoch 63 | Batch 79 | Loss: 0.17050382494926453\n",
      "Epoch 63 | Batch 80 | Loss: 0.44064125418663025\n",
      "Epoch 63 | Batch 81 | Loss: 0.3535987138748169\n",
      "Epoch 63 | Batch 82 | Loss: 0.217774897813797\n",
      "Epoch 63 | Batch 83 | Loss: 0.13653287291526794\n",
      "Epoch 63 | Batch 84 | Loss: 0.5454779267311096\n",
      "Epoch 63 | Batch 85 | Loss: 0.5258282423019409\n",
      "Epoch 63 | Batch 86 | Loss: 0.09936945140361786\n",
      "Epoch 63 | Batch 87 | Loss: 0.4333277642726898\n",
      "Epoch 63 | Batch 88 | Loss: 0.5115798711776733\n",
      "Epoch 63 | Batch 89 | Loss: 0.3937317132949829\n",
      "Epoch 63 | Batch 90 | Loss: 0.00024375309294555336\n",
      "Epoch 64 | Batch 1 | Loss: 0.3120397925376892\n",
      "Epoch 64 | Batch 2 | Loss: 0.49052536487579346\n",
      "Epoch 64 | Batch 3 | Loss: 0.20985469222068787\n",
      "Epoch 64 | Batch 4 | Loss: 0.214064359664917\n",
      "Epoch 64 | Batch 5 | Loss: 0.45682889223098755\n",
      "Epoch 64 | Batch 6 | Loss: 0.6901842951774597\n",
      "Epoch 64 | Batch 7 | Loss: 0.22754523158073425\n",
      "Epoch 64 | Batch 8 | Loss: 0.2659524977207184\n",
      "Epoch 64 | Batch 9 | Loss: 0.13923418521881104\n",
      "Epoch 64 | Batch 10 | Loss: 0.6647037267684937\n",
      "Epoch 64 | Batch 11 | Loss: 0.1323348730802536\n",
      "Epoch 64 | Batch 12 | Loss: 0.7145495414733887\n",
      "Epoch 64 | Batch 13 | Loss: 0.32192590832710266\n",
      "Epoch 64 | Batch 14 | Loss: 0.6679900884628296\n",
      "Epoch 64 | Batch 15 | Loss: 0.3916208744049072\n",
      "Epoch 64 | Batch 16 | Loss: 0.13014379143714905\n",
      "Epoch 64 | Batch 17 | Loss: 0.313042014837265\n",
      "Epoch 64 | Batch 18 | Loss: 0.12730814516544342\n",
      "Epoch 64 | Batch 19 | Loss: 0.16719266772270203\n",
      "Epoch 64 | Batch 20 | Loss: 0.47833696007728577\n",
      "Epoch 64 | Batch 21 | Loss: 0.15515762567520142\n",
      "Epoch 64 | Batch 22 | Loss: 0.08527886867523193\n",
      "Epoch 64 | Batch 23 | Loss: 0.36250385642051697\n",
      "Epoch 64 | Batch 24 | Loss: 0.41216570138931274\n",
      "Epoch 64 | Batch 25 | Loss: 0.7945005893707275\n",
      "Epoch 64 | Batch 26 | Loss: 0.24068182706832886\n",
      "Epoch 64 | Batch 27 | Loss: 0.3264569342136383\n",
      "Epoch 64 | Batch 28 | Loss: 0.535014271736145\n",
      "Epoch 64 | Batch 29 | Loss: 0.6476565003395081\n",
      "Epoch 64 | Batch 30 | Loss: 0.4400782883167267\n",
      "Epoch 64 | Batch 31 | Loss: 0.4883662760257721\n",
      "Epoch 64 | Batch 32 | Loss: 0.2255152463912964\n",
      "Epoch 64 | Batch 33 | Loss: 0.31499239802360535\n",
      "Epoch 64 | Batch 34 | Loss: 0.7072744369506836\n",
      "Epoch 64 | Batch 35 | Loss: 0.3566747009754181\n",
      "Epoch 64 | Batch 36 | Loss: 0.14819595217704773\n",
      "Epoch 64 | Batch 37 | Loss: 0.4715561866760254\n",
      "Epoch 64 | Batch 38 | Loss: 0.718132495880127\n",
      "Epoch 64 | Batch 39 | Loss: 0.35301798582077026\n",
      "Epoch 64 | Batch 40 | Loss: 0.11875761300325394\n",
      "Epoch 64 | Batch 41 | Loss: 0.16480043530464172\n",
      "Epoch 64 | Batch 42 | Loss: 0.16848520934581757\n",
      "Epoch 64 | Batch 43 | Loss: 0.3928542137145996\n",
      "Epoch 64 | Batch 44 | Loss: 0.3349275588989258\n",
      "Epoch 64 | Batch 45 | Loss: 0.19422553479671478\n",
      "Epoch 64 | Batch 46 | Loss: 0.49120843410491943\n",
      "Epoch 64 | Batch 47 | Loss: 0.6379154324531555\n",
      "Epoch 64 | Batch 48 | Loss: 0.218159019947052\n",
      "Epoch 64 | Batch 49 | Loss: 0.4168355166912079\n",
      "Epoch 64 | Batch 50 | Loss: 0.4071207046508789\n",
      "Epoch 64 | Batch 51 | Loss: 0.3651782274246216\n",
      "Epoch 64 | Batch 52 | Loss: 0.2995709180831909\n",
      "Epoch 64 | Batch 53 | Loss: 0.1675569713115692\n",
      "Epoch 64 | Batch 54 | Loss: 0.2708840072154999\n",
      "Epoch 64 | Batch 55 | Loss: 0.590006947517395\n",
      "Epoch 64 | Batch 56 | Loss: 0.18441042304039001\n",
      "Epoch 64 | Batch 57 | Loss: 0.5441384315490723\n",
      "Epoch 64 | Batch 58 | Loss: 0.45064640045166016\n",
      "Epoch 64 | Batch 59 | Loss: 0.3756605088710785\n",
      "Epoch 64 | Batch 60 | Loss: 0.22953814268112183\n",
      "Epoch 64 | Batch 61 | Loss: 0.3448316156864166\n",
      "Epoch 64 | Batch 62 | Loss: 0.6194871068000793\n",
      "Epoch 64 | Batch 63 | Loss: 0.49003636837005615\n",
      "Epoch 64 | Batch 64 | Loss: 0.20120984315872192\n",
      "Epoch 64 | Batch 65 | Loss: 0.2177797257900238\n",
      "Epoch 64 | Batch 66 | Loss: 0.26385393738746643\n",
      "Epoch 64 | Batch 67 | Loss: 0.5101213455200195\n",
      "Epoch 64 | Batch 68 | Loss: 0.08566469699144363\n",
      "Epoch 64 | Batch 69 | Loss: 0.16450509428977966\n",
      "Epoch 64 | Batch 70 | Loss: 0.5742334723472595\n",
      "Epoch 64 | Batch 71 | Loss: 0.5505658984184265\n",
      "Epoch 64 | Batch 72 | Loss: 0.3226829171180725\n",
      "Epoch 64 | Batch 73 | Loss: 0.5106353163719177\n",
      "Epoch 64 | Batch 74 | Loss: 0.18064770102500916\n",
      "Epoch 64 | Batch 75 | Loss: 0.8132269382476807\n",
      "Epoch 64 | Batch 76 | Loss: 0.5485579371452332\n",
      "Epoch 64 | Batch 77 | Loss: 0.46229609847068787\n",
      "Epoch 64 | Batch 78 | Loss: 0.6971815824508667\n",
      "Epoch 64 | Batch 79 | Loss: 0.4060845375061035\n",
      "Epoch 64 | Batch 80 | Loss: 0.25916093587875366\n",
      "Epoch 64 | Batch 81 | Loss: 0.589954674243927\n",
      "Epoch 64 | Batch 82 | Loss: 0.22094380855560303\n",
      "Epoch 64 | Batch 83 | Loss: 0.13172602653503418\n",
      "Epoch 64 | Batch 84 | Loss: 0.16970406472682953\n",
      "Epoch 64 | Batch 85 | Loss: 0.22866886854171753\n",
      "Epoch 64 | Batch 86 | Loss: 0.3726317882537842\n",
      "Epoch 64 | Batch 87 | Loss: 0.15391355752944946\n",
      "Epoch 64 | Batch 88 | Loss: 0.3009299039840698\n",
      "Epoch 64 | Batch 89 | Loss: 0.40285441279411316\n",
      "Epoch 64 | Batch 90 | Loss: 0.19740281999111176\n",
      "Epoch 65 | Batch 1 | Loss: 0.1399610936641693\n",
      "Epoch 65 | Batch 2 | Loss: 0.6271964311599731\n",
      "Epoch 65 | Batch 3 | Loss: 0.3522135019302368\n",
      "Epoch 65 | Batch 4 | Loss: 0.09844940155744553\n",
      "Epoch 65 | Batch 5 | Loss: 0.08137911558151245\n",
      "Epoch 65 | Batch 6 | Loss: 0.6247583031654358\n",
      "Epoch 65 | Batch 7 | Loss: 0.4288431704044342\n",
      "Epoch 65 | Batch 8 | Loss: 0.5167688131332397\n",
      "Epoch 65 | Batch 9 | Loss: 0.10918992757797241\n",
      "Epoch 65 | Batch 10 | Loss: 0.36241623759269714\n",
      "Epoch 65 | Batch 11 | Loss: 0.3048447370529175\n",
      "Epoch 65 | Batch 12 | Loss: 0.4468836188316345\n",
      "Epoch 65 | Batch 13 | Loss: 0.22449561953544617\n",
      "Epoch 65 | Batch 14 | Loss: 0.22429509460926056\n",
      "Epoch 65 | Batch 15 | Loss: 0.21555843949317932\n",
      "Epoch 65 | Batch 16 | Loss: 0.5030227899551392\n",
      "Epoch 65 | Batch 17 | Loss: 0.11787184327840805\n",
      "Epoch 65 | Batch 18 | Loss: 0.21692870557308197\n",
      "Epoch 65 | Batch 19 | Loss: 0.3303910493850708\n",
      "Epoch 65 | Batch 20 | Loss: 0.34528371691703796\n",
      "Epoch 65 | Batch 21 | Loss: 0.32196950912475586\n",
      "Epoch 65 | Batch 22 | Loss: 0.6046289205551147\n",
      "Epoch 65 | Batch 23 | Loss: 0.2763729691505432\n",
      "Epoch 65 | Batch 24 | Loss: 1.0291588306427002\n",
      "Epoch 65 | Batch 25 | Loss: 0.32222291827201843\n",
      "Epoch 65 | Batch 26 | Loss: 0.5994616746902466\n",
      "Epoch 65 | Batch 27 | Loss: 0.2529650926589966\n",
      "Epoch 65 | Batch 28 | Loss: 0.5504637956619263\n",
      "Epoch 65 | Batch 29 | Loss: 0.1358761489391327\n",
      "Epoch 65 | Batch 30 | Loss: 0.4503486752510071\n",
      "Epoch 65 | Batch 31 | Loss: 0.548297643661499\n",
      "Epoch 65 | Batch 32 | Loss: 0.440412700176239\n",
      "Epoch 65 | Batch 33 | Loss: 0.339889258146286\n",
      "Epoch 65 | Batch 34 | Loss: 0.3574080169200897\n",
      "Epoch 65 | Batch 35 | Loss: 0.22899287939071655\n",
      "Epoch 65 | Batch 36 | Loss: 0.08483079075813293\n",
      "Epoch 65 | Batch 37 | Loss: 0.2122460901737213\n",
      "Epoch 65 | Batch 38 | Loss: 0.10612112283706665\n",
      "Epoch 65 | Batch 39 | Loss: 0.23495697975158691\n",
      "Epoch 65 | Batch 40 | Loss: 0.8739174604415894\n",
      "Epoch 65 | Batch 41 | Loss: 0.24919255077838898\n",
      "Epoch 65 | Batch 42 | Loss: 0.6434111595153809\n",
      "Epoch 65 | Batch 43 | Loss: 0.4626895785331726\n",
      "Epoch 65 | Batch 44 | Loss: 0.25613147020339966\n",
      "Epoch 65 | Batch 45 | Loss: 0.3618317246437073\n",
      "Epoch 65 | Batch 46 | Loss: 0.39511987566947937\n",
      "Epoch 65 | Batch 47 | Loss: 0.48175185918807983\n",
      "Epoch 65 | Batch 48 | Loss: 0.43413156270980835\n",
      "Epoch 65 | Batch 49 | Loss: 0.5544794797897339\n",
      "Epoch 65 | Batch 50 | Loss: 0.31883323192596436\n",
      "Epoch 65 | Batch 51 | Loss: 0.735873818397522\n",
      "Epoch 65 | Batch 52 | Loss: 0.4276023507118225\n",
      "Epoch 65 | Batch 53 | Loss: 0.09506972134113312\n",
      "Epoch 65 | Batch 54 | Loss: 0.5090104341506958\n",
      "Epoch 65 | Batch 55 | Loss: 0.299709677696228\n",
      "Epoch 65 | Batch 56 | Loss: 0.18507549166679382\n",
      "Epoch 65 | Batch 57 | Loss: 0.12614722549915314\n",
      "Epoch 65 | Batch 58 | Loss: 0.26093873381614685\n",
      "Epoch 65 | Batch 59 | Loss: 0.510354220867157\n",
      "Epoch 65 | Batch 60 | Loss: 0.27005842328071594\n",
      "Epoch 65 | Batch 61 | Loss: 0.11808948218822479\n",
      "Epoch 65 | Batch 62 | Loss: 0.1463344246149063\n",
      "Epoch 65 | Batch 63 | Loss: 0.5529691576957703\n",
      "Epoch 65 | Batch 64 | Loss: 0.2833489775657654\n",
      "Epoch 65 | Batch 65 | Loss: 0.3525480031967163\n",
      "Epoch 65 | Batch 66 | Loss: 0.4414978623390198\n",
      "Epoch 65 | Batch 67 | Loss: 0.5333350300788879\n",
      "Epoch 65 | Batch 68 | Loss: 0.08957105129957199\n",
      "Epoch 65 | Batch 69 | Loss: 0.14635878801345825\n",
      "Epoch 65 | Batch 70 | Loss: 0.5426084995269775\n",
      "Epoch 65 | Batch 71 | Loss: 0.45098188519477844\n",
      "Epoch 65 | Batch 72 | Loss: 0.8398787975311279\n",
      "Epoch 65 | Batch 73 | Loss: 0.33357107639312744\n",
      "Epoch 65 | Batch 74 | Loss: 0.09035550802946091\n",
      "Epoch 65 | Batch 75 | Loss: 0.5366841554641724\n",
      "Epoch 65 | Batch 76 | Loss: 0.29992419481277466\n",
      "Epoch 65 | Batch 77 | Loss: 0.25853389501571655\n",
      "Epoch 65 | Batch 78 | Loss: 0.364849328994751\n",
      "Epoch 65 | Batch 79 | Loss: 0.45278501510620117\n",
      "Epoch 65 | Batch 80 | Loss: 0.7726798057556152\n",
      "Epoch 65 | Batch 81 | Loss: 0.3802327811717987\n",
      "Epoch 65 | Batch 82 | Loss: 0.1566946804523468\n",
      "Epoch 65 | Batch 83 | Loss: 0.7677602767944336\n",
      "Epoch 65 | Batch 84 | Loss: 0.2199721485376358\n",
      "Epoch 65 | Batch 85 | Loss: 0.4819274842739105\n",
      "Epoch 65 | Batch 86 | Loss: 0.49756935238838196\n",
      "Epoch 65 | Batch 87 | Loss: 0.2567475438117981\n",
      "Epoch 65 | Batch 88 | Loss: 0.16788005828857422\n",
      "Epoch 65 | Batch 89 | Loss: 0.22294235229492188\n",
      "Epoch 65 | Batch 90 | Loss: 2.1991055011749268\n",
      "Epoch 66 | Batch 1 | Loss: 0.7693965435028076\n",
      "Epoch 66 | Batch 2 | Loss: 0.3653219938278198\n",
      "Epoch 66 | Batch 3 | Loss: 1.078485131263733\n",
      "Epoch 66 | Batch 4 | Loss: 0.3217710554599762\n",
      "Epoch 66 | Batch 5 | Loss: 0.2746623754501343\n",
      "Epoch 66 | Batch 6 | Loss: 0.3620333969593048\n",
      "Epoch 66 | Batch 7 | Loss: 0.26002514362335205\n",
      "Epoch 66 | Batch 8 | Loss: 0.3153958320617676\n",
      "Epoch 66 | Batch 9 | Loss: 0.630698561668396\n",
      "Epoch 66 | Batch 10 | Loss: 0.2527333199977875\n",
      "Epoch 66 | Batch 11 | Loss: 0.3546331822872162\n",
      "Epoch 66 | Batch 12 | Loss: 0.1754114329814911\n",
      "Epoch 66 | Batch 13 | Loss: 0.1195509135723114\n",
      "Epoch 66 | Batch 14 | Loss: 0.35841143131256104\n",
      "Epoch 66 | Batch 15 | Loss: 0.27647823095321655\n",
      "Epoch 66 | Batch 16 | Loss: 0.48226484656333923\n",
      "Epoch 66 | Batch 17 | Loss: 0.19667750597000122\n",
      "Epoch 66 | Batch 18 | Loss: 0.28466784954071045\n",
      "Epoch 66 | Batch 19 | Loss: 0.19698688387870789\n",
      "Epoch 66 | Batch 20 | Loss: 0.1851310282945633\n",
      "Epoch 66 | Batch 21 | Loss: 0.2547450065612793\n",
      "Epoch 66 | Batch 22 | Loss: 0.16053424775600433\n",
      "Epoch 66 | Batch 23 | Loss: 0.5832157135009766\n",
      "Epoch 66 | Batch 24 | Loss: 0.622532069683075\n",
      "Epoch 66 | Batch 25 | Loss: 0.2977021038532257\n",
      "Epoch 66 | Batch 26 | Loss: 0.21786625683307648\n",
      "Epoch 66 | Batch 27 | Loss: 0.4917275011539459\n",
      "Epoch 66 | Batch 28 | Loss: 0.5714647769927979\n",
      "Epoch 66 | Batch 29 | Loss: 0.3308798372745514\n",
      "Epoch 66 | Batch 30 | Loss: 0.3971089720726013\n",
      "Epoch 66 | Batch 31 | Loss: 0.15135148167610168\n",
      "Epoch 66 | Batch 32 | Loss: 0.08566519618034363\n",
      "Epoch 66 | Batch 33 | Loss: 0.20968414843082428\n",
      "Epoch 66 | Batch 34 | Loss: 0.6077499389648438\n",
      "Epoch 66 | Batch 35 | Loss: 0.18093082308769226\n",
      "Epoch 66 | Batch 36 | Loss: 0.5901880860328674\n",
      "Epoch 66 | Batch 37 | Loss: 0.14648586511611938\n",
      "Epoch 66 | Batch 38 | Loss: 0.13218870759010315\n",
      "Epoch 66 | Batch 39 | Loss: 0.3045576512813568\n",
      "Epoch 66 | Batch 40 | Loss: 0.33995741605758667\n",
      "Epoch 66 | Batch 41 | Loss: 0.3262825012207031\n",
      "Epoch 66 | Batch 42 | Loss: 0.38739073276519775\n",
      "Epoch 66 | Batch 43 | Loss: 0.22721163928508759\n",
      "Epoch 66 | Batch 44 | Loss: 0.45544251799583435\n",
      "Epoch 66 | Batch 45 | Loss: 0.1513870656490326\n",
      "Epoch 66 | Batch 46 | Loss: 0.2544472813606262\n",
      "Epoch 66 | Batch 47 | Loss: 0.6844721436500549\n",
      "Epoch 66 | Batch 48 | Loss: 0.5124450922012329\n",
      "Epoch 66 | Batch 49 | Loss: 0.1398737132549286\n",
      "Epoch 66 | Batch 50 | Loss: 0.41728833317756653\n",
      "Epoch 66 | Batch 51 | Loss: 0.10484441369771957\n",
      "Epoch 66 | Batch 52 | Loss: 0.15014956891536713\n",
      "Epoch 66 | Batch 53 | Loss: 0.27739113569259644\n",
      "Epoch 66 | Batch 54 | Loss: 0.31152841448783875\n",
      "Epoch 66 | Batch 55 | Loss: 0.8547897338867188\n",
      "Epoch 66 | Batch 56 | Loss: 0.4178256094455719\n",
      "Epoch 66 | Batch 57 | Loss: 0.28630390763282776\n",
      "Epoch 66 | Batch 58 | Loss: 0.16936849057674408\n",
      "Epoch 66 | Batch 59 | Loss: 0.2892124056816101\n",
      "Epoch 66 | Batch 60 | Loss: 0.350159227848053\n",
      "Epoch 66 | Batch 61 | Loss: 0.18826541304588318\n",
      "Epoch 66 | Batch 62 | Loss: 0.6196709275245667\n",
      "Epoch 66 | Batch 63 | Loss: 0.10463814437389374\n",
      "Epoch 66 | Batch 64 | Loss: 0.2818126976490021\n",
      "Epoch 66 | Batch 65 | Loss: 0.962804913520813\n",
      "Epoch 66 | Batch 66 | Loss: 0.0590345561504364\n",
      "Epoch 66 | Batch 67 | Loss: 0.5941089987754822\n",
      "Epoch 66 | Batch 68 | Loss: 0.3252262473106384\n",
      "Epoch 66 | Batch 69 | Loss: 0.8328549265861511\n",
      "Epoch 66 | Batch 70 | Loss: 0.5586211681365967\n",
      "Epoch 66 | Batch 71 | Loss: 0.18414434790611267\n",
      "Epoch 66 | Batch 72 | Loss: 0.3976704180240631\n",
      "Epoch 66 | Batch 73 | Loss: 0.5864161252975464\n",
      "Epoch 66 | Batch 74 | Loss: 0.4265781044960022\n",
      "Epoch 66 | Batch 75 | Loss: 0.2513338327407837\n",
      "Epoch 66 | Batch 76 | Loss: 0.41566556692123413\n",
      "Epoch 66 | Batch 77 | Loss: 0.7182166576385498\n",
      "Epoch 66 | Batch 78 | Loss: 0.43487632274627686\n",
      "Epoch 66 | Batch 79 | Loss: 0.2433452159166336\n",
      "Epoch 66 | Batch 80 | Loss: 0.24559226632118225\n",
      "Epoch 66 | Batch 81 | Loss: 0.1800035685300827\n",
      "Epoch 66 | Batch 82 | Loss: 0.48937374353408813\n",
      "Epoch 66 | Batch 83 | Loss: 0.488444060087204\n",
      "Epoch 66 | Batch 84 | Loss: 0.3413718044757843\n",
      "Epoch 66 | Batch 85 | Loss: 0.4549526274204254\n",
      "Epoch 66 | Batch 86 | Loss: 0.5018960237503052\n",
      "Epoch 66 | Batch 87 | Loss: 0.2075994312763214\n",
      "Epoch 66 | Batch 88 | Loss: 0.6530892252922058\n",
      "Epoch 66 | Batch 89 | Loss: 0.5906471014022827\n",
      "Epoch 66 | Batch 90 | Loss: 0.24824275076389313\n",
      "Epoch 67 | Batch 1 | Loss: 0.28661325573921204\n",
      "Epoch 67 | Batch 2 | Loss: 0.7049909830093384\n",
      "Epoch 67 | Batch 3 | Loss: 0.13239967823028564\n",
      "Epoch 67 | Batch 4 | Loss: 0.48604926466941833\n",
      "Epoch 67 | Batch 5 | Loss: 0.22632750868797302\n",
      "Epoch 67 | Batch 6 | Loss: 0.28386014699935913\n",
      "Epoch 67 | Batch 7 | Loss: 0.5913503170013428\n",
      "Epoch 67 | Batch 8 | Loss: 0.3749091625213623\n",
      "Epoch 67 | Batch 9 | Loss: 0.9558866024017334\n",
      "Epoch 67 | Batch 10 | Loss: 0.15712393820285797\n",
      "Epoch 67 | Batch 11 | Loss: 0.770127534866333\n",
      "Epoch 67 | Batch 12 | Loss: 0.33551663160324097\n",
      "Epoch 67 | Batch 13 | Loss: 0.3752398192882538\n",
      "Epoch 67 | Batch 14 | Loss: 0.49710673093795776\n",
      "Epoch 67 | Batch 15 | Loss: 0.2048109471797943\n",
      "Epoch 67 | Batch 16 | Loss: 0.2763456404209137\n",
      "Epoch 67 | Batch 17 | Loss: 0.7502034902572632\n",
      "Epoch 67 | Batch 18 | Loss: 0.3877444863319397\n",
      "Epoch 67 | Batch 19 | Loss: 0.41221070289611816\n",
      "Epoch 67 | Batch 20 | Loss: 0.144977867603302\n",
      "Epoch 67 | Batch 21 | Loss: 0.21009674668312073\n",
      "Epoch 67 | Batch 22 | Loss: 0.22500771284103394\n",
      "Epoch 67 | Batch 23 | Loss: 0.21148736774921417\n",
      "Epoch 67 | Batch 24 | Loss: 0.4946378171443939\n",
      "Epoch 67 | Batch 25 | Loss: 0.5602854490280151\n",
      "Epoch 67 | Batch 26 | Loss: 0.3351006507873535\n",
      "Epoch 67 | Batch 27 | Loss: 0.16300715506076813\n",
      "Epoch 67 | Batch 28 | Loss: 0.23122239112854004\n",
      "Epoch 67 | Batch 29 | Loss: 0.6084213256835938\n",
      "Epoch 67 | Batch 30 | Loss: 0.3741806149482727\n",
      "Epoch 67 | Batch 31 | Loss: 0.2833283543586731\n",
      "Epoch 67 | Batch 32 | Loss: 0.44303345680236816\n",
      "Epoch 67 | Batch 33 | Loss: 0.2956324815750122\n",
      "Epoch 67 | Batch 34 | Loss: 0.12186398357152939\n",
      "Epoch 67 | Batch 35 | Loss: 0.19059044122695923\n",
      "Epoch 67 | Batch 36 | Loss: 0.281650185585022\n",
      "Epoch 67 | Batch 37 | Loss: 0.32100722193717957\n",
      "Epoch 67 | Batch 38 | Loss: 0.14002501964569092\n",
      "Epoch 67 | Batch 39 | Loss: 0.2472974956035614\n",
      "Epoch 67 | Batch 40 | Loss: 0.42583411931991577\n",
      "Epoch 67 | Batch 41 | Loss: 0.4432973861694336\n",
      "Epoch 67 | Batch 42 | Loss: 0.19373038411140442\n",
      "Epoch 67 | Batch 43 | Loss: 0.1485798954963684\n",
      "Epoch 67 | Batch 44 | Loss: 0.4640277028083801\n",
      "Epoch 67 | Batch 45 | Loss: 0.3997039496898651\n",
      "Epoch 67 | Batch 46 | Loss: 0.4681910276412964\n",
      "Epoch 67 | Batch 47 | Loss: 0.2545074224472046\n",
      "Epoch 67 | Batch 48 | Loss: 0.07616624981164932\n",
      "Epoch 67 | Batch 49 | Loss: 1.0435189008712769\n",
      "Epoch 67 | Batch 50 | Loss: 0.33830526471138\n",
      "Epoch 67 | Batch 51 | Loss: 0.28940004110336304\n",
      "Epoch 67 | Batch 52 | Loss: 0.4170747399330139\n",
      "Epoch 67 | Batch 53 | Loss: 0.6217605471611023\n",
      "Epoch 67 | Batch 54 | Loss: 0.3815280497074127\n",
      "Epoch 67 | Batch 55 | Loss: 0.5037832856178284\n",
      "Epoch 67 | Batch 56 | Loss: 0.39314013719558716\n",
      "Epoch 67 | Batch 57 | Loss: 0.6290955543518066\n",
      "Epoch 67 | Batch 58 | Loss: 0.21846990287303925\n",
      "Epoch 67 | Batch 59 | Loss: 0.7946754693984985\n",
      "Epoch 67 | Batch 60 | Loss: 0.27673569321632385\n",
      "Epoch 67 | Batch 61 | Loss: 0.4303165078163147\n",
      "Epoch 67 | Batch 62 | Loss: 0.17326517403125763\n",
      "Epoch 67 | Batch 63 | Loss: 0.21471133828163147\n",
      "Epoch 67 | Batch 64 | Loss: 0.24417918920516968\n",
      "Epoch 67 | Batch 65 | Loss: 0.270388662815094\n",
      "Epoch 67 | Batch 66 | Loss: 0.6354718208312988\n",
      "Epoch 67 | Batch 67 | Loss: 0.3904575705528259\n",
      "Epoch 67 | Batch 68 | Loss: 0.33800315856933594\n",
      "Epoch 67 | Batch 69 | Loss: 0.46362459659576416\n",
      "Epoch 67 | Batch 70 | Loss: 0.45204275846481323\n",
      "Epoch 67 | Batch 71 | Loss: 0.2356003224849701\n",
      "Epoch 67 | Batch 72 | Loss: 0.46009230613708496\n",
      "Epoch 67 | Batch 73 | Loss: 0.21319547295570374\n",
      "Epoch 67 | Batch 74 | Loss: 0.3772834837436676\n",
      "Epoch 67 | Batch 75 | Loss: 0.3288891315460205\n",
      "Epoch 67 | Batch 76 | Loss: 0.5334886312484741\n",
      "Epoch 67 | Batch 77 | Loss: 0.14902174472808838\n",
      "Epoch 67 | Batch 78 | Loss: 0.5149712562561035\n",
      "Epoch 67 | Batch 79 | Loss: 0.22097483277320862\n",
      "Epoch 67 | Batch 80 | Loss: 0.4879339635372162\n",
      "Epoch 67 | Batch 81 | Loss: 0.25591516494750977\n",
      "Epoch 67 | Batch 82 | Loss: 0.15882763266563416\n",
      "Epoch 67 | Batch 83 | Loss: 0.615312933921814\n",
      "Epoch 67 | Batch 84 | Loss: 0.18507526814937592\n",
      "Epoch 67 | Batch 85 | Loss: 0.10937439650297165\n",
      "Epoch 67 | Batch 86 | Loss: 0.4436265230178833\n",
      "Epoch 67 | Batch 87 | Loss: 0.17191827297210693\n",
      "Epoch 67 | Batch 88 | Loss: 0.09814651310443878\n",
      "Epoch 67 | Batch 89 | Loss: 0.512660026550293\n",
      "Epoch 67 | Batch 90 | Loss: 0.052741557359695435\n",
      "Epoch 68 | Batch 1 | Loss: 0.2713778018951416\n",
      "Epoch 68 | Batch 2 | Loss: 0.823326051235199\n",
      "Epoch 68 | Batch 3 | Loss: 0.39881786704063416\n",
      "Epoch 68 | Batch 4 | Loss: 0.469773530960083\n",
      "Epoch 68 | Batch 5 | Loss: 0.31451961398124695\n",
      "Epoch 68 | Batch 6 | Loss: 0.6077700853347778\n",
      "Epoch 68 | Batch 7 | Loss: 0.11206500977277756\n",
      "Epoch 68 | Batch 8 | Loss: 0.29662930965423584\n",
      "Epoch 68 | Batch 9 | Loss: 0.45902013778686523\n",
      "Epoch 68 | Batch 10 | Loss: 0.5145639181137085\n",
      "Epoch 68 | Batch 11 | Loss: 0.09445630013942719\n",
      "Epoch 68 | Batch 12 | Loss: 0.20225510001182556\n",
      "Epoch 68 | Batch 13 | Loss: 0.33170822262763977\n",
      "Epoch 68 | Batch 14 | Loss: 0.24038250744342804\n",
      "Epoch 68 | Batch 15 | Loss: 0.4791196882724762\n",
      "Epoch 68 | Batch 16 | Loss: 0.4239239990711212\n",
      "Epoch 68 | Batch 17 | Loss: 0.22701236605644226\n",
      "Epoch 68 | Batch 18 | Loss: 0.5574353337287903\n",
      "Epoch 68 | Batch 19 | Loss: 0.4585040807723999\n",
      "Epoch 68 | Batch 20 | Loss: 0.45965278148651123\n",
      "Epoch 68 | Batch 21 | Loss: 0.41645723581314087\n",
      "Epoch 68 | Batch 22 | Loss: 0.32949015498161316\n",
      "Epoch 68 | Batch 23 | Loss: 0.16466988623142242\n",
      "Epoch 68 | Batch 24 | Loss: 0.41252434253692627\n",
      "Epoch 68 | Batch 25 | Loss: 0.13060879707336426\n",
      "Epoch 68 | Batch 26 | Loss: 0.5405205488204956\n",
      "Epoch 68 | Batch 27 | Loss: 0.21438533067703247\n",
      "Epoch 68 | Batch 28 | Loss: 0.25339633226394653\n",
      "Epoch 68 | Batch 29 | Loss: 0.10636034607887268\n",
      "Epoch 68 | Batch 30 | Loss: 0.11015105992555618\n",
      "Epoch 68 | Batch 31 | Loss: 0.752009391784668\n",
      "Epoch 68 | Batch 32 | Loss: 0.202252596616745\n",
      "Epoch 68 | Batch 33 | Loss: 0.10297688096761703\n",
      "Epoch 68 | Batch 34 | Loss: 0.35329771041870117\n",
      "Epoch 68 | Batch 35 | Loss: 0.6332507133483887\n",
      "Epoch 68 | Batch 36 | Loss: 0.4551888108253479\n",
      "Epoch 68 | Batch 37 | Loss: 0.5056993365287781\n",
      "Epoch 68 | Batch 38 | Loss: 0.17280733585357666\n",
      "Epoch 68 | Batch 39 | Loss: 0.27195438742637634\n",
      "Epoch 68 | Batch 40 | Loss: 0.3283265233039856\n",
      "Epoch 68 | Batch 41 | Loss: 0.3893074691295624\n",
      "Epoch 68 | Batch 42 | Loss: 0.2373630404472351\n",
      "Epoch 68 | Batch 43 | Loss: 0.394786536693573\n",
      "Epoch 68 | Batch 44 | Loss: 0.257127583026886\n",
      "Epoch 68 | Batch 45 | Loss: 0.38609012961387634\n",
      "Epoch 68 | Batch 46 | Loss: 0.09545387327671051\n",
      "Epoch 68 | Batch 47 | Loss: 0.15952059626579285\n",
      "Epoch 68 | Batch 48 | Loss: 0.6888986825942993\n",
      "Epoch 68 | Batch 49 | Loss: 0.48471084237098694\n",
      "Epoch 68 | Batch 50 | Loss: 0.2423509955406189\n",
      "Epoch 68 | Batch 51 | Loss: 0.24369841814041138\n",
      "Epoch 68 | Batch 52 | Loss: 0.469946026802063\n",
      "Epoch 68 | Batch 53 | Loss: 0.48820191621780396\n",
      "Epoch 68 | Batch 54 | Loss: 0.4781228005886078\n",
      "Epoch 68 | Batch 55 | Loss: 0.21972136199474335\n",
      "Epoch 68 | Batch 56 | Loss: 0.2468617707490921\n",
      "Epoch 68 | Batch 57 | Loss: 0.48002922534942627\n",
      "Epoch 68 | Batch 58 | Loss: 0.2422807663679123\n",
      "Epoch 68 | Batch 59 | Loss: 0.25434455275535583\n",
      "Epoch 68 | Batch 60 | Loss: 0.16282051801681519\n",
      "Epoch 68 | Batch 61 | Loss: 0.15722090005874634\n",
      "Epoch 68 | Batch 62 | Loss: 0.5195303559303284\n",
      "Epoch 68 | Batch 63 | Loss: 0.1743583083152771\n",
      "Epoch 68 | Batch 64 | Loss: 0.09106321632862091\n",
      "Epoch 68 | Batch 65 | Loss: 0.40325403213500977\n",
      "Epoch 68 | Batch 66 | Loss: 0.3850114345550537\n",
      "Epoch 68 | Batch 67 | Loss: 0.1898534893989563\n",
      "Epoch 68 | Batch 68 | Loss: 0.9573849439620972\n",
      "Epoch 68 | Batch 69 | Loss: 0.16804064810276031\n",
      "Epoch 68 | Batch 70 | Loss: 0.5327865481376648\n",
      "Epoch 68 | Batch 71 | Loss: 0.8884212970733643\n",
      "Epoch 68 | Batch 72 | Loss: 0.46792206168174744\n",
      "Epoch 68 | Batch 73 | Loss: 0.3335362374782562\n",
      "Epoch 68 | Batch 74 | Loss: 0.5787872076034546\n",
      "Epoch 68 | Batch 75 | Loss: 1.1792302131652832\n",
      "Epoch 68 | Batch 76 | Loss: 0.2506008446216583\n",
      "Epoch 68 | Batch 77 | Loss: 0.26428818702697754\n",
      "Epoch 68 | Batch 78 | Loss: 0.5487473607063293\n",
      "Epoch 68 | Batch 79 | Loss: 0.16469022631645203\n",
      "Epoch 68 | Batch 80 | Loss: 0.4405703544616699\n",
      "Epoch 68 | Batch 81 | Loss: 0.7863951921463013\n",
      "Epoch 68 | Batch 82 | Loss: 0.27386271953582764\n",
      "Epoch 68 | Batch 83 | Loss: 0.18670405447483063\n",
      "Epoch 68 | Batch 84 | Loss: 0.17940016090869904\n",
      "Epoch 68 | Batch 85 | Loss: 0.28366798162460327\n",
      "Epoch 68 | Batch 86 | Loss: 0.5224462151527405\n",
      "Epoch 68 | Batch 87 | Loss: 0.508560299873352\n",
      "Epoch 68 | Batch 88 | Loss: 0.11578789353370667\n",
      "Epoch 68 | Batch 89 | Loss: 0.22164443135261536\n",
      "Epoch 68 | Batch 90 | Loss: 0.15980780124664307\n",
      "Epoch 69 | Batch 1 | Loss: 0.05725139379501343\n",
      "Epoch 69 | Batch 2 | Loss: 0.5206494331359863\n",
      "Epoch 69 | Batch 3 | Loss: 0.20278164744377136\n",
      "Epoch 69 | Batch 4 | Loss: 0.41261160373687744\n",
      "Epoch 69 | Batch 5 | Loss: 0.23983414471149445\n",
      "Epoch 69 | Batch 6 | Loss: 0.49327048659324646\n",
      "Epoch 69 | Batch 7 | Loss: 0.6649353504180908\n",
      "Epoch 69 | Batch 8 | Loss: 0.32782021164894104\n",
      "Epoch 69 | Batch 9 | Loss: 0.36600640416145325\n",
      "Epoch 69 | Batch 10 | Loss: 0.2324059009552002\n",
      "Epoch 69 | Batch 11 | Loss: 0.26788434386253357\n",
      "Epoch 69 | Batch 12 | Loss: 0.23531948029994965\n",
      "Epoch 69 | Batch 13 | Loss: 0.2207796573638916\n",
      "Epoch 69 | Batch 14 | Loss: 0.30376866459846497\n",
      "Epoch 69 | Batch 15 | Loss: 0.5483414530754089\n",
      "Epoch 69 | Batch 16 | Loss: 0.14078259468078613\n",
      "Epoch 69 | Batch 17 | Loss: 0.10884103924036026\n",
      "Epoch 69 | Batch 18 | Loss: 0.43432164192199707\n",
      "Epoch 69 | Batch 19 | Loss: 0.3007703721523285\n",
      "Epoch 69 | Batch 20 | Loss: 0.2959107458591461\n",
      "Epoch 69 | Batch 21 | Loss: 0.18054571747779846\n",
      "Epoch 69 | Batch 22 | Loss: 0.2742062211036682\n",
      "Epoch 69 | Batch 23 | Loss: 0.29090869426727295\n",
      "Epoch 69 | Batch 24 | Loss: 0.060887180268764496\n",
      "Epoch 69 | Batch 25 | Loss: 0.2464984953403473\n",
      "Epoch 69 | Batch 26 | Loss: 0.359436571598053\n",
      "Epoch 69 | Batch 27 | Loss: 0.1297307312488556\n",
      "Epoch 69 | Batch 28 | Loss: 0.17103323340415955\n",
      "Epoch 69 | Batch 29 | Loss: 0.48584794998168945\n",
      "Epoch 69 | Batch 30 | Loss: 0.370063453912735\n",
      "Epoch 69 | Batch 31 | Loss: 0.4226614832878113\n",
      "Epoch 69 | Batch 32 | Loss: 0.4970267713069916\n",
      "Epoch 69 | Batch 33 | Loss: 0.10156269371509552\n",
      "Epoch 69 | Batch 34 | Loss: 0.2057284116744995\n",
      "Epoch 69 | Batch 35 | Loss: 0.769045352935791\n",
      "Epoch 69 | Batch 36 | Loss: 0.5025557279586792\n",
      "Epoch 69 | Batch 37 | Loss: 0.39896756410598755\n",
      "Epoch 69 | Batch 38 | Loss: 0.4982788562774658\n",
      "Epoch 69 | Batch 39 | Loss: 0.34810149669647217\n",
      "Epoch 69 | Batch 40 | Loss: 0.757438600063324\n",
      "Epoch 69 | Batch 41 | Loss: 0.13588687777519226\n",
      "Epoch 69 | Batch 42 | Loss: 0.7613109350204468\n",
      "Epoch 69 | Batch 43 | Loss: 0.05764850974082947\n",
      "Epoch 69 | Batch 44 | Loss: 0.6253252625465393\n",
      "Epoch 69 | Batch 45 | Loss: 0.3820154368877411\n",
      "Epoch 69 | Batch 46 | Loss: 0.07745681703090668\n",
      "Epoch 69 | Batch 47 | Loss: 0.5257163047790527\n",
      "Epoch 69 | Batch 48 | Loss: 0.08652627468109131\n",
      "Epoch 69 | Batch 49 | Loss: 0.5241652727127075\n",
      "Epoch 69 | Batch 50 | Loss: 0.1512729674577713\n",
      "Epoch 69 | Batch 51 | Loss: 0.5781540274620056\n",
      "Epoch 69 | Batch 52 | Loss: 0.22989997267723083\n",
      "Epoch 69 | Batch 53 | Loss: 0.6767778396606445\n",
      "Epoch 69 | Batch 54 | Loss: 0.21933576464653015\n",
      "Epoch 69 | Batch 55 | Loss: 0.26796334981918335\n",
      "Epoch 69 | Batch 56 | Loss: 0.2740660309791565\n",
      "Epoch 69 | Batch 57 | Loss: 0.2486133873462677\n",
      "Epoch 69 | Batch 58 | Loss: 0.6482820510864258\n",
      "Epoch 69 | Batch 59 | Loss: 0.5226012468338013\n",
      "Epoch 69 | Batch 60 | Loss: 0.13351468741893768\n",
      "Epoch 69 | Batch 61 | Loss: 0.42298588156700134\n",
      "Epoch 69 | Batch 62 | Loss: 0.13666288554668427\n",
      "Epoch 69 | Batch 63 | Loss: 0.5818530321121216\n",
      "Epoch 69 | Batch 64 | Loss: 0.3426337242126465\n",
      "Epoch 69 | Batch 65 | Loss: 0.42773646116256714\n",
      "Epoch 69 | Batch 66 | Loss: 0.7205770015716553\n",
      "Epoch 69 | Batch 67 | Loss: 0.39016762375831604\n",
      "Epoch 69 | Batch 68 | Loss: 0.8677570819854736\n",
      "Epoch 69 | Batch 69 | Loss: 0.3962678611278534\n",
      "Epoch 69 | Batch 70 | Loss: 0.5732345581054688\n",
      "Epoch 69 | Batch 71 | Loss: 0.2312832772731781\n",
      "Epoch 69 | Batch 72 | Loss: 0.8589115142822266\n",
      "Epoch 69 | Batch 73 | Loss: 0.3897978365421295\n",
      "Epoch 69 | Batch 74 | Loss: 0.2486582249403\n",
      "Epoch 69 | Batch 75 | Loss: 0.5600113272666931\n",
      "Epoch 69 | Batch 76 | Loss: 0.2591228187084198\n",
      "Epoch 69 | Batch 77 | Loss: 0.5994665026664734\n",
      "Epoch 69 | Batch 78 | Loss: 0.2538731098175049\n",
      "Epoch 69 | Batch 79 | Loss: 0.178274005651474\n",
      "Epoch 69 | Batch 80 | Loss: 0.6682775020599365\n",
      "Epoch 69 | Batch 81 | Loss: 0.3582760691642761\n",
      "Epoch 69 | Batch 82 | Loss: 0.11995597928762436\n",
      "Epoch 69 | Batch 83 | Loss: 0.4342955946922302\n",
      "Epoch 69 | Batch 84 | Loss: 0.5469717979431152\n",
      "Epoch 69 | Batch 85 | Loss: 0.22133365273475647\n",
      "Epoch 69 | Batch 86 | Loss: 0.5967757701873779\n",
      "Epoch 69 | Batch 87 | Loss: 0.177634134888649\n",
      "Epoch 69 | Batch 88 | Loss: 0.2383105605840683\n",
      "Epoch 69 | Batch 89 | Loss: 0.2630177438259125\n",
      "Epoch 69 | Batch 90 | Loss: 0.13017188012599945\n",
      "Epoch 70 | Batch 1 | Loss: 0.3654290437698364\n",
      "Epoch 70 | Batch 2 | Loss: 0.5544732213020325\n",
      "Epoch 70 | Batch 3 | Loss: 0.23656931519508362\n",
      "Epoch 70 | Batch 4 | Loss: 0.3678092658519745\n",
      "Epoch 70 | Batch 5 | Loss: 0.1162872463464737\n",
      "Epoch 70 | Batch 6 | Loss: 0.41277381777763367\n",
      "Epoch 70 | Batch 7 | Loss: 0.3917405605316162\n",
      "Epoch 70 | Batch 8 | Loss: 0.3227691650390625\n",
      "Epoch 70 | Batch 9 | Loss: 0.747434139251709\n",
      "Epoch 70 | Batch 10 | Loss: 0.11240771412849426\n",
      "Epoch 70 | Batch 11 | Loss: 0.1841600090265274\n",
      "Epoch 70 | Batch 12 | Loss: 0.23165644705295563\n",
      "Epoch 70 | Batch 13 | Loss: 0.35918086767196655\n",
      "Epoch 70 | Batch 14 | Loss: 0.8118405938148499\n",
      "Epoch 70 | Batch 15 | Loss: 0.1474824845790863\n",
      "Epoch 70 | Batch 16 | Loss: 0.16475963592529297\n",
      "Epoch 70 | Batch 17 | Loss: 0.5049506425857544\n",
      "Epoch 70 | Batch 18 | Loss: 0.1854538917541504\n",
      "Epoch 70 | Batch 19 | Loss: 0.22761420905590057\n",
      "Epoch 70 | Batch 20 | Loss: 1.1805706024169922\n",
      "Epoch 70 | Batch 21 | Loss: 0.37532925605773926\n",
      "Epoch 70 | Batch 22 | Loss: 0.5369770526885986\n",
      "Epoch 70 | Batch 23 | Loss: 0.371234655380249\n",
      "Epoch 70 | Batch 24 | Loss: 0.5067081451416016\n",
      "Epoch 70 | Batch 25 | Loss: 0.253593385219574\n",
      "Epoch 70 | Batch 26 | Loss: 0.3777373433113098\n",
      "Epoch 70 | Batch 27 | Loss: 0.2356659173965454\n",
      "Epoch 70 | Batch 28 | Loss: 0.08941628038883209\n",
      "Epoch 70 | Batch 29 | Loss: 0.13837088644504547\n",
      "Epoch 70 | Batch 30 | Loss: 0.35430634021759033\n",
      "Epoch 70 | Batch 31 | Loss: 0.9248971343040466\n",
      "Epoch 70 | Batch 32 | Loss: 0.1568867266178131\n",
      "Epoch 70 | Batch 33 | Loss: 0.1794038861989975\n",
      "Epoch 70 | Batch 34 | Loss: 0.4691181182861328\n",
      "Epoch 70 | Batch 35 | Loss: 0.2871728539466858\n",
      "Epoch 70 | Batch 36 | Loss: 0.4884718060493469\n",
      "Epoch 70 | Batch 37 | Loss: 0.31821179389953613\n",
      "Epoch 70 | Batch 38 | Loss: 0.2675340175628662\n",
      "Epoch 70 | Batch 39 | Loss: 0.7095526456832886\n",
      "Epoch 70 | Batch 40 | Loss: 0.3077946901321411\n",
      "Epoch 70 | Batch 41 | Loss: 0.3414947986602783\n",
      "Epoch 70 | Batch 42 | Loss: 0.4325794577598572\n",
      "Epoch 70 | Batch 43 | Loss: 0.3126339912414551\n",
      "Epoch 70 | Batch 44 | Loss: 0.42580536007881165\n",
      "Epoch 70 | Batch 45 | Loss: 0.24983443319797516\n",
      "Epoch 70 | Batch 46 | Loss: 0.4405452013015747\n",
      "Epoch 70 | Batch 47 | Loss: 0.3002932071685791\n",
      "Epoch 70 | Batch 48 | Loss: 0.16270580887794495\n",
      "Epoch 70 | Batch 49 | Loss: 0.31118693947792053\n",
      "Epoch 70 | Batch 50 | Loss: 0.42914825677871704\n",
      "Epoch 70 | Batch 51 | Loss: 0.26303935050964355\n",
      "Epoch 70 | Batch 52 | Loss: 0.24771541357040405\n",
      "Epoch 70 | Batch 53 | Loss: 0.6245700716972351\n",
      "Epoch 70 | Batch 54 | Loss: 0.5444176197052002\n",
      "Epoch 70 | Batch 55 | Loss: 0.21197250485420227\n",
      "Epoch 70 | Batch 56 | Loss: 0.19734880328178406\n",
      "Epoch 70 | Batch 57 | Loss: 0.10044971853494644\n",
      "Epoch 70 | Batch 58 | Loss: 0.43298041820526123\n",
      "Epoch 70 | Batch 59 | Loss: 0.758090615272522\n",
      "Epoch 70 | Batch 60 | Loss: 0.10573502629995346\n",
      "Epoch 70 | Batch 61 | Loss: 0.4754542112350464\n",
      "Epoch 70 | Batch 62 | Loss: 0.11975261569023132\n",
      "Epoch 70 | Batch 63 | Loss: 0.47830021381378174\n",
      "Epoch 70 | Batch 64 | Loss: 0.37404316663742065\n",
      "Epoch 70 | Batch 65 | Loss: 0.11404158174991608\n",
      "Epoch 70 | Batch 66 | Loss: 0.12800580263137817\n",
      "Epoch 70 | Batch 67 | Loss: 0.46212950348854065\n",
      "Epoch 70 | Batch 68 | Loss: 0.07470370084047318\n",
      "Epoch 70 | Batch 69 | Loss: 0.6545664668083191\n",
      "Epoch 70 | Batch 70 | Loss: 0.4227238893508911\n",
      "Epoch 70 | Batch 71 | Loss: 0.4216611087322235\n",
      "Epoch 70 | Batch 72 | Loss: 0.19890610873699188\n",
      "Epoch 70 | Batch 73 | Loss: 0.3090553283691406\n",
      "Epoch 70 | Batch 74 | Loss: 0.13859093189239502\n",
      "Epoch 70 | Batch 75 | Loss: 0.679517388343811\n",
      "Epoch 70 | Batch 76 | Loss: 1.4424322843551636\n",
      "Epoch 70 | Batch 77 | Loss: 0.31604084372520447\n",
      "Epoch 70 | Batch 78 | Loss: 0.4724130630493164\n",
      "Epoch 70 | Batch 79 | Loss: 0.18509957194328308\n",
      "Epoch 70 | Batch 80 | Loss: 0.40052953362464905\n",
      "Epoch 70 | Batch 81 | Loss: 0.2599322199821472\n",
      "Epoch 70 | Batch 82 | Loss: 0.21791526675224304\n",
      "Epoch 70 | Batch 83 | Loss: 0.4413807988166809\n",
      "Epoch 70 | Batch 84 | Loss: 0.3826138377189636\n",
      "Epoch 70 | Batch 85 | Loss: 0.36845749616622925\n",
      "Epoch 70 | Batch 86 | Loss: 0.5494508147239685\n",
      "Epoch 70 | Batch 87 | Loss: 0.20750391483306885\n",
      "Epoch 70 | Batch 88 | Loss: 0.1880185306072235\n",
      "Epoch 70 | Batch 89 | Loss: 0.07634248584508896\n",
      "Epoch 70 | Batch 90 | Loss: 1.0251076221466064\n",
      "Epoch 71 | Batch 1 | Loss: 0.40515896677970886\n",
      "Epoch 71 | Batch 2 | Loss: 0.5510866045951843\n",
      "Epoch 71 | Batch 3 | Loss: 0.3256901800632477\n",
      "Epoch 71 | Batch 4 | Loss: 0.3372242748737335\n",
      "Epoch 71 | Batch 5 | Loss: 0.34591954946517944\n",
      "Epoch 71 | Batch 6 | Loss: 0.9487926959991455\n",
      "Epoch 71 | Batch 7 | Loss: 0.3206661343574524\n",
      "Epoch 71 | Batch 8 | Loss: 0.0669831708073616\n",
      "Epoch 71 | Batch 9 | Loss: 0.30641037225723267\n",
      "Epoch 71 | Batch 10 | Loss: 0.2840549945831299\n",
      "Epoch 71 | Batch 11 | Loss: 0.561025857925415\n",
      "Epoch 71 | Batch 12 | Loss: 0.8104970455169678\n",
      "Epoch 71 | Batch 13 | Loss: 0.5213763117790222\n",
      "Epoch 71 | Batch 14 | Loss: 0.20547975599765778\n",
      "Epoch 71 | Batch 15 | Loss: 0.3632492423057556\n",
      "Epoch 71 | Batch 16 | Loss: 0.27136367559432983\n",
      "Epoch 71 | Batch 17 | Loss: 0.09905895590782166\n",
      "Epoch 71 | Batch 18 | Loss: 0.37242060899734497\n",
      "Epoch 71 | Batch 19 | Loss: 0.24764035642147064\n",
      "Epoch 71 | Batch 20 | Loss: 0.6920983791351318\n",
      "Epoch 71 | Batch 21 | Loss: 0.33824941515922546\n",
      "Epoch 71 | Batch 22 | Loss: 0.44344064593315125\n",
      "Epoch 71 | Batch 23 | Loss: 0.8754254579544067\n",
      "Epoch 71 | Batch 24 | Loss: 0.310930073261261\n",
      "Epoch 71 | Batch 25 | Loss: 0.17805103957653046\n",
      "Epoch 71 | Batch 26 | Loss: 0.31481805443763733\n",
      "Epoch 71 | Batch 27 | Loss: 0.4358004331588745\n",
      "Epoch 71 | Batch 28 | Loss: 0.4711660146713257\n",
      "Epoch 71 | Batch 29 | Loss: 0.45362406969070435\n",
      "Epoch 71 | Batch 30 | Loss: 0.21982450783252716\n",
      "Epoch 71 | Batch 31 | Loss: 0.16699038445949554\n",
      "Epoch 71 | Batch 32 | Loss: 0.7833290100097656\n",
      "Epoch 71 | Batch 33 | Loss: 0.31118059158325195\n",
      "Epoch 71 | Batch 34 | Loss: 0.21007126569747925\n",
      "Epoch 71 | Batch 35 | Loss: 0.21073049306869507\n",
      "Epoch 71 | Batch 36 | Loss: 0.3143208622932434\n",
      "Epoch 71 | Batch 37 | Loss: 0.217863991856575\n",
      "Epoch 71 | Batch 38 | Loss: 0.25437048077583313\n",
      "Epoch 71 | Batch 39 | Loss: 0.9636425971984863\n",
      "Epoch 71 | Batch 40 | Loss: 0.15440040826797485\n",
      "Epoch 71 | Batch 41 | Loss: 0.3740818202495575\n",
      "Epoch 71 | Batch 42 | Loss: 0.4778135418891907\n",
      "Epoch 71 | Batch 43 | Loss: 0.41048893332481384\n",
      "Epoch 71 | Batch 44 | Loss: 0.4129195809364319\n",
      "Epoch 71 | Batch 45 | Loss: 0.2855282127857208\n",
      "Epoch 71 | Batch 46 | Loss: 0.22224493324756622\n",
      "Epoch 71 | Batch 47 | Loss: 0.34554317593574524\n",
      "Epoch 71 | Batch 48 | Loss: 0.3459726870059967\n",
      "Epoch 71 | Batch 49 | Loss: 0.2559407353401184\n",
      "Epoch 71 | Batch 50 | Loss: 1.0528806447982788\n",
      "Epoch 71 | Batch 51 | Loss: 0.13775353133678436\n",
      "Epoch 71 | Batch 52 | Loss: 0.38010746240615845\n",
      "Epoch 71 | Batch 53 | Loss: 0.2832600176334381\n",
      "Epoch 71 | Batch 54 | Loss: 0.351962685585022\n",
      "Epoch 71 | Batch 55 | Loss: 0.17109079658985138\n",
      "Epoch 71 | Batch 56 | Loss: 0.11410219967365265\n",
      "Epoch 71 | Batch 57 | Loss: 0.18182073533535004\n",
      "Epoch 71 | Batch 58 | Loss: 0.35337507724761963\n",
      "Epoch 71 | Batch 59 | Loss: 0.33871832489967346\n",
      "Epoch 71 | Batch 60 | Loss: 0.7585403919219971\n",
      "Epoch 71 | Batch 61 | Loss: 0.07920791208744049\n",
      "Epoch 71 | Batch 62 | Loss: 0.29363805055618286\n",
      "Epoch 71 | Batch 63 | Loss: 0.5917187929153442\n",
      "Epoch 71 | Batch 64 | Loss: 0.36195796728134155\n",
      "Epoch 71 | Batch 65 | Loss: 0.32938945293426514\n",
      "Epoch 71 | Batch 66 | Loss: 0.5036903619766235\n",
      "Epoch 71 | Batch 67 | Loss: 0.287702739238739\n",
      "Epoch 71 | Batch 68 | Loss: 0.3912542462348938\n",
      "Epoch 71 | Batch 69 | Loss: 0.5602514743804932\n",
      "Epoch 71 | Batch 70 | Loss: 0.5302497148513794\n",
      "Epoch 71 | Batch 71 | Loss: 0.14117804169654846\n",
      "Epoch 71 | Batch 72 | Loss: 0.2799721956253052\n",
      "Epoch 71 | Batch 73 | Loss: 0.20620286464691162\n",
      "Epoch 71 | Batch 74 | Loss: 0.16430747509002686\n",
      "Epoch 71 | Batch 75 | Loss: 0.33160296082496643\n",
      "Epoch 71 | Batch 76 | Loss: 0.27156487107276917\n",
      "Epoch 71 | Batch 77 | Loss: 0.09636905789375305\n",
      "Epoch 71 | Batch 78 | Loss: 0.7108563780784607\n",
      "Epoch 71 | Batch 79 | Loss: 0.417749285697937\n",
      "Epoch 71 | Batch 80 | Loss: 0.5406684875488281\n",
      "Epoch 71 | Batch 81 | Loss: 0.28300341963768005\n",
      "Epoch 71 | Batch 82 | Loss: 0.26832345128059387\n",
      "Epoch 71 | Batch 83 | Loss: 0.2515331506729126\n",
      "Epoch 71 | Batch 84 | Loss: 0.10415920615196228\n",
      "Epoch 71 | Batch 85 | Loss: 0.29445749521255493\n",
      "Epoch 71 | Batch 86 | Loss: 0.788446843624115\n",
      "Epoch 71 | Batch 87 | Loss: 0.3411758840084076\n",
      "Epoch 71 | Batch 88 | Loss: 0.2832269072532654\n",
      "Epoch 71 | Batch 89 | Loss: 0.3447941541671753\n",
      "Epoch 71 | Batch 90 | Loss: 1.4486943483352661\n",
      "Epoch 72 | Batch 1 | Loss: 0.16991430521011353\n",
      "Epoch 72 | Batch 2 | Loss: 0.16891571879386902\n",
      "Epoch 72 | Batch 3 | Loss: 0.23851278424263\n",
      "Epoch 72 | Batch 4 | Loss: 0.3352760374546051\n",
      "Epoch 72 | Batch 5 | Loss: 0.4317086935043335\n",
      "Epoch 72 | Batch 6 | Loss: 0.3729929029941559\n",
      "Epoch 72 | Batch 7 | Loss: 0.5988532304763794\n",
      "Epoch 72 | Batch 8 | Loss: 0.4979514181613922\n",
      "Epoch 72 | Batch 9 | Loss: 0.26429009437561035\n",
      "Epoch 72 | Batch 10 | Loss: 0.16545531153678894\n",
      "Epoch 72 | Batch 11 | Loss: 0.7418147921562195\n",
      "Epoch 72 | Batch 12 | Loss: 0.661885142326355\n",
      "Epoch 72 | Batch 13 | Loss: 0.22836646437644958\n",
      "Epoch 72 | Batch 14 | Loss: 0.37306976318359375\n",
      "Epoch 72 | Batch 15 | Loss: 1.037708044052124\n",
      "Epoch 72 | Batch 16 | Loss: 0.2821160852909088\n",
      "Epoch 72 | Batch 17 | Loss: 0.4399186670780182\n",
      "Epoch 72 | Batch 18 | Loss: 0.36765146255493164\n",
      "Epoch 72 | Batch 19 | Loss: 0.5534602999687195\n",
      "Epoch 72 | Batch 20 | Loss: 0.527527928352356\n",
      "Epoch 72 | Batch 21 | Loss: 0.3912822902202606\n",
      "Epoch 72 | Batch 22 | Loss: 0.22145295143127441\n",
      "Epoch 72 | Batch 23 | Loss: 0.5408163070678711\n",
      "Epoch 72 | Batch 24 | Loss: 0.1662883758544922\n",
      "Epoch 72 | Batch 25 | Loss: 0.4392732083797455\n",
      "Epoch 72 | Batch 26 | Loss: 0.6872118711471558\n",
      "Epoch 72 | Batch 27 | Loss: 0.5683274269104004\n",
      "Epoch 72 | Batch 28 | Loss: 0.44096288084983826\n",
      "Epoch 72 | Batch 29 | Loss: 0.7014122605323792\n",
      "Epoch 72 | Batch 30 | Loss: 0.5489233136177063\n",
      "Epoch 72 | Batch 31 | Loss: 0.20902779698371887\n",
      "Epoch 72 | Batch 32 | Loss: 0.486678808927536\n",
      "Epoch 72 | Batch 33 | Loss: 0.21098247170448303\n",
      "Epoch 72 | Batch 34 | Loss: 0.11606895923614502\n",
      "Epoch 72 | Batch 35 | Loss: 0.5230869054794312\n",
      "Epoch 72 | Batch 36 | Loss: 0.24410110712051392\n",
      "Epoch 72 | Batch 37 | Loss: 0.1043189987540245\n",
      "Epoch 72 | Batch 38 | Loss: 0.5617578029632568\n",
      "Epoch 72 | Batch 39 | Loss: 0.22260499000549316\n",
      "Epoch 72 | Batch 40 | Loss: 0.6022096276283264\n",
      "Epoch 72 | Batch 41 | Loss: 0.09605003893375397\n",
      "Epoch 72 | Batch 42 | Loss: 0.4314604699611664\n",
      "Epoch 72 | Batch 43 | Loss: 0.4288763403892517\n",
      "Epoch 72 | Batch 44 | Loss: 0.47292906045913696\n",
      "Epoch 72 | Batch 45 | Loss: 0.70550137758255\n",
      "Epoch 72 | Batch 46 | Loss: 0.35963940620422363\n",
      "Epoch 72 | Batch 47 | Loss: 0.18110348284244537\n",
      "Epoch 72 | Batch 48 | Loss: 0.23923543095588684\n",
      "Epoch 72 | Batch 49 | Loss: 0.4554430842399597\n",
      "Epoch 72 | Batch 50 | Loss: 0.3769572675228119\n",
      "Epoch 72 | Batch 51 | Loss: 0.3504177927970886\n",
      "Epoch 72 | Batch 52 | Loss: 0.320894718170166\n",
      "Epoch 72 | Batch 53 | Loss: 0.5354071259498596\n",
      "Epoch 72 | Batch 54 | Loss: 0.11128054559230804\n",
      "Epoch 72 | Batch 55 | Loss: 0.4680655896663666\n",
      "Epoch 72 | Batch 56 | Loss: 0.14285695552825928\n",
      "Epoch 72 | Batch 57 | Loss: 0.5216548442840576\n",
      "Epoch 72 | Batch 58 | Loss: 0.21378622949123383\n",
      "Epoch 72 | Batch 59 | Loss: 0.4187058210372925\n",
      "Epoch 72 | Batch 60 | Loss: 0.30148011445999146\n",
      "Epoch 72 | Batch 61 | Loss: 0.7636645436286926\n",
      "Epoch 72 | Batch 62 | Loss: 0.2528708875179291\n",
      "Epoch 72 | Batch 63 | Loss: 0.7567546367645264\n",
      "Epoch 72 | Batch 64 | Loss: 0.1416890025138855\n",
      "Epoch 72 | Batch 65 | Loss: 0.32587724924087524\n",
      "Epoch 72 | Batch 66 | Loss: 0.3431552052497864\n",
      "Epoch 72 | Batch 67 | Loss: 0.18253488838672638\n",
      "Epoch 72 | Batch 68 | Loss: 0.23031561076641083\n",
      "Epoch 72 | Batch 69 | Loss: 0.626605212688446\n",
      "Epoch 72 | Batch 70 | Loss: 0.4242822825908661\n",
      "Epoch 72 | Batch 71 | Loss: 0.096366748213768\n",
      "Epoch 72 | Batch 72 | Loss: 0.4160897731781006\n",
      "Epoch 72 | Batch 73 | Loss: 0.6528760194778442\n",
      "Epoch 72 | Batch 74 | Loss: 0.17447510361671448\n",
      "Epoch 72 | Batch 75 | Loss: 0.6988803744316101\n",
      "Epoch 72 | Batch 76 | Loss: 0.40335792303085327\n",
      "Epoch 72 | Batch 77 | Loss: 0.25895801186561584\n",
      "Epoch 72 | Batch 78 | Loss: 0.4003245234489441\n",
      "Epoch 72 | Batch 79 | Loss: 0.43945789337158203\n",
      "Epoch 72 | Batch 80 | Loss: 0.3022913336753845\n",
      "Epoch 72 | Batch 81 | Loss: 0.4406317174434662\n",
      "Epoch 72 | Batch 82 | Loss: 0.5287697911262512\n",
      "Epoch 72 | Batch 83 | Loss: 0.16336508095264435\n",
      "Epoch 72 | Batch 84 | Loss: 0.13744693994522095\n",
      "Epoch 72 | Batch 85 | Loss: 0.30094894766807556\n",
      "Epoch 72 | Batch 86 | Loss: 0.5162825584411621\n",
      "Epoch 72 | Batch 87 | Loss: 0.46586060523986816\n",
      "Epoch 72 | Batch 88 | Loss: 0.4422414004802704\n",
      "Epoch 72 | Batch 89 | Loss: 0.6461772918701172\n",
      "Epoch 72 | Batch 90 | Loss: 0.1664523333311081\n",
      "Epoch 73 | Batch 1 | Loss: 0.2756028175354004\n",
      "Epoch 73 | Batch 2 | Loss: 0.379343181848526\n",
      "Epoch 73 | Batch 3 | Loss: 0.3139444589614868\n",
      "Epoch 73 | Batch 4 | Loss: 0.4505312740802765\n",
      "Epoch 73 | Batch 5 | Loss: 0.3892117738723755\n",
      "Epoch 73 | Batch 6 | Loss: 0.3488336205482483\n",
      "Epoch 73 | Batch 7 | Loss: 0.7167419791221619\n",
      "Epoch 73 | Batch 8 | Loss: 0.16777417063713074\n",
      "Epoch 73 | Batch 9 | Loss: 0.3460786044597626\n",
      "Epoch 73 | Batch 10 | Loss: 0.7188750505447388\n",
      "Epoch 73 | Batch 11 | Loss: 0.0994286984205246\n",
      "Epoch 73 | Batch 12 | Loss: 0.5743786096572876\n",
      "Epoch 73 | Batch 13 | Loss: 0.8649574518203735\n",
      "Epoch 73 | Batch 14 | Loss: 0.22978195548057556\n",
      "Epoch 73 | Batch 15 | Loss: 0.36209022998809814\n",
      "Epoch 73 | Batch 16 | Loss: 0.622433602809906\n",
      "Epoch 73 | Batch 17 | Loss: 0.15777285397052765\n",
      "Epoch 73 | Batch 18 | Loss: 0.21620014309883118\n",
      "Epoch 73 | Batch 19 | Loss: 0.18511053919792175\n",
      "Epoch 73 | Batch 20 | Loss: 0.15614597499370575\n",
      "Epoch 73 | Batch 21 | Loss: 0.22989895939826965\n",
      "Epoch 73 | Batch 22 | Loss: 0.16376756131649017\n",
      "Epoch 73 | Batch 23 | Loss: 0.24650093913078308\n",
      "Epoch 73 | Batch 24 | Loss: 0.14173623919487\n",
      "Epoch 73 | Batch 25 | Loss: 0.2140887975692749\n",
      "Epoch 73 | Batch 26 | Loss: 0.14170140027999878\n",
      "Epoch 73 | Batch 27 | Loss: 0.5952153205871582\n",
      "Epoch 73 | Batch 28 | Loss: 0.11404106020927429\n",
      "Epoch 73 | Batch 29 | Loss: 0.11068051308393478\n",
      "Epoch 73 | Batch 30 | Loss: 0.2401534616947174\n",
      "Epoch 73 | Batch 31 | Loss: 0.556490421295166\n",
      "Epoch 73 | Batch 32 | Loss: 0.34428131580352783\n",
      "Epoch 73 | Batch 33 | Loss: 0.5493409037590027\n",
      "Epoch 73 | Batch 34 | Loss: 0.11224072426557541\n",
      "Epoch 73 | Batch 35 | Loss: 0.8528101444244385\n",
      "Epoch 73 | Batch 36 | Loss: 0.3846535384654999\n",
      "Epoch 73 | Batch 37 | Loss: 0.3598802387714386\n",
      "Epoch 73 | Batch 38 | Loss: 0.13291098177433014\n",
      "Epoch 73 | Batch 39 | Loss: 0.27642035484313965\n",
      "Epoch 73 | Batch 40 | Loss: 0.5678694844245911\n",
      "Epoch 73 | Batch 41 | Loss: 0.11824323236942291\n",
      "Epoch 73 | Batch 42 | Loss: 0.4723939299583435\n",
      "Epoch 73 | Batch 43 | Loss: 0.4737253189086914\n",
      "Epoch 73 | Batch 44 | Loss: 0.38600677251815796\n",
      "Epoch 73 | Batch 45 | Loss: 0.46039193868637085\n",
      "Epoch 73 | Batch 46 | Loss: 0.1626763939857483\n",
      "Epoch 73 | Batch 47 | Loss: 0.12373729795217514\n",
      "Epoch 73 | Batch 48 | Loss: 0.2874768376350403\n",
      "Epoch 73 | Batch 49 | Loss: 0.22097785770893097\n",
      "Epoch 73 | Batch 50 | Loss: 0.24022668600082397\n",
      "Epoch 73 | Batch 51 | Loss: 0.5136033892631531\n",
      "Epoch 73 | Batch 52 | Loss: 0.09240590780973434\n",
      "Epoch 73 | Batch 53 | Loss: 0.23464784026145935\n",
      "Epoch 73 | Batch 54 | Loss: 0.40911760926246643\n",
      "Epoch 73 | Batch 55 | Loss: 0.6397271156311035\n",
      "Epoch 73 | Batch 56 | Loss: 0.12361227720975876\n",
      "Epoch 73 | Batch 57 | Loss: 0.5438560247421265\n",
      "Epoch 73 | Batch 58 | Loss: 0.3406582474708557\n",
      "Epoch 73 | Batch 59 | Loss: 0.3544107675552368\n",
      "Epoch 73 | Batch 60 | Loss: 0.2508629858493805\n",
      "Epoch 73 | Batch 61 | Loss: 0.5215057730674744\n",
      "Epoch 73 | Batch 62 | Loss: 0.19204169511795044\n",
      "Epoch 73 | Batch 63 | Loss: 0.307517945766449\n",
      "Epoch 73 | Batch 64 | Loss: 0.5183231234550476\n",
      "Epoch 73 | Batch 65 | Loss: 0.7005857229232788\n",
      "Epoch 73 | Batch 66 | Loss: 0.7432348728179932\n",
      "Epoch 73 | Batch 67 | Loss: 0.2176068127155304\n",
      "Epoch 73 | Batch 68 | Loss: 0.5311373472213745\n",
      "Epoch 73 | Batch 69 | Loss: 0.6491310596466064\n",
      "Epoch 73 | Batch 70 | Loss: 0.17412035167217255\n",
      "Epoch 73 | Batch 71 | Loss: 0.70496666431427\n",
      "Epoch 73 | Batch 72 | Loss: 0.1935221552848816\n",
      "Epoch 73 | Batch 73 | Loss: 0.2782599925994873\n",
      "Epoch 73 | Batch 74 | Loss: 0.1765497922897339\n",
      "Epoch 73 | Batch 75 | Loss: 0.23554985225200653\n",
      "Epoch 73 | Batch 76 | Loss: 0.23485249280929565\n",
      "Epoch 73 | Batch 77 | Loss: 0.32556551694869995\n",
      "Epoch 73 | Batch 78 | Loss: 0.7018040418624878\n",
      "Epoch 73 | Batch 79 | Loss: 0.34755775332450867\n",
      "Epoch 73 | Batch 80 | Loss: 0.49564099311828613\n",
      "Epoch 73 | Batch 81 | Loss: 0.600100040435791\n",
      "Epoch 73 | Batch 82 | Loss: 0.6161576509475708\n",
      "Epoch 73 | Batch 83 | Loss: 0.1771702766418457\n",
      "Epoch 73 | Batch 84 | Loss: 0.46266600489616394\n",
      "Epoch 73 | Batch 85 | Loss: 0.6180415749549866\n",
      "Epoch 73 | Batch 86 | Loss: 0.43966957926750183\n",
      "Epoch 73 | Batch 87 | Loss: 0.1869223713874817\n",
      "Epoch 73 | Batch 88 | Loss: 0.2875160574913025\n",
      "Epoch 73 | Batch 89 | Loss: 0.36675453186035156\n",
      "Epoch 73 | Batch 90 | Loss: 0.20369119942188263\n",
      "Epoch 74 | Batch 1 | Loss: 0.31121546030044556\n",
      "Epoch 74 | Batch 2 | Loss: 0.6769042015075684\n",
      "Epoch 74 | Batch 3 | Loss: 0.14579565823078156\n",
      "Epoch 74 | Batch 4 | Loss: 0.3903234004974365\n",
      "Epoch 74 | Batch 5 | Loss: 0.3403117060661316\n",
      "Epoch 74 | Batch 6 | Loss: 0.4719529151916504\n",
      "Epoch 74 | Batch 7 | Loss: 0.48938116431236267\n",
      "Epoch 74 | Batch 8 | Loss: 0.1327245533466339\n",
      "Epoch 74 | Batch 9 | Loss: 0.2945159077644348\n",
      "Epoch 74 | Batch 10 | Loss: 0.4909052550792694\n",
      "Epoch 74 | Batch 11 | Loss: 0.4688073992729187\n",
      "Epoch 74 | Batch 12 | Loss: 0.3505699336528778\n",
      "Epoch 74 | Batch 13 | Loss: 0.16370101273059845\n",
      "Epoch 74 | Batch 14 | Loss: 0.11219136416912079\n",
      "Epoch 74 | Batch 15 | Loss: 0.29777276515960693\n",
      "Epoch 74 | Batch 16 | Loss: 0.4679517447948456\n",
      "Epoch 74 | Batch 17 | Loss: 0.09803619235754013\n",
      "Epoch 74 | Batch 18 | Loss: 0.44561001658439636\n",
      "Epoch 74 | Batch 19 | Loss: 0.1628861129283905\n",
      "Epoch 74 | Batch 20 | Loss: 0.330233097076416\n",
      "Epoch 74 | Batch 21 | Loss: 0.49092477560043335\n",
      "Epoch 74 | Batch 22 | Loss: 0.5228753685951233\n",
      "Epoch 74 | Batch 23 | Loss: 0.9740335941314697\n",
      "Epoch 74 | Batch 24 | Loss: 0.5881723761558533\n",
      "Epoch 74 | Batch 25 | Loss: 0.325131356716156\n",
      "Epoch 74 | Batch 26 | Loss: 0.2893020212650299\n",
      "Epoch 74 | Batch 27 | Loss: 0.5236251354217529\n",
      "Epoch 74 | Batch 28 | Loss: 0.12336479127407074\n",
      "Epoch 74 | Batch 29 | Loss: 0.1979369968175888\n",
      "Epoch 74 | Batch 30 | Loss: 0.4127770662307739\n",
      "Epoch 74 | Batch 31 | Loss: 0.5358538627624512\n",
      "Epoch 74 | Batch 32 | Loss: 0.11054351925849915\n",
      "Epoch 74 | Batch 33 | Loss: 0.27473050355911255\n",
      "Epoch 74 | Batch 34 | Loss: 0.38326770067214966\n",
      "Epoch 74 | Batch 35 | Loss: 0.2339080274105072\n",
      "Epoch 74 | Batch 36 | Loss: 1.1951262950897217\n",
      "Epoch 74 | Batch 37 | Loss: 0.5531625151634216\n",
      "Epoch 74 | Batch 38 | Loss: 0.3324226140975952\n",
      "Epoch 74 | Batch 39 | Loss: 0.526799201965332\n",
      "Epoch 74 | Batch 40 | Loss: 0.22618547081947327\n",
      "Epoch 74 | Batch 41 | Loss: 0.16627857089042664\n",
      "Epoch 74 | Batch 42 | Loss: 0.33216941356658936\n",
      "Epoch 74 | Batch 43 | Loss: 0.15155024826526642\n",
      "Epoch 74 | Batch 44 | Loss: 0.752813458442688\n",
      "Epoch 74 | Batch 45 | Loss: 0.08455273509025574\n",
      "Epoch 74 | Batch 46 | Loss: 0.259168803691864\n",
      "Epoch 74 | Batch 47 | Loss: 0.29784584045410156\n",
      "Epoch 74 | Batch 48 | Loss: 0.2058859020471573\n",
      "Epoch 74 | Batch 49 | Loss: 0.2595747709274292\n",
      "Epoch 74 | Batch 50 | Loss: 0.25381433963775635\n",
      "Epoch 74 | Batch 51 | Loss: 0.49612316489219666\n",
      "Epoch 74 | Batch 52 | Loss: 0.7313873171806335\n",
      "Epoch 74 | Batch 53 | Loss: 0.10876785218715668\n",
      "Epoch 74 | Batch 54 | Loss: 0.24609380960464478\n",
      "Epoch 74 | Batch 55 | Loss: 0.22707131505012512\n",
      "Epoch 74 | Batch 56 | Loss: 0.2186681628227234\n",
      "Epoch 74 | Batch 57 | Loss: 0.43961161375045776\n",
      "Epoch 74 | Batch 58 | Loss: 0.3914797008037567\n",
      "Epoch 74 | Batch 59 | Loss: 0.5026742815971375\n",
      "Epoch 74 | Batch 60 | Loss: 0.5039858818054199\n",
      "Epoch 74 | Batch 61 | Loss: 0.6768509149551392\n",
      "Epoch 74 | Batch 62 | Loss: 0.2456602305173874\n",
      "Epoch 74 | Batch 63 | Loss: 0.40617120265960693\n",
      "Epoch 74 | Batch 64 | Loss: 0.2933621406555176\n",
      "Epoch 74 | Batch 65 | Loss: 0.24579623341560364\n",
      "Epoch 74 | Batch 66 | Loss: 0.8464733958244324\n",
      "Epoch 74 | Batch 67 | Loss: 0.23091740906238556\n",
      "Epoch 74 | Batch 68 | Loss: 0.37771499156951904\n",
      "Epoch 74 | Batch 69 | Loss: 0.5438409447669983\n",
      "Epoch 74 | Batch 70 | Loss: 0.4476082921028137\n",
      "Epoch 74 | Batch 71 | Loss: 0.2763817012310028\n",
      "Epoch 74 | Batch 72 | Loss: 0.19525516033172607\n",
      "Epoch 74 | Batch 73 | Loss: 0.20987752079963684\n",
      "Epoch 74 | Batch 74 | Loss: 0.14607667922973633\n",
      "Epoch 74 | Batch 75 | Loss: 0.507006049156189\n",
      "Epoch 74 | Batch 76 | Loss: 0.22276487946510315\n",
      "Epoch 74 | Batch 77 | Loss: 0.34499916434288025\n",
      "Epoch 74 | Batch 78 | Loss: 0.27423006296157837\n",
      "Epoch 74 | Batch 79 | Loss: 0.11305873095989227\n",
      "Epoch 74 | Batch 80 | Loss: 0.32191917300224304\n",
      "Epoch 74 | Batch 81 | Loss: 0.632038414478302\n",
      "Epoch 74 | Batch 82 | Loss: 0.2795456647872925\n",
      "Epoch 74 | Batch 83 | Loss: 0.5044420957565308\n",
      "Epoch 74 | Batch 84 | Loss: 0.5617392063140869\n",
      "Epoch 74 | Batch 85 | Loss: 0.0630037933588028\n",
      "Epoch 74 | Batch 86 | Loss: 0.10372031480073929\n",
      "Epoch 74 | Batch 87 | Loss: 1.093126893043518\n",
      "Epoch 74 | Batch 88 | Loss: 0.34503430128097534\n",
      "Epoch 74 | Batch 89 | Loss: 0.19106432795524597\n",
      "Epoch 74 | Batch 90 | Loss: 0.6451606154441833\n",
      "Epoch 75 | Batch 1 | Loss: 0.27093109488487244\n",
      "Epoch 75 | Batch 2 | Loss: 0.08868597447872162\n",
      "Epoch 75 | Batch 3 | Loss: 0.4874981939792633\n",
      "Epoch 75 | Batch 4 | Loss: 0.33214932680130005\n",
      "Epoch 75 | Batch 5 | Loss: 0.2125139832496643\n",
      "Epoch 75 | Batch 6 | Loss: 0.29095137119293213\n",
      "Epoch 75 | Batch 7 | Loss: 0.3807584345340729\n",
      "Epoch 75 | Batch 8 | Loss: 0.13031888008117676\n",
      "Epoch 75 | Batch 9 | Loss: 0.3448156714439392\n",
      "Epoch 75 | Batch 10 | Loss: 0.5392149686813354\n",
      "Epoch 75 | Batch 11 | Loss: 0.587578296661377\n",
      "Epoch 75 | Batch 12 | Loss: 0.09213538467884064\n",
      "Epoch 75 | Batch 13 | Loss: 0.5582729578018188\n",
      "Epoch 75 | Batch 14 | Loss: 0.43453970551490784\n",
      "Epoch 75 | Batch 15 | Loss: 0.955235481262207\n",
      "Epoch 75 | Batch 16 | Loss: 0.29440948367118835\n",
      "Epoch 75 | Batch 17 | Loss: 0.4330681562423706\n",
      "Epoch 75 | Batch 18 | Loss: 0.08640524744987488\n",
      "Epoch 75 | Batch 19 | Loss: 0.4708597660064697\n",
      "Epoch 75 | Batch 20 | Loss: 0.8472747802734375\n",
      "Epoch 75 | Batch 21 | Loss: 0.1797921061515808\n",
      "Epoch 75 | Batch 22 | Loss: 0.25373685359954834\n",
      "Epoch 75 | Batch 23 | Loss: 0.699912965297699\n",
      "Epoch 75 | Batch 24 | Loss: 0.3277962803840637\n",
      "Epoch 75 | Batch 25 | Loss: 0.44415879249572754\n",
      "Epoch 75 | Batch 26 | Loss: 0.10100014507770538\n",
      "Epoch 75 | Batch 27 | Loss: 0.2816968858242035\n",
      "Epoch 75 | Batch 28 | Loss: 0.35843342542648315\n",
      "Epoch 75 | Batch 29 | Loss: 0.44439417123794556\n",
      "Epoch 75 | Batch 30 | Loss: 0.3224937617778778\n",
      "Epoch 75 | Batch 31 | Loss: 0.22970178723335266\n",
      "Epoch 75 | Batch 32 | Loss: 0.1784932017326355\n",
      "Epoch 75 | Batch 33 | Loss: 0.514337956905365\n",
      "Epoch 75 | Batch 34 | Loss: 0.40524476766586304\n",
      "Epoch 75 | Batch 35 | Loss: 0.31477367877960205\n",
      "Epoch 75 | Batch 36 | Loss: 0.18870297074317932\n",
      "Epoch 75 | Batch 37 | Loss: 0.138825923204422\n",
      "Epoch 75 | Batch 38 | Loss: 0.06781549006700516\n",
      "Epoch 75 | Batch 39 | Loss: 0.42648667097091675\n",
      "Epoch 75 | Batch 40 | Loss: 0.4967625141143799\n",
      "Epoch 75 | Batch 41 | Loss: 0.16428051888942719\n",
      "Epoch 75 | Batch 42 | Loss: 0.28133052587509155\n",
      "Epoch 75 | Batch 43 | Loss: 0.423611044883728\n",
      "Epoch 75 | Batch 44 | Loss: 0.3107984662055969\n",
      "Epoch 75 | Batch 45 | Loss: 0.44490846991539\n",
      "Epoch 75 | Batch 46 | Loss: 0.21244920790195465\n",
      "Epoch 75 | Batch 47 | Loss: 0.4215981662273407\n",
      "Epoch 75 | Batch 48 | Loss: 0.20311856269836426\n",
      "Epoch 75 | Batch 49 | Loss: 0.1949974000453949\n",
      "Epoch 75 | Batch 50 | Loss: 0.46656039357185364\n",
      "Epoch 75 | Batch 51 | Loss: 0.9059025645256042\n",
      "Epoch 75 | Batch 52 | Loss: 0.22516757249832153\n",
      "Epoch 75 | Batch 53 | Loss: 0.5700840950012207\n",
      "Epoch 75 | Batch 54 | Loss: 0.13821682333946228\n",
      "Epoch 75 | Batch 55 | Loss: 0.6081956624984741\n",
      "Epoch 75 | Batch 56 | Loss: 0.569091796875\n",
      "Epoch 75 | Batch 57 | Loss: 0.3422714173793793\n",
      "Epoch 75 | Batch 58 | Loss: 0.2588151693344116\n",
      "Epoch 75 | Batch 59 | Loss: 0.7733677625656128\n",
      "Epoch 75 | Batch 60 | Loss: 0.14000734686851501\n",
      "Epoch 75 | Batch 61 | Loss: 0.4456789195537567\n",
      "Epoch 75 | Batch 62 | Loss: 0.5286788940429688\n",
      "Epoch 75 | Batch 63 | Loss: 0.361630380153656\n",
      "Epoch 75 | Batch 64 | Loss: 0.23806054890155792\n",
      "Epoch 75 | Batch 65 | Loss: 0.08861380815505981\n",
      "Epoch 75 | Batch 66 | Loss: 0.19474855065345764\n",
      "Epoch 75 | Batch 67 | Loss: 0.39054661989212036\n",
      "Epoch 75 | Batch 68 | Loss: 0.16714170575141907\n",
      "Epoch 75 | Batch 69 | Loss: 0.34814268350601196\n",
      "Epoch 75 | Batch 70 | Loss: 0.20880800485610962\n",
      "Epoch 75 | Batch 71 | Loss: 0.45514532923698425\n",
      "Epoch 75 | Batch 72 | Loss: 0.2697690725326538\n",
      "Epoch 75 | Batch 73 | Loss: 0.5356401205062866\n",
      "Epoch 75 | Batch 74 | Loss: 0.20258252322673798\n",
      "Epoch 75 | Batch 75 | Loss: 0.4853438138961792\n",
      "Epoch 75 | Batch 76 | Loss: 0.4187861680984497\n",
      "Epoch 75 | Batch 77 | Loss: 0.18355944752693176\n",
      "Epoch 75 | Batch 78 | Loss: 0.6308289766311646\n",
      "Epoch 75 | Batch 79 | Loss: 0.11176340281963348\n",
      "Epoch 75 | Batch 80 | Loss: 0.40016284584999084\n",
      "Epoch 75 | Batch 81 | Loss: 0.387614369392395\n",
      "Epoch 75 | Batch 82 | Loss: 0.47496625781059265\n",
      "Epoch 75 | Batch 83 | Loss: 0.4662162661552429\n",
      "Epoch 75 | Batch 84 | Loss: 0.40787002444267273\n",
      "Epoch 75 | Batch 85 | Loss: 0.46005019545555115\n",
      "Epoch 75 | Batch 86 | Loss: 0.10114340484142303\n",
      "Epoch 75 | Batch 87 | Loss: 0.47477540373802185\n",
      "Epoch 75 | Batch 88 | Loss: 0.2551654875278473\n",
      "Epoch 75 | Batch 89 | Loss: 0.6769136190414429\n",
      "Epoch 75 | Batch 90 | Loss: 0.22968530654907227\n",
      "Epoch 76 | Batch 1 | Loss: 0.28594011068344116\n",
      "Epoch 76 | Batch 2 | Loss: 0.2172131985425949\n",
      "Epoch 76 | Batch 3 | Loss: 0.2020765095949173\n",
      "Epoch 76 | Batch 4 | Loss: 0.43261298537254333\n",
      "Epoch 76 | Batch 5 | Loss: 0.7645357251167297\n",
      "Epoch 76 | Batch 6 | Loss: 0.13084718585014343\n",
      "Epoch 76 | Batch 7 | Loss: 0.17238202691078186\n",
      "Epoch 76 | Batch 8 | Loss: 0.34212520718574524\n",
      "Epoch 76 | Batch 9 | Loss: 0.2984549105167389\n",
      "Epoch 76 | Batch 10 | Loss: 0.3598761558532715\n",
      "Epoch 76 | Batch 11 | Loss: 0.44334110617637634\n",
      "Epoch 76 | Batch 12 | Loss: 0.16738905012607574\n",
      "Epoch 76 | Batch 13 | Loss: 0.5055638551712036\n",
      "Epoch 76 | Batch 14 | Loss: 0.35196512937545776\n",
      "Epoch 76 | Batch 15 | Loss: 0.18740373849868774\n",
      "Epoch 76 | Batch 16 | Loss: 0.15526118874549866\n",
      "Epoch 76 | Batch 17 | Loss: 0.4325435757637024\n",
      "Epoch 76 | Batch 18 | Loss: 0.47113487124443054\n",
      "Epoch 76 | Batch 19 | Loss: 0.21544167399406433\n",
      "Epoch 76 | Batch 20 | Loss: 0.3213343322277069\n",
      "Epoch 76 | Batch 21 | Loss: 0.5283277034759521\n",
      "Epoch 76 | Batch 22 | Loss: 0.5231099724769592\n",
      "Epoch 76 | Batch 23 | Loss: 0.14228010177612305\n",
      "Epoch 76 | Batch 24 | Loss: 0.6501787304878235\n",
      "Epoch 76 | Batch 25 | Loss: 0.3504452705383301\n",
      "Epoch 76 | Batch 26 | Loss: 0.4513722062110901\n",
      "Epoch 76 | Batch 27 | Loss: 0.2578091621398926\n",
      "Epoch 76 | Batch 28 | Loss: 0.2667188048362732\n",
      "Epoch 76 | Batch 29 | Loss: 0.072188600897789\n",
      "Epoch 76 | Batch 30 | Loss: 0.5671191811561584\n",
      "Epoch 76 | Batch 31 | Loss: 0.15493009984493256\n",
      "Epoch 76 | Batch 32 | Loss: 0.2148478627204895\n",
      "Epoch 76 | Batch 33 | Loss: 0.31571143865585327\n",
      "Epoch 76 | Batch 34 | Loss: 0.2102910876274109\n",
      "Epoch 76 | Batch 35 | Loss: 0.18301710486412048\n",
      "Epoch 76 | Batch 36 | Loss: 0.7074519395828247\n",
      "Epoch 76 | Batch 37 | Loss: 0.4100034832954407\n",
      "Epoch 76 | Batch 38 | Loss: 0.14644894003868103\n",
      "Epoch 76 | Batch 39 | Loss: 0.46842414140701294\n",
      "Epoch 76 | Batch 40 | Loss: 0.70054030418396\n",
      "Epoch 76 | Batch 41 | Loss: 0.48164102435112\n",
      "Epoch 76 | Batch 42 | Loss: 0.21292290091514587\n",
      "Epoch 76 | Batch 43 | Loss: 0.24141356348991394\n",
      "Epoch 76 | Batch 44 | Loss: 0.7125141024589539\n",
      "Epoch 76 | Batch 45 | Loss: 0.33175182342529297\n",
      "Epoch 76 | Batch 46 | Loss: 0.12360314279794693\n",
      "Epoch 76 | Batch 47 | Loss: 0.4355125427246094\n",
      "Epoch 76 | Batch 48 | Loss: 0.216648668050766\n",
      "Epoch 76 | Batch 49 | Loss: 0.4055096209049225\n",
      "Epoch 76 | Batch 50 | Loss: 0.7277681827545166\n",
      "Epoch 76 | Batch 51 | Loss: 0.6914454102516174\n",
      "Epoch 76 | Batch 52 | Loss: 0.4199710786342621\n",
      "Epoch 76 | Batch 53 | Loss: 0.1096033975481987\n",
      "Epoch 76 | Batch 54 | Loss: 0.13698983192443848\n",
      "Epoch 76 | Batch 55 | Loss: 0.24013417959213257\n",
      "Epoch 76 | Batch 56 | Loss: 0.372025728225708\n",
      "Epoch 76 | Batch 57 | Loss: 0.6025874018669128\n",
      "Epoch 76 | Batch 58 | Loss: 0.4830159842967987\n",
      "Epoch 76 | Batch 59 | Loss: 0.13396579027175903\n",
      "Epoch 76 | Batch 60 | Loss: 0.6208029985427856\n",
      "Epoch 76 | Batch 61 | Loss: 0.11877001821994781\n",
      "Epoch 76 | Batch 62 | Loss: 0.7388983964920044\n",
      "Epoch 76 | Batch 63 | Loss: 0.24362947046756744\n",
      "Epoch 76 | Batch 64 | Loss: 0.2925652861595154\n",
      "Epoch 76 | Batch 65 | Loss: 0.49064764380455017\n",
      "Epoch 76 | Batch 66 | Loss: 0.6575409770011902\n",
      "Epoch 76 | Batch 67 | Loss: 0.7858951091766357\n",
      "Epoch 76 | Batch 68 | Loss: 0.41665247082710266\n",
      "Epoch 76 | Batch 69 | Loss: 0.15775331854820251\n",
      "Epoch 76 | Batch 70 | Loss: 0.45964112877845764\n",
      "Epoch 76 | Batch 71 | Loss: 0.5526314377784729\n",
      "Epoch 76 | Batch 72 | Loss: 0.3719264268875122\n",
      "Epoch 76 | Batch 73 | Loss: 0.20940491557121277\n",
      "Epoch 76 | Batch 74 | Loss: 0.4479246139526367\n",
      "Epoch 76 | Batch 75 | Loss: 0.3231889307498932\n",
      "Epoch 76 | Batch 76 | Loss: 0.47288525104522705\n",
      "Epoch 76 | Batch 77 | Loss: 0.4074968099594116\n",
      "Epoch 76 | Batch 78 | Loss: 0.19144174456596375\n",
      "Epoch 76 | Batch 79 | Loss: 0.3772861361503601\n",
      "Epoch 76 | Batch 80 | Loss: 0.17967355251312256\n",
      "Epoch 76 | Batch 81 | Loss: 0.2901683449745178\n",
      "Epoch 76 | Batch 82 | Loss: 0.1554204672574997\n",
      "Epoch 76 | Batch 83 | Loss: 0.4748028516769409\n",
      "Epoch 76 | Batch 84 | Loss: 0.3649914562702179\n",
      "Epoch 76 | Batch 85 | Loss: 0.49460333585739136\n",
      "Epoch 76 | Batch 86 | Loss: 0.40320056676864624\n",
      "Epoch 76 | Batch 87 | Loss: 0.21058470010757446\n",
      "Epoch 76 | Batch 88 | Loss: 0.29710060358047485\n",
      "Epoch 76 | Batch 89 | Loss: 0.13241046667099\n",
      "Epoch 76 | Batch 90 | Loss: 1.8610122203826904\n",
      "Epoch 77 | Batch 1 | Loss: 0.2559545636177063\n",
      "Epoch 77 | Batch 2 | Loss: 0.32011497020721436\n",
      "Epoch 77 | Batch 3 | Loss: 0.342489629983902\n",
      "Epoch 77 | Batch 4 | Loss: 0.09542025625705719\n",
      "Epoch 77 | Batch 5 | Loss: 0.15124836564064026\n",
      "Epoch 77 | Batch 6 | Loss: 0.17265905439853668\n",
      "Epoch 77 | Batch 7 | Loss: 0.44927340745925903\n",
      "Epoch 77 | Batch 8 | Loss: 0.3999537229537964\n",
      "Epoch 77 | Batch 9 | Loss: 0.17284861207008362\n",
      "Epoch 77 | Batch 10 | Loss: 0.21652363240718842\n",
      "Epoch 77 | Batch 11 | Loss: 0.2364400327205658\n",
      "Epoch 77 | Batch 12 | Loss: 0.32714027166366577\n",
      "Epoch 77 | Batch 13 | Loss: 0.6135209202766418\n",
      "Epoch 77 | Batch 14 | Loss: 0.09841600805521011\n",
      "Epoch 77 | Batch 15 | Loss: 0.2636198401451111\n",
      "Epoch 77 | Batch 16 | Loss: 0.21726760268211365\n",
      "Epoch 77 | Batch 17 | Loss: 0.38257133960723877\n",
      "Epoch 77 | Batch 18 | Loss: 0.5684484839439392\n",
      "Epoch 77 | Batch 19 | Loss: 0.16343672573566437\n",
      "Epoch 77 | Batch 20 | Loss: 0.22800517082214355\n",
      "Epoch 77 | Batch 21 | Loss: 0.718559980392456\n",
      "Epoch 77 | Batch 22 | Loss: 0.24262365698814392\n",
      "Epoch 77 | Batch 23 | Loss: 0.15373903512954712\n",
      "Epoch 77 | Batch 24 | Loss: 0.3820858299732208\n",
      "Epoch 77 | Batch 25 | Loss: 0.2146298587322235\n",
      "Epoch 77 | Batch 26 | Loss: 0.37909209728240967\n",
      "Epoch 77 | Batch 27 | Loss: 0.6817184090614319\n",
      "Epoch 77 | Batch 28 | Loss: 0.24336956441402435\n",
      "Epoch 77 | Batch 29 | Loss: 0.07936728000640869\n",
      "Epoch 77 | Batch 30 | Loss: 0.6973224878311157\n",
      "Epoch 77 | Batch 31 | Loss: 0.16013847291469574\n",
      "Epoch 77 | Batch 32 | Loss: 0.46618011593818665\n",
      "Epoch 77 | Batch 33 | Loss: 0.1931566745042801\n",
      "Epoch 77 | Batch 34 | Loss: 0.3793114125728607\n",
      "Epoch 77 | Batch 35 | Loss: 0.42822951078414917\n",
      "Epoch 77 | Batch 36 | Loss: 0.2873486578464508\n",
      "Epoch 77 | Batch 37 | Loss: 0.216603621840477\n",
      "Epoch 77 | Batch 38 | Loss: 0.33223286271095276\n",
      "Epoch 77 | Batch 39 | Loss: 0.25566554069519043\n",
      "Epoch 77 | Batch 40 | Loss: 0.3983088731765747\n",
      "Epoch 77 | Batch 41 | Loss: 0.3549038767814636\n",
      "Epoch 77 | Batch 42 | Loss: 0.2636655569076538\n",
      "Epoch 77 | Batch 43 | Loss: 0.3633739948272705\n",
      "Epoch 77 | Batch 44 | Loss: 0.82271808385849\n",
      "Epoch 77 | Batch 45 | Loss: 0.34386947751045227\n",
      "Epoch 77 | Batch 46 | Loss: 0.4160967469215393\n",
      "Epoch 77 | Batch 47 | Loss: 0.5106268525123596\n",
      "Epoch 77 | Batch 48 | Loss: 0.2344854325056076\n",
      "Epoch 77 | Batch 49 | Loss: 0.7309390306472778\n",
      "Epoch 77 | Batch 50 | Loss: 0.8625444173812866\n",
      "Epoch 77 | Batch 51 | Loss: 0.6057097315788269\n",
      "Epoch 77 | Batch 52 | Loss: 0.34940242767333984\n",
      "Epoch 77 | Batch 53 | Loss: 0.2463313788175583\n",
      "Epoch 77 | Batch 54 | Loss: 0.18046477437019348\n",
      "Epoch 77 | Batch 55 | Loss: 0.18859511613845825\n",
      "Epoch 77 | Batch 56 | Loss: 0.34127360582351685\n",
      "Epoch 77 | Batch 57 | Loss: 0.40675121545791626\n",
      "Epoch 77 | Batch 58 | Loss: 0.4976547360420227\n",
      "Epoch 77 | Batch 59 | Loss: 0.24236111342906952\n",
      "Epoch 77 | Batch 60 | Loss: 0.11025579273700714\n",
      "Epoch 77 | Batch 61 | Loss: 0.1496545374393463\n",
      "Epoch 77 | Batch 62 | Loss: 0.4687042832374573\n",
      "Epoch 77 | Batch 63 | Loss: 0.739001989364624\n",
      "Epoch 77 | Batch 64 | Loss: 0.3108009696006775\n",
      "Epoch 77 | Batch 65 | Loss: 0.37689077854156494\n",
      "Epoch 77 | Batch 66 | Loss: 0.4104664921760559\n",
      "Epoch 77 | Batch 67 | Loss: 1.0639910697937012\n",
      "Epoch 77 | Batch 68 | Loss: 0.08799032866954803\n",
      "Epoch 77 | Batch 69 | Loss: 0.5071158409118652\n",
      "Epoch 77 | Batch 70 | Loss: 0.3543570637702942\n",
      "Epoch 77 | Batch 71 | Loss: 0.511970043182373\n",
      "Epoch 77 | Batch 72 | Loss: 0.10369674116373062\n",
      "Epoch 77 | Batch 73 | Loss: 0.2393360435962677\n",
      "Epoch 77 | Batch 74 | Loss: 0.16524559259414673\n",
      "Epoch 77 | Batch 75 | Loss: 0.29923006892204285\n",
      "Epoch 77 | Batch 76 | Loss: 0.20115497708320618\n",
      "Epoch 77 | Batch 77 | Loss: 0.1764565110206604\n",
      "Epoch 77 | Batch 78 | Loss: 0.16270914673805237\n",
      "Epoch 77 | Batch 79 | Loss: 0.4007602035999298\n",
      "Epoch 77 | Batch 80 | Loss: 0.45026540756225586\n",
      "Epoch 77 | Batch 81 | Loss: 0.4062455892562866\n",
      "Epoch 77 | Batch 82 | Loss: 0.6005252599716187\n",
      "Epoch 77 | Batch 83 | Loss: 0.6406745910644531\n",
      "Epoch 77 | Batch 84 | Loss: 0.1594436764717102\n",
      "Epoch 77 | Batch 85 | Loss: 0.47968345880508423\n",
      "Epoch 77 | Batch 86 | Loss: 0.47257429361343384\n",
      "Epoch 77 | Batch 87 | Loss: 0.5399588942527771\n",
      "Epoch 77 | Batch 88 | Loss: 0.7482821345329285\n",
      "Epoch 77 | Batch 89 | Loss: 0.7557359933853149\n",
      "Epoch 77 | Batch 90 | Loss: 0.14835204184055328\n",
      "Epoch 78 | Batch 1 | Loss: 0.24273169040679932\n",
      "Epoch 78 | Batch 2 | Loss: 0.15030595660209656\n",
      "Epoch 78 | Batch 3 | Loss: 0.23565667867660522\n",
      "Epoch 78 | Batch 4 | Loss: 0.2696995139122009\n",
      "Epoch 78 | Batch 5 | Loss: 0.20052386820316315\n",
      "Epoch 78 | Batch 6 | Loss: 0.6849592924118042\n",
      "Epoch 78 | Batch 7 | Loss: 0.25130385160446167\n",
      "Epoch 78 | Batch 8 | Loss: 0.13182857632637024\n",
      "Epoch 78 | Batch 9 | Loss: 0.1516808271408081\n",
      "Epoch 78 | Batch 10 | Loss: 0.5393137335777283\n",
      "Epoch 78 | Batch 11 | Loss: 0.27763831615448\n",
      "Epoch 78 | Batch 12 | Loss: 0.2622484862804413\n",
      "Epoch 78 | Batch 13 | Loss: 0.45349904894828796\n",
      "Epoch 78 | Batch 14 | Loss: 0.5511051416397095\n",
      "Epoch 78 | Batch 15 | Loss: 0.5191773176193237\n",
      "Epoch 78 | Batch 16 | Loss: 0.6418138146400452\n",
      "Epoch 78 | Batch 17 | Loss: 0.43969210982322693\n",
      "Epoch 78 | Batch 18 | Loss: 0.3335743546485901\n",
      "Epoch 78 | Batch 19 | Loss: 0.2503005862236023\n",
      "Epoch 78 | Batch 20 | Loss: 0.6861497163772583\n",
      "Epoch 78 | Batch 21 | Loss: 0.34415552020072937\n",
      "Epoch 78 | Batch 22 | Loss: 0.27983856201171875\n",
      "Epoch 78 | Batch 23 | Loss: 0.6720682382583618\n",
      "Epoch 78 | Batch 24 | Loss: 0.27111077308654785\n",
      "Epoch 78 | Batch 25 | Loss: 0.34816086292266846\n",
      "Epoch 78 | Batch 26 | Loss: 0.4577127695083618\n",
      "Epoch 78 | Batch 27 | Loss: 0.1118181049823761\n",
      "Epoch 78 | Batch 28 | Loss: 0.13657543063163757\n",
      "Epoch 78 | Batch 29 | Loss: 0.16327250003814697\n",
      "Epoch 78 | Batch 30 | Loss: 0.2157542109489441\n",
      "Epoch 78 | Batch 31 | Loss: 0.6539990901947021\n",
      "Epoch 78 | Batch 32 | Loss: 0.8799325823783875\n",
      "Epoch 78 | Batch 33 | Loss: 0.5051199793815613\n",
      "Epoch 78 | Batch 34 | Loss: 0.4546573758125305\n",
      "Epoch 78 | Batch 35 | Loss: 0.6376500129699707\n",
      "Epoch 78 | Batch 36 | Loss: 0.41502809524536133\n",
      "Epoch 78 | Batch 37 | Loss: 0.2987484931945801\n",
      "Epoch 78 | Batch 38 | Loss: 0.28856733441352844\n",
      "Epoch 78 | Batch 39 | Loss: 0.30735132098197937\n",
      "Epoch 78 | Batch 40 | Loss: 0.09296837449073792\n",
      "Epoch 78 | Batch 41 | Loss: 0.22592772543430328\n",
      "Epoch 78 | Batch 42 | Loss: 0.35037699341773987\n",
      "Epoch 78 | Batch 43 | Loss: 0.3977741003036499\n",
      "Epoch 78 | Batch 44 | Loss: 0.11932072043418884\n",
      "Epoch 78 | Batch 45 | Loss: 0.10777252912521362\n",
      "Epoch 78 | Batch 46 | Loss: 0.9465317726135254\n",
      "Epoch 78 | Batch 47 | Loss: 0.1363929808139801\n",
      "Epoch 78 | Batch 48 | Loss: 0.5318477153778076\n",
      "Epoch 78 | Batch 49 | Loss: 0.2575308084487915\n",
      "Epoch 78 | Batch 50 | Loss: 0.5135418176651001\n",
      "Epoch 78 | Batch 51 | Loss: 0.28654980659484863\n",
      "Epoch 78 | Batch 52 | Loss: 0.6463062167167664\n",
      "Epoch 78 | Batch 53 | Loss: 0.2732863426208496\n",
      "Epoch 78 | Batch 54 | Loss: 0.347301185131073\n",
      "Epoch 78 | Batch 55 | Loss: 0.43305104970932007\n",
      "Epoch 78 | Batch 56 | Loss: 0.37635043263435364\n",
      "Epoch 78 | Batch 57 | Loss: 0.3566148579120636\n",
      "Epoch 78 | Batch 58 | Loss: 0.35293179750442505\n",
      "Epoch 78 | Batch 59 | Loss: 0.5290604829788208\n",
      "Epoch 78 | Batch 60 | Loss: 0.3958747386932373\n",
      "Epoch 78 | Batch 61 | Loss: 0.4954493045806885\n",
      "Epoch 78 | Batch 62 | Loss: 0.4818320870399475\n",
      "Epoch 78 | Batch 63 | Loss: 0.1906072199344635\n",
      "Epoch 78 | Batch 64 | Loss: 0.184661865234375\n",
      "Epoch 78 | Batch 65 | Loss: 0.5705265998840332\n",
      "Epoch 78 | Batch 66 | Loss: 0.5516403317451477\n",
      "Epoch 78 | Batch 67 | Loss: 0.2980778217315674\n",
      "Epoch 78 | Batch 68 | Loss: 0.37570661306381226\n",
      "Epoch 78 | Batch 69 | Loss: 0.26356837153434753\n",
      "Epoch 78 | Batch 70 | Loss: 0.5027766823768616\n",
      "Epoch 78 | Batch 71 | Loss: 0.3544762432575226\n",
      "Epoch 78 | Batch 72 | Loss: 0.4053863286972046\n",
      "Epoch 78 | Batch 73 | Loss: 0.8090791702270508\n",
      "Epoch 78 | Batch 74 | Loss: 0.3528251051902771\n",
      "Epoch 78 | Batch 75 | Loss: 0.2767820954322815\n",
      "Epoch 78 | Batch 76 | Loss: 0.17447228729724884\n",
      "Epoch 78 | Batch 77 | Loss: 0.13348227739334106\n",
      "Epoch 78 | Batch 78 | Loss: 0.3320339322090149\n",
      "Epoch 78 | Batch 79 | Loss: 0.14102090895175934\n",
      "Epoch 78 | Batch 80 | Loss: 0.14359284937381744\n",
      "Epoch 78 | Batch 81 | Loss: 0.3582105040550232\n",
      "Epoch 78 | Batch 82 | Loss: 0.3490070104598999\n",
      "Epoch 78 | Batch 83 | Loss: 0.0908605307340622\n",
      "Epoch 78 | Batch 84 | Loss: 0.10802563279867172\n",
      "Epoch 78 | Batch 85 | Loss: 0.19228266179561615\n",
      "Epoch 78 | Batch 86 | Loss: 0.1663471758365631\n",
      "Epoch 78 | Batch 87 | Loss: 0.3998638391494751\n",
      "Epoch 78 | Batch 88 | Loss: 0.5562101602554321\n",
      "Epoch 78 | Batch 89 | Loss: 0.46664243936538696\n",
      "Epoch 78 | Batch 90 | Loss: 0.5579537153244019\n",
      "Epoch 79 | Batch 1 | Loss: 0.4595854580402374\n",
      "Epoch 79 | Batch 2 | Loss: 0.44050726294517517\n",
      "Epoch 79 | Batch 3 | Loss: 0.7370271682739258\n",
      "Epoch 79 | Batch 4 | Loss: 0.13432620465755463\n",
      "Epoch 79 | Batch 5 | Loss: 0.0732983648777008\n",
      "Epoch 79 | Batch 6 | Loss: 0.380003958940506\n",
      "Epoch 79 | Batch 7 | Loss: 0.17990078032016754\n",
      "Epoch 79 | Batch 8 | Loss: 0.34772348403930664\n",
      "Epoch 79 | Batch 9 | Loss: 0.5578215718269348\n",
      "Epoch 79 | Batch 10 | Loss: 0.35672512650489807\n",
      "Epoch 79 | Batch 11 | Loss: 0.4577384293079376\n",
      "Epoch 79 | Batch 12 | Loss: 0.20442277193069458\n",
      "Epoch 79 | Batch 13 | Loss: 0.20915387570858002\n",
      "Epoch 79 | Batch 14 | Loss: 0.1061403825879097\n",
      "Epoch 79 | Batch 15 | Loss: 0.37675946950912476\n",
      "Epoch 79 | Batch 16 | Loss: 0.14889153838157654\n",
      "Epoch 79 | Batch 17 | Loss: 0.12462242692708969\n",
      "Epoch 79 | Batch 18 | Loss: 0.0945233553647995\n",
      "Epoch 79 | Batch 19 | Loss: 1.00361967086792\n",
      "Epoch 79 | Batch 20 | Loss: 0.4883342683315277\n",
      "Epoch 79 | Batch 21 | Loss: 0.4159364104270935\n",
      "Epoch 79 | Batch 22 | Loss: 0.3439086377620697\n",
      "Epoch 79 | Batch 23 | Loss: 0.20681627094745636\n",
      "Epoch 79 | Batch 24 | Loss: 0.27969786524772644\n",
      "Epoch 79 | Batch 25 | Loss: 0.19716408848762512\n",
      "Epoch 79 | Batch 26 | Loss: 0.47830164432525635\n",
      "Epoch 79 | Batch 27 | Loss: 0.46823573112487793\n",
      "Epoch 79 | Batch 28 | Loss: 0.20904164016246796\n",
      "Epoch 79 | Batch 29 | Loss: 0.363773912191391\n",
      "Epoch 79 | Batch 30 | Loss: 0.46336686611175537\n",
      "Epoch 79 | Batch 31 | Loss: 0.25956064462661743\n",
      "Epoch 79 | Batch 32 | Loss: 0.5020880103111267\n",
      "Epoch 79 | Batch 33 | Loss: 0.16103330254554749\n",
      "Epoch 79 | Batch 34 | Loss: 0.26196208596229553\n",
      "Epoch 79 | Batch 35 | Loss: 0.4546593427658081\n",
      "Epoch 79 | Batch 36 | Loss: 0.3200318217277527\n",
      "Epoch 79 | Batch 37 | Loss: 0.2531720995903015\n",
      "Epoch 79 | Batch 38 | Loss: 0.40804243087768555\n",
      "Epoch 79 | Batch 39 | Loss: 1.0891387462615967\n",
      "Epoch 79 | Batch 40 | Loss: 0.360164999961853\n",
      "Epoch 79 | Batch 41 | Loss: 0.30098772048950195\n",
      "Epoch 79 | Batch 42 | Loss: 0.22792638838291168\n",
      "Epoch 79 | Batch 43 | Loss: 0.5990844964981079\n",
      "Epoch 79 | Batch 44 | Loss: 0.16775038838386536\n",
      "Epoch 79 | Batch 45 | Loss: 0.19572779536247253\n",
      "Epoch 79 | Batch 46 | Loss: 0.11900261044502258\n",
      "Epoch 79 | Batch 47 | Loss: 0.35516172647476196\n",
      "Epoch 79 | Batch 48 | Loss: 0.4481176435947418\n",
      "Epoch 79 | Batch 49 | Loss: 0.24557580053806305\n",
      "Epoch 79 | Batch 50 | Loss: 0.19593051075935364\n",
      "Epoch 79 | Batch 51 | Loss: 0.1727694272994995\n",
      "Epoch 79 | Batch 52 | Loss: 0.4139873683452606\n",
      "Epoch 79 | Batch 53 | Loss: 0.40437912940979004\n",
      "Epoch 79 | Batch 54 | Loss: 0.1589697301387787\n",
      "Epoch 79 | Batch 55 | Loss: 0.4929850697517395\n",
      "Epoch 79 | Batch 56 | Loss: 0.5443860292434692\n",
      "Epoch 79 | Batch 57 | Loss: 0.6129394769668579\n",
      "Epoch 79 | Batch 58 | Loss: 0.5931837558746338\n",
      "Epoch 79 | Batch 59 | Loss: 0.3883741796016693\n",
      "Epoch 79 | Batch 60 | Loss: 0.4311877489089966\n",
      "Epoch 79 | Batch 61 | Loss: 0.2346823662519455\n",
      "Epoch 79 | Batch 62 | Loss: 0.6890875697135925\n",
      "Epoch 79 | Batch 63 | Loss: 0.18603281676769257\n",
      "Epoch 79 | Batch 64 | Loss: 0.4688814580440521\n",
      "Epoch 79 | Batch 65 | Loss: 0.3289320766925812\n",
      "Epoch 79 | Batch 66 | Loss: 0.223399817943573\n",
      "Epoch 79 | Batch 67 | Loss: 0.3308494985103607\n",
      "Epoch 79 | Batch 68 | Loss: 0.2382417470216751\n",
      "Epoch 79 | Batch 69 | Loss: 0.3110266327857971\n",
      "Epoch 79 | Batch 70 | Loss: 0.09944739937782288\n",
      "Epoch 79 | Batch 71 | Loss: 0.0856662392616272\n",
      "Epoch 79 | Batch 72 | Loss: 0.35677823424339294\n",
      "Epoch 79 | Batch 73 | Loss: 0.20359237492084503\n",
      "Epoch 79 | Batch 74 | Loss: 0.7661416530609131\n",
      "Epoch 79 | Batch 75 | Loss: 0.31015545129776\n",
      "Epoch 79 | Batch 76 | Loss: 0.5523232221603394\n",
      "Epoch 79 | Batch 77 | Loss: 0.36270999908447266\n",
      "Epoch 79 | Batch 78 | Loss: 0.344571053981781\n",
      "Epoch 79 | Batch 79 | Loss: 0.22268635034561157\n",
      "Epoch 79 | Batch 80 | Loss: 0.103394515812397\n",
      "Epoch 79 | Batch 81 | Loss: 0.5722756385803223\n",
      "Epoch 79 | Batch 82 | Loss: 0.15790528059005737\n",
      "Epoch 79 | Batch 83 | Loss: 0.733822762966156\n",
      "Epoch 79 | Batch 84 | Loss: 0.4018429219722748\n",
      "Epoch 79 | Batch 85 | Loss: 0.8509185910224915\n",
      "Epoch 79 | Batch 86 | Loss: 0.21860317885875702\n",
      "Epoch 79 | Batch 87 | Loss: 0.15283137559890747\n",
      "Epoch 79 | Batch 88 | Loss: 0.7414155602455139\n",
      "Epoch 79 | Batch 89 | Loss: 0.6276005506515503\n",
      "Epoch 79 | Batch 90 | Loss: 0.05854576826095581\n",
      "Epoch 80 | Batch 1 | Loss: 0.49261730909347534\n",
      "Epoch 80 | Batch 2 | Loss: 0.41690152883529663\n",
      "Epoch 80 | Batch 3 | Loss: 0.5048187971115112\n",
      "Epoch 80 | Batch 4 | Loss: 0.5633467435836792\n",
      "Epoch 80 | Batch 5 | Loss: 0.4899807572364807\n",
      "Epoch 80 | Batch 6 | Loss: 0.4169941842556\n",
      "Epoch 80 | Batch 7 | Loss: 0.7406110167503357\n",
      "Epoch 80 | Batch 8 | Loss: 0.2341398149728775\n",
      "Epoch 80 | Batch 9 | Loss: 0.23639598488807678\n",
      "Epoch 80 | Batch 10 | Loss: 0.27860045433044434\n",
      "Epoch 80 | Batch 11 | Loss: 0.80171799659729\n",
      "Epoch 80 | Batch 12 | Loss: 0.2913063168525696\n",
      "Epoch 80 | Batch 13 | Loss: 0.25787317752838135\n",
      "Epoch 80 | Batch 14 | Loss: 0.29420608282089233\n",
      "Epoch 80 | Batch 15 | Loss: 0.3239864706993103\n",
      "Epoch 80 | Batch 16 | Loss: 0.3458143472671509\n",
      "Epoch 80 | Batch 17 | Loss: 0.22917744517326355\n",
      "Epoch 80 | Batch 18 | Loss: 0.29984885454177856\n",
      "Epoch 80 | Batch 19 | Loss: 0.10477717965841293\n",
      "Epoch 80 | Batch 20 | Loss: 0.1574101448059082\n",
      "Epoch 80 | Batch 21 | Loss: 0.7696582674980164\n",
      "Epoch 80 | Batch 22 | Loss: 0.2854050397872925\n",
      "Epoch 80 | Batch 23 | Loss: 0.37107741832733154\n",
      "Epoch 80 | Batch 24 | Loss: 0.2740730047225952\n",
      "Epoch 80 | Batch 25 | Loss: 0.22112786769866943\n",
      "Epoch 80 | Batch 26 | Loss: 0.1973891258239746\n",
      "Epoch 80 | Batch 27 | Loss: 0.31698229908943176\n",
      "Epoch 80 | Batch 28 | Loss: 0.12924231588840485\n",
      "Epoch 80 | Batch 29 | Loss: 0.73888099193573\n",
      "Epoch 80 | Batch 30 | Loss: 0.1179908812046051\n",
      "Epoch 80 | Batch 31 | Loss: 0.359956830739975\n",
      "Epoch 80 | Batch 32 | Loss: 0.12601305544376373\n",
      "Epoch 80 | Batch 33 | Loss: 0.5805696249008179\n",
      "Epoch 80 | Batch 34 | Loss: 0.8800324201583862\n",
      "Epoch 80 | Batch 35 | Loss: 1.1269999742507935\n",
      "Epoch 80 | Batch 36 | Loss: 0.3852084279060364\n",
      "Epoch 80 | Batch 37 | Loss: 0.36810505390167236\n",
      "Epoch 80 | Batch 38 | Loss: 0.21663103997707367\n",
      "Epoch 80 | Batch 39 | Loss: 0.09184039384126663\n",
      "Epoch 80 | Batch 40 | Loss: 0.5302944183349609\n",
      "Epoch 80 | Batch 41 | Loss: 0.19934196770191193\n",
      "Epoch 80 | Batch 42 | Loss: 0.3485153913497925\n",
      "Epoch 80 | Batch 43 | Loss: 0.34129565954208374\n",
      "Epoch 80 | Batch 44 | Loss: 0.2934102416038513\n",
      "Epoch 80 | Batch 45 | Loss: 0.17917177081108093\n",
      "Epoch 80 | Batch 46 | Loss: 0.821297287940979\n",
      "Epoch 80 | Batch 47 | Loss: 0.34108680486679077\n",
      "Epoch 80 | Batch 48 | Loss: 0.4734303951263428\n",
      "Epoch 80 | Batch 49 | Loss: 0.34964874386787415\n",
      "Epoch 80 | Batch 50 | Loss: 0.32686734199523926\n",
      "Epoch 80 | Batch 51 | Loss: 0.804698646068573\n",
      "Epoch 80 | Batch 52 | Loss: 0.08652809262275696\n",
      "Epoch 80 | Batch 53 | Loss: 0.5809240341186523\n",
      "Epoch 80 | Batch 54 | Loss: 0.1979706585407257\n",
      "Epoch 80 | Batch 55 | Loss: 0.07933220267295837\n",
      "Epoch 80 | Batch 56 | Loss: 0.4707808494567871\n",
      "Epoch 80 | Batch 57 | Loss: 0.2708479166030884\n",
      "Epoch 80 | Batch 58 | Loss: 0.37333518266677856\n",
      "Epoch 80 | Batch 59 | Loss: 0.4229508638381958\n",
      "Epoch 80 | Batch 60 | Loss: 0.4603658616542816\n",
      "Epoch 80 | Batch 61 | Loss: 0.34905070066452026\n",
      "Epoch 80 | Batch 62 | Loss: 0.4705367088317871\n",
      "Epoch 80 | Batch 63 | Loss: 0.1959826946258545\n",
      "Epoch 80 | Batch 64 | Loss: 0.11814478784799576\n",
      "Epoch 80 | Batch 65 | Loss: 0.5901366472244263\n",
      "Epoch 80 | Batch 66 | Loss: 0.10016423463821411\n",
      "Epoch 80 | Batch 67 | Loss: 0.42617499828338623\n",
      "Epoch 80 | Batch 68 | Loss: 0.22338995337486267\n",
      "Epoch 80 | Batch 69 | Loss: 0.09182329475879669\n",
      "Epoch 80 | Batch 70 | Loss: 0.46337294578552246\n",
      "Epoch 80 | Batch 71 | Loss: 0.22809109091758728\n",
      "Epoch 80 | Batch 72 | Loss: 0.17335915565490723\n",
      "Epoch 80 | Batch 73 | Loss: 0.2689723074436188\n",
      "Epoch 80 | Batch 74 | Loss: 0.48505347967147827\n",
      "Epoch 80 | Batch 75 | Loss: 0.6138662099838257\n",
      "Epoch 80 | Batch 76 | Loss: 0.267683207988739\n",
      "Epoch 80 | Batch 77 | Loss: 0.43388429284095764\n",
      "Epoch 80 | Batch 78 | Loss: 0.12448768317699432\n",
      "Epoch 80 | Batch 79 | Loss: 0.3540267050266266\n",
      "Epoch 80 | Batch 80 | Loss: 0.45601511001586914\n",
      "Epoch 80 | Batch 81 | Loss: 0.22260193526744843\n",
      "Epoch 80 | Batch 82 | Loss: 0.40032893419265747\n",
      "Epoch 80 | Batch 83 | Loss: 0.32808390259742737\n",
      "Epoch 80 | Batch 84 | Loss: 0.4448523223400116\n",
      "Epoch 80 | Batch 85 | Loss: 0.44816896319389343\n",
      "Epoch 80 | Batch 86 | Loss: 0.3418102264404297\n",
      "Epoch 80 | Batch 87 | Loss: 0.08576373755931854\n",
      "Epoch 80 | Batch 88 | Loss: 0.925295352935791\n",
      "Epoch 80 | Batch 89 | Loss: 0.126237690448761\n",
      "Epoch 80 | Batch 90 | Loss: 0.06610750406980515\n",
      "Epoch 81 | Batch 1 | Loss: 0.2641379237174988\n",
      "Epoch 81 | Batch 2 | Loss: 0.25081944465637207\n",
      "Epoch 81 | Batch 3 | Loss: 0.35322514176368713\n",
      "Epoch 81 | Batch 4 | Loss: 0.5369987487792969\n",
      "Epoch 81 | Batch 5 | Loss: 0.2740025520324707\n",
      "Epoch 81 | Batch 6 | Loss: 0.6358471512794495\n",
      "Epoch 81 | Batch 7 | Loss: 0.594135046005249\n",
      "Epoch 81 | Batch 8 | Loss: 0.12986059486865997\n",
      "Epoch 81 | Batch 9 | Loss: 0.5189722776412964\n",
      "Epoch 81 | Batch 10 | Loss: 0.14524154365062714\n",
      "Epoch 81 | Batch 11 | Loss: 0.6150151491165161\n",
      "Epoch 81 | Batch 12 | Loss: 0.36019107699394226\n",
      "Epoch 81 | Batch 13 | Loss: 0.5745359063148499\n",
      "Epoch 81 | Batch 14 | Loss: 0.438950777053833\n",
      "Epoch 81 | Batch 15 | Loss: 0.6053520441055298\n",
      "Epoch 81 | Batch 16 | Loss: 0.1711091250181198\n",
      "Epoch 81 | Batch 17 | Loss: 0.3507070243358612\n",
      "Epoch 81 | Batch 18 | Loss: 0.16260850429534912\n",
      "Epoch 81 | Batch 19 | Loss: 0.6089863777160645\n",
      "Epoch 81 | Batch 20 | Loss: 0.3063836097717285\n",
      "Epoch 81 | Batch 21 | Loss: 0.4118862748146057\n",
      "Epoch 81 | Batch 22 | Loss: 0.11854709684848785\n",
      "Epoch 81 | Batch 23 | Loss: 0.24811039865016937\n",
      "Epoch 81 | Batch 24 | Loss: 0.4758099913597107\n",
      "Epoch 81 | Batch 25 | Loss: 0.28733116388320923\n",
      "Epoch 81 | Batch 26 | Loss: 0.6104832291603088\n",
      "Epoch 81 | Batch 27 | Loss: 0.23921605944633484\n",
      "Epoch 81 | Batch 28 | Loss: 0.20020896196365356\n",
      "Epoch 81 | Batch 29 | Loss: 0.26919934153556824\n",
      "Epoch 81 | Batch 30 | Loss: 0.4602887034416199\n",
      "Epoch 81 | Batch 31 | Loss: 0.39663150906562805\n",
      "Epoch 81 | Batch 32 | Loss: 0.315351277589798\n",
      "Epoch 81 | Batch 33 | Loss: 0.4731987416744232\n",
      "Epoch 81 | Batch 34 | Loss: 0.3118424415588379\n",
      "Epoch 81 | Batch 35 | Loss: 0.15843020379543304\n",
      "Epoch 81 | Batch 36 | Loss: 0.6842095851898193\n",
      "Epoch 81 | Batch 37 | Loss: 0.33845165371894836\n",
      "Epoch 81 | Batch 38 | Loss: 0.34195390343666077\n",
      "Epoch 81 | Batch 39 | Loss: 0.17142358422279358\n",
      "Epoch 81 | Batch 40 | Loss: 0.19503745436668396\n",
      "Epoch 81 | Batch 41 | Loss: 0.20169655978679657\n",
      "Epoch 81 | Batch 42 | Loss: 0.37840020656585693\n",
      "Epoch 81 | Batch 43 | Loss: 0.10813213139772415\n",
      "Epoch 81 | Batch 44 | Loss: 0.45482873916625977\n",
      "Epoch 81 | Batch 45 | Loss: 0.18482863903045654\n",
      "Epoch 81 | Batch 46 | Loss: 0.7915526032447815\n",
      "Epoch 81 | Batch 47 | Loss: 0.16345933079719543\n",
      "Epoch 81 | Batch 48 | Loss: 0.7072346210479736\n",
      "Epoch 81 | Batch 49 | Loss: 0.5256045460700989\n",
      "Epoch 81 | Batch 50 | Loss: 0.10882127285003662\n",
      "Epoch 81 | Batch 51 | Loss: 0.3026410937309265\n",
      "Epoch 81 | Batch 52 | Loss: 0.1016777977347374\n",
      "Epoch 81 | Batch 53 | Loss: 0.33375561237335205\n",
      "Epoch 81 | Batch 54 | Loss: 0.16749821603298187\n",
      "Epoch 81 | Batch 55 | Loss: 0.211422860622406\n",
      "Epoch 81 | Batch 56 | Loss: 0.25620555877685547\n",
      "Epoch 81 | Batch 57 | Loss: 0.24083229899406433\n",
      "Epoch 81 | Batch 58 | Loss: 0.23828953504562378\n",
      "Epoch 81 | Batch 59 | Loss: 0.4752843379974365\n",
      "Epoch 81 | Batch 60 | Loss: 0.440886914730072\n",
      "Epoch 81 | Batch 61 | Loss: 0.12174365669488907\n",
      "Epoch 81 | Batch 62 | Loss: 0.12047652155160904\n",
      "Epoch 81 | Batch 63 | Loss: 0.30367839336395264\n",
      "Epoch 81 | Batch 64 | Loss: 0.3244844675064087\n",
      "Epoch 81 | Batch 65 | Loss: 0.42879557609558105\n",
      "Epoch 81 | Batch 66 | Loss: 0.1739223152399063\n",
      "Epoch 81 | Batch 67 | Loss: 0.7060569524765015\n",
      "Epoch 81 | Batch 68 | Loss: 0.17839117348194122\n",
      "Epoch 81 | Batch 69 | Loss: 0.7473805546760559\n",
      "Epoch 81 | Batch 70 | Loss: 0.6141974925994873\n",
      "Epoch 81 | Batch 71 | Loss: 0.47523969411849976\n",
      "Epoch 81 | Batch 72 | Loss: 0.5024161338806152\n",
      "Epoch 81 | Batch 73 | Loss: 0.1772044152021408\n",
      "Epoch 81 | Batch 74 | Loss: 0.08697818219661713\n",
      "Epoch 81 | Batch 75 | Loss: 0.10713949054479599\n",
      "Epoch 81 | Batch 76 | Loss: 0.8270947337150574\n",
      "Epoch 81 | Batch 77 | Loss: 0.2908936142921448\n",
      "Epoch 81 | Batch 78 | Loss: 0.5283770561218262\n",
      "Epoch 81 | Batch 79 | Loss: 0.26917052268981934\n",
      "Epoch 81 | Batch 80 | Loss: 0.355490505695343\n",
      "Epoch 81 | Batch 81 | Loss: 0.19441357254981995\n",
      "Epoch 81 | Batch 82 | Loss: 0.4502917528152466\n",
      "Epoch 81 | Batch 83 | Loss: 0.6955174207687378\n",
      "Epoch 81 | Batch 84 | Loss: 0.6793017387390137\n",
      "Epoch 81 | Batch 85 | Loss: 0.3683522343635559\n",
      "Epoch 81 | Batch 86 | Loss: 0.1733722686767578\n",
      "Epoch 81 | Batch 87 | Loss: 0.5554795265197754\n",
      "Epoch 81 | Batch 88 | Loss: 0.1162947416305542\n",
      "Epoch 81 | Batch 89 | Loss: 0.7260560989379883\n",
      "Epoch 81 | Batch 90 | Loss: 1.0517690181732178\n",
      "Epoch 82 | Batch 1 | Loss: 0.32204627990722656\n",
      "Epoch 82 | Batch 2 | Loss: 0.1625913679599762\n",
      "Epoch 82 | Batch 3 | Loss: 0.19399785995483398\n",
      "Epoch 82 | Batch 4 | Loss: 0.056435517966747284\n",
      "Epoch 82 | Batch 5 | Loss: 0.4870678186416626\n",
      "Epoch 82 | Batch 6 | Loss: 0.49016231298446655\n",
      "Epoch 82 | Batch 7 | Loss: 0.470869243144989\n",
      "Epoch 82 | Batch 8 | Loss: 0.3591881990432739\n",
      "Epoch 82 | Batch 9 | Loss: 0.14565324783325195\n",
      "Epoch 82 | Batch 10 | Loss: 0.28820937871932983\n",
      "Epoch 82 | Batch 11 | Loss: 0.4224342107772827\n",
      "Epoch 82 | Batch 12 | Loss: 0.6749898195266724\n",
      "Epoch 82 | Batch 13 | Loss: 0.07908140122890472\n",
      "Epoch 82 | Batch 14 | Loss: 0.3854515850543976\n",
      "Epoch 82 | Batch 15 | Loss: 0.15261107683181763\n",
      "Epoch 82 | Batch 16 | Loss: 0.3684875965118408\n",
      "Epoch 82 | Batch 17 | Loss: 0.25299614667892456\n",
      "Epoch 82 | Batch 18 | Loss: 0.43206214904785156\n",
      "Epoch 82 | Batch 19 | Loss: 0.29513853788375854\n",
      "Epoch 82 | Batch 20 | Loss: 0.28464752435684204\n",
      "Epoch 82 | Batch 21 | Loss: 0.46420907974243164\n",
      "Epoch 82 | Batch 22 | Loss: 0.5820202827453613\n",
      "Epoch 82 | Batch 23 | Loss: 0.166961669921875\n",
      "Epoch 82 | Batch 24 | Loss: 0.18205618858337402\n",
      "Epoch 82 | Batch 25 | Loss: 0.15447738766670227\n",
      "Epoch 82 | Batch 26 | Loss: 0.4058982729911804\n",
      "Epoch 82 | Batch 27 | Loss: 0.30107617378234863\n",
      "Epoch 82 | Batch 28 | Loss: 0.5928465723991394\n",
      "Epoch 82 | Batch 29 | Loss: 0.5722293257713318\n",
      "Epoch 82 | Batch 30 | Loss: 0.42493948340415955\n",
      "Epoch 82 | Batch 31 | Loss: 0.5237699151039124\n",
      "Epoch 82 | Batch 32 | Loss: 0.7323141694068909\n",
      "Epoch 82 | Batch 33 | Loss: 0.28001660108566284\n",
      "Epoch 82 | Batch 34 | Loss: 0.2936505079269409\n",
      "Epoch 82 | Batch 35 | Loss: 0.46907681226730347\n",
      "Epoch 82 | Batch 36 | Loss: 0.12226231396198273\n",
      "Epoch 82 | Batch 37 | Loss: 0.33357375860214233\n",
      "Epoch 82 | Batch 38 | Loss: 0.615398108959198\n",
      "Epoch 82 | Batch 39 | Loss: 0.3945290446281433\n",
      "Epoch 82 | Batch 40 | Loss: 0.44502338767051697\n",
      "Epoch 82 | Batch 41 | Loss: 0.6610361337661743\n",
      "Epoch 82 | Batch 42 | Loss: 0.5053935647010803\n",
      "Epoch 82 | Batch 43 | Loss: 0.31803321838378906\n",
      "Epoch 82 | Batch 44 | Loss: 0.22733831405639648\n",
      "Epoch 82 | Batch 45 | Loss: 0.48133474588394165\n",
      "Epoch 82 | Batch 46 | Loss: 0.32873183488845825\n",
      "Epoch 82 | Batch 47 | Loss: 0.8076205849647522\n",
      "Epoch 82 | Batch 48 | Loss: 0.4054500460624695\n",
      "Epoch 82 | Batch 49 | Loss: 0.249128058552742\n",
      "Epoch 82 | Batch 50 | Loss: 0.27376997470855713\n",
      "Epoch 82 | Batch 51 | Loss: 0.2776518166065216\n",
      "Epoch 82 | Batch 52 | Loss: 0.11335282027721405\n",
      "Epoch 82 | Batch 53 | Loss: 0.14863471686840057\n",
      "Epoch 82 | Batch 54 | Loss: 0.08220714330673218\n",
      "Epoch 82 | Batch 55 | Loss: 0.2859709858894348\n",
      "Epoch 82 | Batch 56 | Loss: 0.22805151343345642\n",
      "Epoch 82 | Batch 57 | Loss: 0.6374816298484802\n",
      "Epoch 82 | Batch 58 | Loss: 0.16843637824058533\n",
      "Epoch 82 | Batch 59 | Loss: 0.7223796844482422\n",
      "Epoch 82 | Batch 60 | Loss: 0.15788671374320984\n",
      "Epoch 82 | Batch 61 | Loss: 0.6816971302032471\n",
      "Epoch 82 | Batch 62 | Loss: 0.37381380796432495\n",
      "Epoch 82 | Batch 63 | Loss: 0.3123907744884491\n",
      "Epoch 82 | Batch 64 | Loss: 0.644838809967041\n",
      "Epoch 82 | Batch 65 | Loss: 0.38130462169647217\n",
      "Epoch 82 | Batch 66 | Loss: 0.12925666570663452\n",
      "Epoch 82 | Batch 67 | Loss: 0.10734224319458008\n",
      "Epoch 82 | Batch 68 | Loss: 0.5274215340614319\n",
      "Epoch 82 | Batch 69 | Loss: 0.263513445854187\n",
      "Epoch 82 | Batch 70 | Loss: 0.3611987829208374\n",
      "Epoch 82 | Batch 71 | Loss: 0.1328846514225006\n",
      "Epoch 82 | Batch 72 | Loss: 0.1237996369600296\n",
      "Epoch 82 | Batch 73 | Loss: 0.3378796875476837\n",
      "Epoch 82 | Batch 74 | Loss: 0.13433146476745605\n",
      "Epoch 82 | Batch 75 | Loss: 0.5255656242370605\n",
      "Epoch 82 | Batch 76 | Loss: 0.43922293186187744\n",
      "Epoch 82 | Batch 77 | Loss: 0.4803154170513153\n",
      "Epoch 82 | Batch 78 | Loss: 0.10968497395515442\n",
      "Epoch 82 | Batch 79 | Loss: 0.5078368186950684\n",
      "Epoch 82 | Batch 80 | Loss: 0.1771698296070099\n",
      "Epoch 82 | Batch 81 | Loss: 0.18548381328582764\n",
      "Epoch 82 | Batch 82 | Loss: 0.7391289472579956\n",
      "Epoch 82 | Batch 83 | Loss: 0.7928686738014221\n",
      "Epoch 82 | Batch 84 | Loss: 0.13925907015800476\n",
      "Epoch 82 | Batch 85 | Loss: 0.7000890374183655\n",
      "Epoch 82 | Batch 86 | Loss: 0.5892252326011658\n",
      "Epoch 82 | Batch 87 | Loss: 0.41975700855255127\n",
      "Epoch 82 | Batch 88 | Loss: 0.31440529227256775\n",
      "Epoch 82 | Batch 89 | Loss: 0.11605486273765564\n",
      "Epoch 82 | Batch 90 | Loss: 0.2612905502319336\n",
      "Epoch 83 | Batch 1 | Loss: 0.456574022769928\n",
      "Epoch 83 | Batch 2 | Loss: 0.49581730365753174\n",
      "Epoch 83 | Batch 3 | Loss: 0.10915590077638626\n",
      "Epoch 83 | Batch 4 | Loss: 0.21975329518318176\n",
      "Epoch 83 | Batch 5 | Loss: 0.5339517593383789\n",
      "Epoch 83 | Batch 6 | Loss: 0.22094957530498505\n",
      "Epoch 83 | Batch 7 | Loss: 0.6977785229682922\n",
      "Epoch 83 | Batch 8 | Loss: 0.13687598705291748\n",
      "Epoch 83 | Batch 9 | Loss: 0.44576627016067505\n",
      "Epoch 83 | Batch 10 | Loss: 0.4831080734729767\n",
      "Epoch 83 | Batch 11 | Loss: 0.5681636929512024\n",
      "Epoch 83 | Batch 12 | Loss: 0.0634792223572731\n",
      "Epoch 83 | Batch 13 | Loss: 0.3876965045928955\n",
      "Epoch 83 | Batch 14 | Loss: 0.148586705327034\n",
      "Epoch 83 | Batch 15 | Loss: 0.2124401181936264\n",
      "Epoch 83 | Batch 16 | Loss: 0.144692525267601\n",
      "Epoch 83 | Batch 17 | Loss: 0.7399954199790955\n",
      "Epoch 83 | Batch 18 | Loss: 0.1667613387107849\n",
      "Epoch 83 | Batch 19 | Loss: 0.43693894147872925\n",
      "Epoch 83 | Batch 20 | Loss: 0.8304368853569031\n",
      "Epoch 83 | Batch 21 | Loss: 0.4502697288990021\n",
      "Epoch 83 | Batch 22 | Loss: 0.3619275391101837\n",
      "Epoch 83 | Batch 23 | Loss: 0.5929125547409058\n",
      "Epoch 83 | Batch 24 | Loss: 0.3655834496021271\n",
      "Epoch 83 | Batch 25 | Loss: 0.6865280866622925\n",
      "Epoch 83 | Batch 26 | Loss: 0.14606121182441711\n",
      "Epoch 83 | Batch 27 | Loss: 0.5638922452926636\n",
      "Epoch 83 | Batch 28 | Loss: 0.3162142038345337\n",
      "Epoch 83 | Batch 29 | Loss: 0.1097717136144638\n",
      "Epoch 83 | Batch 30 | Loss: 0.21119913458824158\n",
      "Epoch 83 | Batch 31 | Loss: 0.6984673738479614\n",
      "Epoch 83 | Batch 32 | Loss: 0.3806593120098114\n",
      "Epoch 83 | Batch 33 | Loss: 0.15800803899765015\n",
      "Epoch 83 | Batch 34 | Loss: 0.3759695589542389\n",
      "Epoch 83 | Batch 35 | Loss: 0.12937533855438232\n",
      "Epoch 83 | Batch 36 | Loss: 0.3499358892440796\n",
      "Epoch 83 | Batch 37 | Loss: 0.463537335395813\n",
      "Epoch 83 | Batch 38 | Loss: 0.19734787940979004\n",
      "Epoch 83 | Batch 39 | Loss: 0.2868387997150421\n",
      "Epoch 83 | Batch 40 | Loss: 0.33215010166168213\n",
      "Epoch 83 | Batch 41 | Loss: 0.7146676778793335\n",
      "Epoch 83 | Batch 42 | Loss: 0.3701113164424896\n",
      "Epoch 83 | Batch 43 | Loss: 0.310374915599823\n",
      "Epoch 83 | Batch 44 | Loss: 0.6770074367523193\n",
      "Epoch 83 | Batch 45 | Loss: 0.31315183639526367\n",
      "Epoch 83 | Batch 46 | Loss: 0.6211725473403931\n",
      "Epoch 83 | Batch 47 | Loss: 0.2761025130748749\n",
      "Epoch 83 | Batch 48 | Loss: 0.5815544128417969\n",
      "Epoch 83 | Batch 49 | Loss: 0.26352500915527344\n",
      "Epoch 83 | Batch 50 | Loss: 0.39292314648628235\n",
      "Epoch 83 | Batch 51 | Loss: 0.46546679735183716\n",
      "Epoch 83 | Batch 52 | Loss: 0.30808037519454956\n",
      "Epoch 83 | Batch 53 | Loss: 0.25029948353767395\n",
      "Epoch 83 | Batch 54 | Loss: 0.3638182282447815\n",
      "Epoch 83 | Batch 55 | Loss: 0.2540651559829712\n",
      "Epoch 83 | Batch 56 | Loss: 0.3785037696361542\n",
      "Epoch 83 | Batch 57 | Loss: 0.3973613381385803\n",
      "Epoch 83 | Batch 58 | Loss: 0.44932782649993896\n",
      "Epoch 83 | Batch 59 | Loss: 0.17369556427001953\n",
      "Epoch 83 | Batch 60 | Loss: 0.36611270904541016\n",
      "Epoch 83 | Batch 61 | Loss: 0.36151212453842163\n",
      "Epoch 83 | Batch 62 | Loss: 0.27297133207321167\n",
      "Epoch 83 | Batch 63 | Loss: 0.23380747437477112\n",
      "Epoch 83 | Batch 64 | Loss: 0.33268630504608154\n",
      "Epoch 83 | Batch 65 | Loss: 1.0866864919662476\n",
      "Epoch 83 | Batch 66 | Loss: 0.46710801124572754\n",
      "Epoch 83 | Batch 67 | Loss: 0.14825573563575745\n",
      "Epoch 83 | Batch 68 | Loss: 0.733534574508667\n",
      "Epoch 83 | Batch 69 | Loss: 0.217606320977211\n",
      "Epoch 83 | Batch 70 | Loss: 0.4965015649795532\n",
      "Epoch 83 | Batch 71 | Loss: 0.2329792082309723\n",
      "Epoch 83 | Batch 72 | Loss: 0.4091711640357971\n",
      "Epoch 83 | Batch 73 | Loss: 0.17679476737976074\n",
      "Epoch 83 | Batch 74 | Loss: 0.3826053738594055\n",
      "Epoch 83 | Batch 75 | Loss: 0.4202122688293457\n",
      "Epoch 83 | Batch 76 | Loss: 0.20981818437576294\n",
      "Epoch 83 | Batch 77 | Loss: 0.19070236384868622\n",
      "Epoch 83 | Batch 78 | Loss: 0.16427573561668396\n",
      "Epoch 83 | Batch 79 | Loss: 0.14736831188201904\n",
      "Epoch 83 | Batch 80 | Loss: 0.2654169201850891\n",
      "Epoch 83 | Batch 81 | Loss: 0.44555020332336426\n",
      "Epoch 83 | Batch 82 | Loss: 0.5009013414382935\n",
      "Epoch 83 | Batch 83 | Loss: 0.5685309171676636\n",
      "Epoch 83 | Batch 84 | Loss: 0.5396703481674194\n",
      "Epoch 83 | Batch 85 | Loss: 0.3170565366744995\n",
      "Epoch 83 | Batch 86 | Loss: 0.15765073895454407\n",
      "Epoch 83 | Batch 87 | Loss: 0.2508869767189026\n",
      "Epoch 83 | Batch 88 | Loss: 0.21893860399723053\n",
      "Epoch 83 | Batch 89 | Loss: 0.6465559005737305\n",
      "Epoch 83 | Batch 90 | Loss: 0.2604961097240448\n",
      "Epoch 84 | Batch 1 | Loss: 0.534083366394043\n",
      "Epoch 84 | Batch 2 | Loss: 0.3195192515850067\n",
      "Epoch 84 | Batch 3 | Loss: 0.4506101608276367\n",
      "Epoch 84 | Batch 4 | Loss: 0.34141242504119873\n",
      "Epoch 84 | Batch 5 | Loss: 0.12990331649780273\n",
      "Epoch 84 | Batch 6 | Loss: 0.44083327054977417\n",
      "Epoch 84 | Batch 7 | Loss: 0.9125410914421082\n",
      "Epoch 84 | Batch 8 | Loss: 0.2845834791660309\n",
      "Epoch 84 | Batch 9 | Loss: 0.18246357142925262\n",
      "Epoch 84 | Batch 10 | Loss: 0.39868488907814026\n",
      "Epoch 84 | Batch 11 | Loss: 0.27510473132133484\n",
      "Epoch 84 | Batch 12 | Loss: 0.1496424376964569\n",
      "Epoch 84 | Batch 13 | Loss: 0.49193212389945984\n",
      "Epoch 84 | Batch 14 | Loss: 0.3601701259613037\n",
      "Epoch 84 | Batch 15 | Loss: 0.5353014469146729\n",
      "Epoch 84 | Batch 16 | Loss: 0.42866992950439453\n",
      "Epoch 84 | Batch 17 | Loss: 0.5834416747093201\n",
      "Epoch 84 | Batch 18 | Loss: 0.3866908550262451\n",
      "Epoch 84 | Batch 19 | Loss: 0.3895742893218994\n",
      "Epoch 84 | Batch 20 | Loss: 0.9251292943954468\n",
      "Epoch 84 | Batch 21 | Loss: 0.3627799153327942\n",
      "Epoch 84 | Batch 22 | Loss: 0.27680808305740356\n",
      "Epoch 84 | Batch 23 | Loss: 0.35784032940864563\n",
      "Epoch 84 | Batch 24 | Loss: 0.4957520365715027\n",
      "Epoch 84 | Batch 25 | Loss: 0.5088228583335876\n",
      "Epoch 84 | Batch 26 | Loss: 0.14988721907138824\n",
      "Epoch 84 | Batch 27 | Loss: 0.22080397605895996\n",
      "Epoch 84 | Batch 28 | Loss: 0.4376745820045471\n",
      "Epoch 84 | Batch 29 | Loss: 0.3523007035255432\n",
      "Epoch 84 | Batch 30 | Loss: 0.3031230568885803\n",
      "Epoch 84 | Batch 31 | Loss: 0.0932944118976593\n",
      "Epoch 84 | Batch 32 | Loss: 0.5051183700561523\n",
      "Epoch 84 | Batch 33 | Loss: 0.41112181544303894\n",
      "Epoch 84 | Batch 34 | Loss: 0.17579659819602966\n",
      "Epoch 84 | Batch 35 | Loss: 0.30148524045944214\n",
      "Epoch 84 | Batch 36 | Loss: 0.22636778652668\n",
      "Epoch 84 | Batch 37 | Loss: 0.515693724155426\n",
      "Epoch 84 | Batch 38 | Loss: 0.3757741451263428\n",
      "Epoch 84 | Batch 39 | Loss: 0.2659655213356018\n",
      "Epoch 84 | Batch 40 | Loss: 0.4221436083316803\n",
      "Epoch 84 | Batch 41 | Loss: 0.11461324989795685\n",
      "Epoch 84 | Batch 42 | Loss: 0.7121266722679138\n",
      "Epoch 84 | Batch 43 | Loss: 0.3528596758842468\n",
      "Epoch 84 | Batch 44 | Loss: 0.4976884722709656\n",
      "Epoch 84 | Batch 45 | Loss: 0.3549773097038269\n",
      "Epoch 84 | Batch 46 | Loss: 0.3272244334220886\n",
      "Epoch 84 | Batch 47 | Loss: 0.4319319427013397\n",
      "Epoch 84 | Batch 48 | Loss: 0.25458258390426636\n",
      "Epoch 84 | Batch 49 | Loss: 0.2519008219242096\n",
      "Epoch 84 | Batch 50 | Loss: 0.7983924150466919\n",
      "Epoch 84 | Batch 51 | Loss: 0.13628414273262024\n",
      "Epoch 84 | Batch 52 | Loss: 0.26288625597953796\n",
      "Epoch 84 | Batch 53 | Loss: 0.1505632996559143\n",
      "Epoch 84 | Batch 54 | Loss: 0.4321422576904297\n",
      "Epoch 84 | Batch 55 | Loss: 0.17276285588741302\n",
      "Epoch 84 | Batch 56 | Loss: 0.12570513784885406\n",
      "Epoch 84 | Batch 57 | Loss: 0.04647878184914589\n",
      "Epoch 84 | Batch 58 | Loss: 0.12467797100543976\n",
      "Epoch 84 | Batch 59 | Loss: 0.8882489800453186\n",
      "Epoch 84 | Batch 60 | Loss: 0.43041425943374634\n",
      "Epoch 84 | Batch 61 | Loss: 0.4153684377670288\n",
      "Epoch 84 | Batch 62 | Loss: 0.24513357877731323\n",
      "Epoch 84 | Batch 63 | Loss: 0.3845180869102478\n",
      "Epoch 84 | Batch 64 | Loss: 0.39173245429992676\n",
      "Epoch 84 | Batch 65 | Loss: 0.17757242918014526\n",
      "Epoch 84 | Batch 66 | Loss: 0.1115771159529686\n",
      "Epoch 84 | Batch 67 | Loss: 0.5081678628921509\n",
      "Epoch 84 | Batch 68 | Loss: 0.1485663205385208\n",
      "Epoch 84 | Batch 69 | Loss: 0.09673885256052017\n",
      "Epoch 84 | Batch 70 | Loss: 0.09253561496734619\n",
      "Epoch 84 | Batch 71 | Loss: 0.28814369440078735\n",
      "Epoch 84 | Batch 72 | Loss: 0.1210370883345604\n",
      "Epoch 84 | Batch 73 | Loss: 0.3503135144710541\n",
      "Epoch 84 | Batch 74 | Loss: 0.2678770124912262\n",
      "Epoch 84 | Batch 75 | Loss: 0.1665600687265396\n",
      "Epoch 84 | Batch 76 | Loss: 0.6530623435974121\n",
      "Epoch 84 | Batch 77 | Loss: 0.64065021276474\n",
      "Epoch 84 | Batch 78 | Loss: 0.5032954812049866\n",
      "Epoch 84 | Batch 79 | Loss: 0.581871509552002\n",
      "Epoch 84 | Batch 80 | Loss: 0.5982384085655212\n",
      "Epoch 84 | Batch 81 | Loss: 0.45100605487823486\n",
      "Epoch 84 | Batch 82 | Loss: 0.27083152532577515\n",
      "Epoch 84 | Batch 83 | Loss: 0.18563388288021088\n",
      "Epoch 84 | Batch 84 | Loss: 0.5293416976928711\n",
      "Epoch 84 | Batch 85 | Loss: 0.46278899908065796\n",
      "Epoch 84 | Batch 86 | Loss: 0.42179369926452637\n",
      "Epoch 84 | Batch 87 | Loss: 0.230872243642807\n",
      "Epoch 84 | Batch 88 | Loss: 0.8858071565628052\n",
      "Epoch 84 | Batch 89 | Loss: 0.14904871582984924\n",
      "Epoch 84 | Batch 90 | Loss: 0.10326553881168365\n",
      "Epoch 85 | Batch 1 | Loss: 0.1535065472126007\n",
      "Epoch 85 | Batch 2 | Loss: 0.46110501885414124\n",
      "Epoch 85 | Batch 3 | Loss: 0.4402916431427002\n",
      "Epoch 85 | Batch 4 | Loss: 0.3652822971343994\n",
      "Epoch 85 | Batch 5 | Loss: 0.2857564687728882\n",
      "Epoch 85 | Batch 6 | Loss: 0.2770584225654602\n",
      "Epoch 85 | Batch 7 | Loss: 0.3088972270488739\n",
      "Epoch 85 | Batch 8 | Loss: 0.42647862434387207\n",
      "Epoch 85 | Batch 9 | Loss: 0.24086794257164001\n",
      "Epoch 85 | Batch 10 | Loss: 0.12643954157829285\n",
      "Epoch 85 | Batch 11 | Loss: 0.3514290452003479\n",
      "Epoch 85 | Batch 12 | Loss: 0.19667935371398926\n",
      "Epoch 85 | Batch 13 | Loss: 0.18748535215854645\n",
      "Epoch 85 | Batch 14 | Loss: 0.12068803608417511\n",
      "Epoch 85 | Batch 15 | Loss: 0.16330459713935852\n",
      "Epoch 85 | Batch 16 | Loss: 0.1801307648420334\n",
      "Epoch 85 | Batch 17 | Loss: 0.1602870225906372\n",
      "Epoch 85 | Batch 18 | Loss: 0.28321409225463867\n",
      "Epoch 85 | Batch 19 | Loss: 0.4764023721218109\n",
      "Epoch 85 | Batch 20 | Loss: 0.5016452074050903\n",
      "Epoch 85 | Batch 21 | Loss: 0.09535388648509979\n",
      "Epoch 85 | Batch 22 | Loss: 0.29264622926712036\n",
      "Epoch 85 | Batch 23 | Loss: 0.6726671457290649\n",
      "Epoch 85 | Batch 24 | Loss: 0.4591408371925354\n",
      "Epoch 85 | Batch 25 | Loss: 0.17453521490097046\n",
      "Epoch 85 | Batch 26 | Loss: 0.3535158634185791\n",
      "Epoch 85 | Batch 27 | Loss: 0.4323403239250183\n",
      "Epoch 85 | Batch 28 | Loss: 0.25542908906936646\n",
      "Epoch 85 | Batch 29 | Loss: 0.23928320407867432\n",
      "Epoch 85 | Batch 30 | Loss: 0.834111750125885\n",
      "Epoch 85 | Batch 31 | Loss: 0.16544565558433533\n",
      "Epoch 85 | Batch 32 | Loss: 0.8910893797874451\n",
      "Epoch 85 | Batch 33 | Loss: 0.35268980264663696\n",
      "Epoch 85 | Batch 34 | Loss: 0.3840528130531311\n",
      "Epoch 85 | Batch 35 | Loss: 0.4757186472415924\n",
      "Epoch 85 | Batch 36 | Loss: 0.360372394323349\n",
      "Epoch 85 | Batch 37 | Loss: 0.2362869381904602\n",
      "Epoch 85 | Batch 38 | Loss: 0.1639392226934433\n",
      "Epoch 85 | Batch 39 | Loss: 0.37005069851875305\n",
      "Epoch 85 | Batch 40 | Loss: 0.10675281286239624\n",
      "Epoch 85 | Batch 41 | Loss: 0.2740272283554077\n",
      "Epoch 85 | Batch 42 | Loss: 0.1741725504398346\n",
      "Epoch 85 | Batch 43 | Loss: 0.36510026454925537\n",
      "Epoch 85 | Batch 44 | Loss: 0.12064816057682037\n",
      "Epoch 85 | Batch 45 | Loss: 0.9086005091667175\n",
      "Epoch 85 | Batch 46 | Loss: 0.4690825939178467\n",
      "Epoch 85 | Batch 47 | Loss: 0.2899089753627777\n",
      "Epoch 85 | Batch 48 | Loss: 0.2730254828929901\n",
      "Epoch 85 | Batch 49 | Loss: 0.5360636711120605\n",
      "Epoch 85 | Batch 50 | Loss: 0.545252799987793\n",
      "Epoch 85 | Batch 51 | Loss: 0.30888068675994873\n",
      "Epoch 85 | Batch 52 | Loss: 0.4901050925254822\n",
      "Epoch 85 | Batch 53 | Loss: 0.23241400718688965\n",
      "Epoch 85 | Batch 54 | Loss: 0.5514675974845886\n",
      "Epoch 85 | Batch 55 | Loss: 0.5379366874694824\n",
      "Epoch 85 | Batch 56 | Loss: 0.12461227178573608\n",
      "Epoch 85 | Batch 57 | Loss: 0.252973735332489\n",
      "Epoch 85 | Batch 58 | Loss: 0.18229787051677704\n",
      "Epoch 85 | Batch 59 | Loss: 0.17238843441009521\n",
      "Epoch 85 | Batch 60 | Loss: 0.33756256103515625\n",
      "Epoch 85 | Batch 61 | Loss: 0.506874680519104\n",
      "Epoch 85 | Batch 62 | Loss: 0.2853570580482483\n",
      "Epoch 85 | Batch 63 | Loss: 0.2841563820838928\n",
      "Epoch 85 | Batch 64 | Loss: 0.1932503581047058\n",
      "Epoch 85 | Batch 65 | Loss: 0.4253775477409363\n",
      "Epoch 85 | Batch 66 | Loss: 0.6852431893348694\n",
      "Epoch 85 | Batch 67 | Loss: 1.0985360145568848\n",
      "Epoch 85 | Batch 68 | Loss: 0.6329039335250854\n",
      "Epoch 85 | Batch 69 | Loss: 0.1965377926826477\n",
      "Epoch 85 | Batch 70 | Loss: 0.4495065212249756\n",
      "Epoch 85 | Batch 71 | Loss: 0.10746189951896667\n",
      "Epoch 85 | Batch 72 | Loss: 0.3075611889362335\n",
      "Epoch 85 | Batch 73 | Loss: 0.7110137939453125\n",
      "Epoch 85 | Batch 74 | Loss: 0.2248561680316925\n",
      "Epoch 85 | Batch 75 | Loss: 0.36828988790512085\n",
      "Epoch 85 | Batch 76 | Loss: 0.08905360102653503\n",
      "Epoch 85 | Batch 77 | Loss: 0.5248299241065979\n",
      "Epoch 85 | Batch 78 | Loss: 0.09041506797075272\n",
      "Epoch 85 | Batch 79 | Loss: 0.17668205499649048\n",
      "Epoch 85 | Batch 80 | Loss: 0.14914178848266602\n",
      "Epoch 85 | Batch 81 | Loss: 0.9455331563949585\n",
      "Epoch 85 | Batch 82 | Loss: 0.3532622456550598\n",
      "Epoch 85 | Batch 83 | Loss: 0.12312313169240952\n",
      "Epoch 85 | Batch 84 | Loss: 0.7349790334701538\n",
      "Epoch 85 | Batch 85 | Loss: 0.07124596834182739\n",
      "Epoch 85 | Batch 86 | Loss: 0.33750492334365845\n",
      "Epoch 85 | Batch 87 | Loss: 0.3443262577056885\n",
      "Epoch 85 | Batch 88 | Loss: 0.8340258598327637\n",
      "Epoch 85 | Batch 89 | Loss: 0.644881010055542\n",
      "Epoch 85 | Batch 90 | Loss: 0.17776083946228027\n",
      "Epoch 86 | Batch 1 | Loss: 0.2974695861339569\n",
      "Epoch 86 | Batch 2 | Loss: 0.5260919332504272\n",
      "Epoch 86 | Batch 3 | Loss: 0.403258353471756\n",
      "Epoch 86 | Batch 4 | Loss: 0.5150617361068726\n",
      "Epoch 86 | Batch 5 | Loss: 0.1683732271194458\n",
      "Epoch 86 | Batch 6 | Loss: 0.4252001643180847\n",
      "Epoch 86 | Batch 7 | Loss: 0.18648257851600647\n",
      "Epoch 86 | Batch 8 | Loss: 0.595028281211853\n",
      "Epoch 86 | Batch 9 | Loss: 0.15160366892814636\n",
      "Epoch 86 | Batch 10 | Loss: 0.11045917868614197\n",
      "Epoch 86 | Batch 11 | Loss: 0.5043182969093323\n",
      "Epoch 86 | Batch 12 | Loss: 0.1152058020234108\n",
      "Epoch 86 | Batch 13 | Loss: 0.6077763438224792\n",
      "Epoch 86 | Batch 14 | Loss: 0.3041090667247772\n",
      "Epoch 86 | Batch 15 | Loss: 0.23341336846351624\n",
      "Epoch 86 | Batch 16 | Loss: 0.288673996925354\n",
      "Epoch 86 | Batch 17 | Loss: 0.3680064380168915\n",
      "Epoch 86 | Batch 18 | Loss: 0.37275266647338867\n",
      "Epoch 86 | Batch 19 | Loss: 0.2551710605621338\n",
      "Epoch 86 | Batch 20 | Loss: 0.6166183352470398\n",
      "Epoch 86 | Batch 21 | Loss: 0.13199833035469055\n",
      "Epoch 86 | Batch 22 | Loss: 0.2993509769439697\n",
      "Epoch 86 | Batch 23 | Loss: 0.5522927045822144\n",
      "Epoch 86 | Batch 24 | Loss: 0.6231291890144348\n",
      "Epoch 86 | Batch 25 | Loss: 0.26817500591278076\n",
      "Epoch 86 | Batch 26 | Loss: 0.6512285470962524\n",
      "Epoch 86 | Batch 27 | Loss: 0.13812296092510223\n",
      "Epoch 86 | Batch 28 | Loss: 0.12239724397659302\n",
      "Epoch 86 | Batch 29 | Loss: 0.4394705295562744\n",
      "Epoch 86 | Batch 30 | Loss: 0.2003173828125\n",
      "Epoch 86 | Batch 31 | Loss: 0.2348453402519226\n",
      "Epoch 86 | Batch 32 | Loss: 0.3509746491909027\n",
      "Epoch 86 | Batch 33 | Loss: 0.42917728424072266\n",
      "Epoch 86 | Batch 34 | Loss: 0.6532489061355591\n",
      "Epoch 86 | Batch 35 | Loss: 0.13333797454833984\n",
      "Epoch 86 | Batch 36 | Loss: 0.3284416198730469\n",
      "Epoch 86 | Batch 37 | Loss: 0.28441983461380005\n",
      "Epoch 86 | Batch 38 | Loss: 0.19673509895801544\n",
      "Epoch 86 | Batch 39 | Loss: 0.13161277770996094\n",
      "Epoch 86 | Batch 40 | Loss: 0.17484818398952484\n",
      "Epoch 86 | Batch 41 | Loss: 0.07997901737689972\n",
      "Epoch 86 | Batch 42 | Loss: 0.4281361699104309\n",
      "Epoch 86 | Batch 43 | Loss: 0.24831286072731018\n",
      "Epoch 86 | Batch 44 | Loss: 0.44061344861984253\n",
      "Epoch 86 | Batch 45 | Loss: 0.0787452906370163\n",
      "Epoch 86 | Batch 46 | Loss: 1.0505685806274414\n",
      "Epoch 86 | Batch 47 | Loss: 0.12725159525871277\n",
      "Epoch 86 | Batch 48 | Loss: 0.4206351041793823\n",
      "Epoch 86 | Batch 49 | Loss: 0.23717358708381653\n",
      "Epoch 86 | Batch 50 | Loss: 0.31168630719184875\n",
      "Epoch 86 | Batch 51 | Loss: 0.1565130650997162\n",
      "Epoch 86 | Batch 52 | Loss: 0.4463760256767273\n",
      "Epoch 86 | Batch 53 | Loss: 0.3076520264148712\n",
      "Epoch 86 | Batch 54 | Loss: 0.5648097395896912\n",
      "Epoch 86 | Batch 55 | Loss: 0.2782074809074402\n",
      "Epoch 86 | Batch 56 | Loss: 0.9142841696739197\n",
      "Epoch 86 | Batch 57 | Loss: 0.16154319047927856\n",
      "Epoch 86 | Batch 58 | Loss: 0.6296919584274292\n",
      "Epoch 86 | Batch 59 | Loss: 0.16366557776927948\n",
      "Epoch 86 | Batch 60 | Loss: 0.3490932881832123\n",
      "Epoch 86 | Batch 61 | Loss: 0.35170361399650574\n",
      "Epoch 86 | Batch 62 | Loss: 0.3759773075580597\n",
      "Epoch 86 | Batch 63 | Loss: 0.25035592913627625\n",
      "Epoch 86 | Batch 64 | Loss: 0.40590447187423706\n",
      "Epoch 86 | Batch 65 | Loss: 0.1278914362192154\n",
      "Epoch 86 | Batch 66 | Loss: 0.5213155150413513\n",
      "Epoch 86 | Batch 67 | Loss: 0.1334577202796936\n",
      "Epoch 86 | Batch 68 | Loss: 0.22908011078834534\n",
      "Epoch 86 | Batch 69 | Loss: 0.6268644332885742\n",
      "Epoch 86 | Batch 70 | Loss: 0.3498336672782898\n",
      "Epoch 86 | Batch 71 | Loss: 0.5020622611045837\n",
      "Epoch 86 | Batch 72 | Loss: 0.5926796197891235\n",
      "Epoch 86 | Batch 73 | Loss: 0.4717678725719452\n",
      "Epoch 86 | Batch 74 | Loss: 0.2840118706226349\n",
      "Epoch 86 | Batch 75 | Loss: 0.09159031510353088\n",
      "Epoch 86 | Batch 76 | Loss: 0.3331744372844696\n",
      "Epoch 86 | Batch 77 | Loss: 0.8351284265518188\n",
      "Epoch 86 | Batch 78 | Loss: 0.5950882434844971\n",
      "Epoch 86 | Batch 79 | Loss: 0.3741644620895386\n",
      "Epoch 86 | Batch 80 | Loss: 1.0636504888534546\n",
      "Epoch 86 | Batch 81 | Loss: 0.24110499024391174\n",
      "Epoch 86 | Batch 82 | Loss: 0.31803223490715027\n",
      "Epoch 86 | Batch 83 | Loss: 0.43926727771759033\n",
      "Epoch 86 | Batch 84 | Loss: 0.3983374238014221\n",
      "Epoch 86 | Batch 85 | Loss: 0.5882855653762817\n",
      "Epoch 86 | Batch 86 | Loss: 0.4158000946044922\n",
      "Epoch 86 | Batch 87 | Loss: 0.219882071018219\n",
      "Epoch 86 | Batch 88 | Loss: 0.6629810929298401\n",
      "Epoch 86 | Batch 89 | Loss: 0.3814737796783447\n",
      "Epoch 86 | Batch 90 | Loss: 0.08088143914937973\n",
      "Epoch 87 | Batch 1 | Loss: 0.18934328854084015\n",
      "Epoch 87 | Batch 2 | Loss: 0.38500910997390747\n",
      "Epoch 87 | Batch 3 | Loss: 0.2999120354652405\n",
      "Epoch 87 | Batch 4 | Loss: 0.3431277275085449\n",
      "Epoch 87 | Batch 5 | Loss: 0.1601470708847046\n",
      "Epoch 87 | Batch 6 | Loss: 0.5887655019760132\n",
      "Epoch 87 | Batch 7 | Loss: 0.26958054304122925\n",
      "Epoch 87 | Batch 8 | Loss: 0.5028026700019836\n",
      "Epoch 87 | Batch 9 | Loss: 0.39231058955192566\n",
      "Epoch 87 | Batch 10 | Loss: 0.18302902579307556\n",
      "Epoch 87 | Batch 11 | Loss: 0.40943673253059387\n",
      "Epoch 87 | Batch 12 | Loss: 0.6262918710708618\n",
      "Epoch 87 | Batch 13 | Loss: 0.21850402653217316\n",
      "Epoch 87 | Batch 14 | Loss: 0.27142512798309326\n",
      "Epoch 87 | Batch 15 | Loss: 0.1449638307094574\n",
      "Epoch 87 | Batch 16 | Loss: 0.6105085611343384\n",
      "Epoch 87 | Batch 17 | Loss: 0.622726559638977\n",
      "Epoch 87 | Batch 18 | Loss: 0.13179606199264526\n",
      "Epoch 87 | Batch 19 | Loss: 0.1495351493358612\n",
      "Epoch 87 | Batch 20 | Loss: 0.14159303903579712\n",
      "Epoch 87 | Batch 21 | Loss: 0.3520372211933136\n",
      "Epoch 87 | Batch 22 | Loss: 0.44900041818618774\n",
      "Epoch 87 | Batch 23 | Loss: 0.412520170211792\n",
      "Epoch 87 | Batch 24 | Loss: 0.4333590269088745\n",
      "Epoch 87 | Batch 25 | Loss: 0.6048517227172852\n",
      "Epoch 87 | Batch 26 | Loss: 0.6710612773895264\n",
      "Epoch 87 | Batch 27 | Loss: 0.198985755443573\n",
      "Epoch 87 | Batch 28 | Loss: 0.43003034591674805\n",
      "Epoch 87 | Batch 29 | Loss: 0.8332536220550537\n",
      "Epoch 87 | Batch 30 | Loss: 0.5344829559326172\n",
      "Epoch 87 | Batch 31 | Loss: 0.1756715625524521\n",
      "Epoch 87 | Batch 32 | Loss: 0.41947734355926514\n",
      "Epoch 87 | Batch 33 | Loss: 0.7531967163085938\n",
      "Epoch 87 | Batch 34 | Loss: 0.460909366607666\n",
      "Epoch 87 | Batch 35 | Loss: 0.14378750324249268\n",
      "Epoch 87 | Batch 36 | Loss: 0.2705477476119995\n",
      "Epoch 87 | Batch 37 | Loss: 0.32034575939178467\n",
      "Epoch 87 | Batch 38 | Loss: 0.3665245771408081\n",
      "Epoch 87 | Batch 39 | Loss: 0.16834111511707306\n",
      "Epoch 87 | Batch 40 | Loss: 0.38035017251968384\n",
      "Epoch 87 | Batch 41 | Loss: 0.44581979513168335\n",
      "Epoch 87 | Batch 42 | Loss: 0.5732327699661255\n",
      "Epoch 87 | Batch 43 | Loss: 0.15505239367485046\n",
      "Epoch 87 | Batch 44 | Loss: 0.3348459005355835\n",
      "Epoch 87 | Batch 45 | Loss: 0.5821121335029602\n",
      "Epoch 87 | Batch 46 | Loss: 0.5765250325202942\n",
      "Epoch 87 | Batch 47 | Loss: 0.3332989811897278\n",
      "Epoch 87 | Batch 48 | Loss: 0.5039728879928589\n",
      "Epoch 87 | Batch 49 | Loss: 0.46807268261909485\n",
      "Epoch 87 | Batch 50 | Loss: 0.2379608005285263\n",
      "Epoch 87 | Batch 51 | Loss: 0.34806057810783386\n",
      "Epoch 87 | Batch 52 | Loss: 0.23223741352558136\n",
      "Epoch 87 | Batch 53 | Loss: 0.16681727766990662\n",
      "Epoch 87 | Batch 54 | Loss: 0.15391424298286438\n",
      "Epoch 87 | Batch 55 | Loss: 0.3224058151245117\n",
      "Epoch 87 | Batch 56 | Loss: 0.5295389890670776\n",
      "Epoch 87 | Batch 57 | Loss: 0.24590279161930084\n",
      "Epoch 87 | Batch 58 | Loss: 0.23319950699806213\n",
      "Epoch 87 | Batch 59 | Loss: 0.18261229991912842\n",
      "Epoch 87 | Batch 60 | Loss: 0.18373803794384003\n",
      "Epoch 87 | Batch 61 | Loss: 0.11098708212375641\n",
      "Epoch 87 | Batch 62 | Loss: 0.399539977312088\n",
      "Epoch 87 | Batch 63 | Loss: 0.3705996870994568\n",
      "Epoch 87 | Batch 64 | Loss: 0.31082966923713684\n",
      "Epoch 87 | Batch 65 | Loss: 0.23020543158054352\n",
      "Epoch 87 | Batch 66 | Loss: 0.1652824580669403\n",
      "Epoch 87 | Batch 67 | Loss: 0.36588042974472046\n",
      "Epoch 87 | Batch 68 | Loss: 0.5426163673400879\n",
      "Epoch 87 | Batch 69 | Loss: 0.18166814744472504\n",
      "Epoch 87 | Batch 70 | Loss: 0.08480474352836609\n",
      "Epoch 87 | Batch 71 | Loss: 0.42253684997558594\n",
      "Epoch 87 | Batch 72 | Loss: 0.3946402072906494\n",
      "Epoch 87 | Batch 73 | Loss: 0.6057754755020142\n",
      "Epoch 87 | Batch 74 | Loss: 0.32200267910957336\n",
      "Epoch 87 | Batch 75 | Loss: 0.11816444993019104\n",
      "Epoch 87 | Batch 76 | Loss: 0.3871549069881439\n",
      "Epoch 87 | Batch 77 | Loss: 0.2296106070280075\n",
      "Epoch 87 | Batch 78 | Loss: 0.6695302724838257\n",
      "Epoch 87 | Batch 79 | Loss: 0.1638212651014328\n",
      "Epoch 87 | Batch 80 | Loss: 0.3475452661514282\n",
      "Epoch 87 | Batch 81 | Loss: 0.7006884813308716\n",
      "Epoch 87 | Batch 82 | Loss: 0.4644813537597656\n",
      "Epoch 87 | Batch 83 | Loss: 0.24246904253959656\n",
      "Epoch 87 | Batch 84 | Loss: 0.253627210855484\n",
      "Epoch 87 | Batch 85 | Loss: 0.5753368139266968\n",
      "Epoch 87 | Batch 86 | Loss: 0.16362033784389496\n",
      "Epoch 87 | Batch 87 | Loss: 0.2781487703323364\n",
      "Epoch 87 | Batch 88 | Loss: 0.45821845531463623\n",
      "Epoch 87 | Batch 89 | Loss: 0.8177504539489746\n",
      "Epoch 87 | Batch 90 | Loss: 1.9314460754394531\n",
      "Epoch 88 | Batch 1 | Loss: 0.15923646092414856\n",
      "Epoch 88 | Batch 2 | Loss: 0.27459585666656494\n",
      "Epoch 88 | Batch 3 | Loss: 0.3949536085128784\n",
      "Epoch 88 | Batch 4 | Loss: 0.3314347267150879\n",
      "Epoch 88 | Batch 5 | Loss: 0.3766791820526123\n",
      "Epoch 88 | Batch 6 | Loss: 0.43721988797187805\n",
      "Epoch 88 | Batch 7 | Loss: 0.44947195053100586\n",
      "Epoch 88 | Batch 8 | Loss: 0.36399778723716736\n",
      "Epoch 88 | Batch 9 | Loss: 0.18714144825935364\n",
      "Epoch 88 | Batch 10 | Loss: 0.7503472566604614\n",
      "Epoch 88 | Batch 11 | Loss: 0.12564074993133545\n",
      "Epoch 88 | Batch 12 | Loss: 0.37071743607521057\n",
      "Epoch 88 | Batch 13 | Loss: 0.21265777945518494\n",
      "Epoch 88 | Batch 14 | Loss: 0.3965686559677124\n",
      "Epoch 88 | Batch 15 | Loss: 0.34145402908325195\n",
      "Epoch 88 | Batch 16 | Loss: 0.44896960258483887\n",
      "Epoch 88 | Batch 17 | Loss: 0.8711111545562744\n",
      "Epoch 88 | Batch 18 | Loss: 0.7495931386947632\n",
      "Epoch 88 | Batch 19 | Loss: 0.07709522545337677\n",
      "Epoch 88 | Batch 20 | Loss: 0.829679012298584\n",
      "Epoch 88 | Batch 21 | Loss: 0.18026383221149445\n",
      "Epoch 88 | Batch 22 | Loss: 0.5023518204689026\n",
      "Epoch 88 | Batch 23 | Loss: 0.20588186383247375\n",
      "Epoch 88 | Batch 24 | Loss: 0.44587579369544983\n",
      "Epoch 88 | Batch 25 | Loss: 0.1862810105085373\n",
      "Epoch 88 | Batch 26 | Loss: 0.403578519821167\n",
      "Epoch 88 | Batch 27 | Loss: 0.5537391901016235\n",
      "Epoch 88 | Batch 28 | Loss: 0.3647707998752594\n",
      "Epoch 88 | Batch 29 | Loss: 0.5442004799842834\n",
      "Epoch 88 | Batch 30 | Loss: 0.15479256212711334\n",
      "Epoch 88 | Batch 31 | Loss: 0.20535603165626526\n",
      "Epoch 88 | Batch 32 | Loss: 0.7827248573303223\n",
      "Epoch 88 | Batch 33 | Loss: 0.40232691168785095\n",
      "Epoch 88 | Batch 34 | Loss: 0.3812828063964844\n",
      "Epoch 88 | Batch 35 | Loss: 0.5735762119293213\n",
      "Epoch 88 | Batch 36 | Loss: 0.46545547246932983\n",
      "Epoch 88 | Batch 37 | Loss: 0.3606267273426056\n",
      "Epoch 88 | Batch 38 | Loss: 0.17436483502388\n",
      "Epoch 88 | Batch 39 | Loss: 0.446846067905426\n",
      "Epoch 88 | Batch 40 | Loss: 0.4645783603191376\n",
      "Epoch 88 | Batch 41 | Loss: 0.11973616480827332\n",
      "Epoch 88 | Batch 42 | Loss: 0.3557748794555664\n",
      "Epoch 88 | Batch 43 | Loss: 0.44504836201667786\n",
      "Epoch 88 | Batch 44 | Loss: 0.22115439176559448\n",
      "Epoch 88 | Batch 45 | Loss: 0.21227958798408508\n",
      "Epoch 88 | Batch 46 | Loss: 0.36865514516830444\n",
      "Epoch 88 | Batch 47 | Loss: 0.5867598056793213\n",
      "Epoch 88 | Batch 48 | Loss: 0.38747110962867737\n",
      "Epoch 88 | Batch 49 | Loss: 0.26879140734672546\n",
      "Epoch 88 | Batch 50 | Loss: 0.16349947452545166\n",
      "Epoch 88 | Batch 51 | Loss: 0.34441423416137695\n",
      "Epoch 88 | Batch 52 | Loss: 0.32737115025520325\n",
      "Epoch 88 | Batch 53 | Loss: 0.20508380234241486\n",
      "Epoch 88 | Batch 54 | Loss: 0.49591541290283203\n",
      "Epoch 88 | Batch 55 | Loss: 0.3184458911418915\n",
      "Epoch 88 | Batch 56 | Loss: 0.525615394115448\n",
      "Epoch 88 | Batch 57 | Loss: 0.14403562247753143\n",
      "Epoch 88 | Batch 58 | Loss: 0.3590204417705536\n",
      "Epoch 88 | Batch 59 | Loss: 0.4859507083892822\n",
      "Epoch 88 | Batch 60 | Loss: 0.17369410395622253\n",
      "Epoch 88 | Batch 61 | Loss: 0.3209901750087738\n",
      "Epoch 88 | Batch 62 | Loss: 0.15749499201774597\n",
      "Epoch 88 | Batch 63 | Loss: 0.15670263767242432\n",
      "Epoch 88 | Batch 64 | Loss: 0.41965290904045105\n",
      "Epoch 88 | Batch 65 | Loss: 0.4289357364177704\n",
      "Epoch 88 | Batch 66 | Loss: 0.26920437812805176\n",
      "Epoch 88 | Batch 67 | Loss: 0.18107882142066956\n",
      "Epoch 88 | Batch 68 | Loss: 0.1372484266757965\n",
      "Epoch 88 | Batch 69 | Loss: 0.2265518605709076\n",
      "Epoch 88 | Batch 70 | Loss: 0.25377151370048523\n",
      "Epoch 88 | Batch 71 | Loss: 0.09977006167173386\n",
      "Epoch 88 | Batch 72 | Loss: 0.17773908376693726\n",
      "Epoch 88 | Batch 73 | Loss: 0.37179118394851685\n",
      "Epoch 88 | Batch 74 | Loss: 0.3076359033584595\n",
      "Epoch 88 | Batch 75 | Loss: 0.2126341462135315\n",
      "Epoch 88 | Batch 76 | Loss: 0.15585005283355713\n",
      "Epoch 88 | Batch 77 | Loss: 0.5434813499450684\n",
      "Epoch 88 | Batch 78 | Loss: 0.7281848192214966\n",
      "Epoch 88 | Batch 79 | Loss: 0.5882012844085693\n",
      "Epoch 88 | Batch 80 | Loss: 0.09312975406646729\n",
      "Epoch 88 | Batch 81 | Loss: 0.5527982711791992\n",
      "Epoch 88 | Batch 82 | Loss: 0.2644966244697571\n",
      "Epoch 88 | Batch 83 | Loss: 0.2843286097049713\n",
      "Epoch 88 | Batch 84 | Loss: 0.5404084324836731\n",
      "Epoch 88 | Batch 85 | Loss: 0.5517500042915344\n",
      "Epoch 88 | Batch 86 | Loss: 0.30069154500961304\n",
      "Epoch 88 | Batch 87 | Loss: 0.2766239643096924\n",
      "Epoch 88 | Batch 88 | Loss: 1.0072910785675049\n",
      "Epoch 88 | Batch 89 | Loss: 0.3057766556739807\n",
      "Epoch 88 | Batch 90 | Loss: 0.00022133653692435473\n",
      "Epoch 89 | Batch 1 | Loss: 0.25851941108703613\n",
      "Epoch 89 | Batch 2 | Loss: 0.5616503953933716\n",
      "Epoch 89 | Batch 3 | Loss: 0.1689411848783493\n",
      "Epoch 89 | Batch 4 | Loss: 0.33499234914779663\n",
      "Epoch 89 | Batch 5 | Loss: 0.3327213227748871\n",
      "Epoch 89 | Batch 6 | Loss: 0.13539394736289978\n",
      "Epoch 89 | Batch 7 | Loss: 0.35279810428619385\n",
      "Epoch 89 | Batch 8 | Loss: 0.41271963715553284\n",
      "Epoch 89 | Batch 9 | Loss: 0.5003482699394226\n",
      "Epoch 89 | Batch 10 | Loss: 0.5510989427566528\n",
      "Epoch 89 | Batch 11 | Loss: 0.09516342729330063\n",
      "Epoch 89 | Batch 12 | Loss: 0.397152304649353\n",
      "Epoch 89 | Batch 13 | Loss: 0.324325829744339\n",
      "Epoch 89 | Batch 14 | Loss: 0.1258225440979004\n",
      "Epoch 89 | Batch 15 | Loss: 0.5308545231819153\n",
      "Epoch 89 | Batch 16 | Loss: 0.5842757225036621\n",
      "Epoch 89 | Batch 17 | Loss: 0.7785217761993408\n",
      "Epoch 89 | Batch 18 | Loss: 0.135626882314682\n",
      "Epoch 89 | Batch 19 | Loss: 0.35983890295028687\n",
      "Epoch 89 | Batch 20 | Loss: 0.3329322636127472\n",
      "Epoch 89 | Batch 21 | Loss: 0.2634678781032562\n",
      "Epoch 89 | Batch 22 | Loss: 0.2879047691822052\n",
      "Epoch 89 | Batch 23 | Loss: 0.45066431164741516\n",
      "Epoch 89 | Batch 24 | Loss: 0.6222801208496094\n",
      "Epoch 89 | Batch 25 | Loss: 0.4484047293663025\n",
      "Epoch 89 | Batch 26 | Loss: 0.1252335011959076\n",
      "Epoch 89 | Batch 27 | Loss: 0.6772730350494385\n",
      "Epoch 89 | Batch 28 | Loss: 0.2931080460548401\n",
      "Epoch 89 | Batch 29 | Loss: 0.25181272625923157\n",
      "Epoch 89 | Batch 30 | Loss: 0.4607483744621277\n",
      "Epoch 89 | Batch 31 | Loss: 0.40237247943878174\n",
      "Epoch 89 | Batch 32 | Loss: 0.38402897119522095\n",
      "Epoch 89 | Batch 33 | Loss: 0.1866818219423294\n",
      "Epoch 89 | Batch 34 | Loss: 0.646268367767334\n",
      "Epoch 89 | Batch 35 | Loss: 0.4318885803222656\n",
      "Epoch 89 | Batch 36 | Loss: 0.22456873953342438\n",
      "Epoch 89 | Batch 37 | Loss: 0.4721391797065735\n",
      "Epoch 89 | Batch 38 | Loss: 0.431418776512146\n",
      "Epoch 89 | Batch 39 | Loss: 0.35937801003456116\n",
      "Epoch 89 | Batch 40 | Loss: 0.5033029317855835\n",
      "Epoch 89 | Batch 41 | Loss: 0.23407824337482452\n",
      "Epoch 89 | Batch 42 | Loss: 0.4486427307128906\n",
      "Epoch 89 | Batch 43 | Loss: 0.6228772401809692\n",
      "Epoch 89 | Batch 44 | Loss: 0.4241677522659302\n",
      "Epoch 89 | Batch 45 | Loss: 0.11971168220043182\n",
      "Epoch 89 | Batch 46 | Loss: 0.36554014682769775\n",
      "Epoch 89 | Batch 47 | Loss: 0.4260166883468628\n",
      "Epoch 89 | Batch 48 | Loss: 0.2645367980003357\n",
      "Epoch 89 | Batch 49 | Loss: 0.5239616632461548\n",
      "Epoch 89 | Batch 50 | Loss: 0.5706661343574524\n",
      "Epoch 89 | Batch 51 | Loss: 0.38604748249053955\n",
      "Epoch 89 | Batch 52 | Loss: 0.18995560705661774\n",
      "Epoch 89 | Batch 53 | Loss: 0.32552093267440796\n",
      "Epoch 89 | Batch 54 | Loss: 0.2849956154823303\n",
      "Epoch 89 | Batch 55 | Loss: 0.41651588678359985\n",
      "Epoch 89 | Batch 56 | Loss: 0.6163376569747925\n",
      "Epoch 89 | Batch 57 | Loss: 0.19480454921722412\n",
      "Epoch 89 | Batch 58 | Loss: 0.19434714317321777\n",
      "Epoch 89 | Batch 59 | Loss: 0.6023223400115967\n",
      "Epoch 89 | Batch 60 | Loss: 0.1261763870716095\n",
      "Epoch 89 | Batch 61 | Loss: 0.4207199215888977\n",
      "Epoch 89 | Batch 62 | Loss: 0.5850257277488708\n",
      "Epoch 89 | Batch 63 | Loss: 0.49513471126556396\n",
      "Epoch 89 | Batch 64 | Loss: 0.5527109503746033\n",
      "Epoch 89 | Batch 65 | Loss: 0.2852756083011627\n",
      "Epoch 89 | Batch 66 | Loss: 0.21718667447566986\n",
      "Epoch 89 | Batch 67 | Loss: 0.3605065941810608\n",
      "Epoch 89 | Batch 68 | Loss: 0.19833600521087646\n",
      "Epoch 89 | Batch 69 | Loss: 0.26162418723106384\n",
      "Epoch 89 | Batch 70 | Loss: 0.5777459144592285\n",
      "Epoch 89 | Batch 71 | Loss: 0.2936893701553345\n",
      "Epoch 89 | Batch 72 | Loss: 0.2856055498123169\n",
      "Epoch 89 | Batch 73 | Loss: 0.21659806370735168\n",
      "Epoch 89 | Batch 74 | Loss: 0.5322249531745911\n",
      "Epoch 89 | Batch 75 | Loss: 0.0736679881811142\n",
      "Epoch 89 | Batch 76 | Loss: 0.8202954530715942\n",
      "Epoch 89 | Batch 77 | Loss: 0.5965901017189026\n",
      "Epoch 89 | Batch 78 | Loss: 0.14577457308769226\n",
      "Epoch 89 | Batch 79 | Loss: 0.375626802444458\n",
      "Epoch 89 | Batch 80 | Loss: 0.1379648745059967\n",
      "Epoch 89 | Batch 81 | Loss: 0.17794284224510193\n",
      "Epoch 89 | Batch 82 | Loss: 0.2569197416305542\n",
      "Epoch 89 | Batch 83 | Loss: 0.2739751935005188\n",
      "Epoch 89 | Batch 84 | Loss: 0.1497150957584381\n",
      "Epoch 89 | Batch 85 | Loss: 0.5120173692703247\n",
      "Epoch 89 | Batch 86 | Loss: 0.08364299684762955\n",
      "Epoch 89 | Batch 87 | Loss: 0.20308884978294373\n",
      "Epoch 89 | Batch 88 | Loss: 0.7243988513946533\n",
      "Epoch 89 | Batch 89 | Loss: 0.38203075528144836\n",
      "Epoch 89 | Batch 90 | Loss: 0.11730709671974182\n",
      "Epoch 90 | Batch 1 | Loss: 0.6231434941291809\n",
      "Epoch 90 | Batch 2 | Loss: 0.4216045141220093\n",
      "Epoch 90 | Batch 3 | Loss: 0.22028513252735138\n",
      "Epoch 90 | Batch 4 | Loss: 0.16079837083816528\n",
      "Epoch 90 | Batch 5 | Loss: 0.5216963887214661\n",
      "Epoch 90 | Batch 6 | Loss: 0.35310450196266174\n",
      "Epoch 90 | Batch 7 | Loss: 0.412325918674469\n",
      "Epoch 90 | Batch 8 | Loss: 0.4502778947353363\n",
      "Epoch 90 | Batch 9 | Loss: 0.13908931612968445\n",
      "Epoch 90 | Batch 10 | Loss: 0.22312679886817932\n",
      "Epoch 90 | Batch 11 | Loss: 0.778854250907898\n",
      "Epoch 90 | Batch 12 | Loss: 0.11675851047039032\n",
      "Epoch 90 | Batch 13 | Loss: 0.40287575125694275\n",
      "Epoch 90 | Batch 14 | Loss: 0.5675516128540039\n",
      "Epoch 90 | Batch 15 | Loss: 0.24722209572792053\n",
      "Epoch 90 | Batch 16 | Loss: 0.401081383228302\n",
      "Epoch 90 | Batch 17 | Loss: 0.32255998253822327\n",
      "Epoch 90 | Batch 18 | Loss: 0.45418232679367065\n",
      "Epoch 90 | Batch 19 | Loss: 0.3803444504737854\n",
      "Epoch 90 | Batch 20 | Loss: 0.7372817993164062\n",
      "Epoch 90 | Batch 21 | Loss: 0.33152639865875244\n",
      "Epoch 90 | Batch 22 | Loss: 0.6623061895370483\n",
      "Epoch 90 | Batch 23 | Loss: 0.4276963770389557\n",
      "Epoch 90 | Batch 24 | Loss: 0.2372964769601822\n",
      "Epoch 90 | Batch 25 | Loss: 0.5886185169219971\n",
      "Epoch 90 | Batch 26 | Loss: 0.22395896911621094\n",
      "Epoch 90 | Batch 27 | Loss: 0.12447312474250793\n",
      "Epoch 90 | Batch 28 | Loss: 0.1832490712404251\n",
      "Epoch 90 | Batch 29 | Loss: 0.3848750591278076\n",
      "Epoch 90 | Batch 30 | Loss: 0.5285825729370117\n",
      "Epoch 90 | Batch 31 | Loss: 0.41520729660987854\n",
      "Epoch 90 | Batch 32 | Loss: 0.6624593138694763\n",
      "Epoch 90 | Batch 33 | Loss: 0.4012312591075897\n",
      "Epoch 90 | Batch 34 | Loss: 0.08074951171875\n",
      "Epoch 90 | Batch 35 | Loss: 1.0032398700714111\n",
      "Epoch 90 | Batch 36 | Loss: 0.3514140546321869\n",
      "Epoch 90 | Batch 37 | Loss: 0.6171426773071289\n",
      "Epoch 90 | Batch 38 | Loss: 0.6130857467651367\n",
      "Epoch 90 | Batch 39 | Loss: 0.28079953789711\n",
      "Epoch 90 | Batch 40 | Loss: 0.09963692724704742\n",
      "Epoch 90 | Batch 41 | Loss: 0.1333448886871338\n",
      "Epoch 90 | Batch 42 | Loss: 0.12173736840486526\n",
      "Epoch 90 | Batch 43 | Loss: 0.4071146249771118\n",
      "Epoch 90 | Batch 44 | Loss: 0.4153183102607727\n",
      "Epoch 90 | Batch 45 | Loss: 0.507835865020752\n",
      "Epoch 90 | Batch 46 | Loss: 0.30481982231140137\n",
      "Epoch 90 | Batch 47 | Loss: 0.15768134593963623\n",
      "Epoch 90 | Batch 48 | Loss: 0.49833136796951294\n",
      "Epoch 90 | Batch 49 | Loss: 0.31495237350463867\n",
      "Epoch 90 | Batch 50 | Loss: 0.3939916491508484\n",
      "Epoch 90 | Batch 51 | Loss: 0.5474278330802917\n",
      "Epoch 90 | Batch 52 | Loss: 0.46322202682495117\n",
      "Epoch 90 | Batch 53 | Loss: 0.9388515949249268\n",
      "Epoch 90 | Batch 54 | Loss: 0.3936891555786133\n",
      "Epoch 90 | Batch 55 | Loss: 0.3656088411808014\n",
      "Epoch 90 | Batch 56 | Loss: 0.1248464584350586\n",
      "Epoch 90 | Batch 57 | Loss: 0.494204580783844\n",
      "Epoch 90 | Batch 58 | Loss: 0.20518900454044342\n",
      "Epoch 90 | Batch 59 | Loss: 0.16735266149044037\n",
      "Epoch 90 | Batch 60 | Loss: 0.12673577666282654\n",
      "Epoch 90 | Batch 61 | Loss: 0.37141674757003784\n",
      "Epoch 90 | Batch 62 | Loss: 0.389171302318573\n",
      "Epoch 90 | Batch 63 | Loss: 0.4472607970237732\n",
      "Epoch 90 | Batch 64 | Loss: 0.33548492193222046\n",
      "Epoch 90 | Batch 65 | Loss: 0.45044028759002686\n",
      "Epoch 90 | Batch 66 | Loss: 0.4437289237976074\n",
      "Epoch 90 | Batch 67 | Loss: 0.4142703413963318\n",
      "Epoch 90 | Batch 68 | Loss: 0.30406349897384644\n",
      "Epoch 90 | Batch 69 | Loss: 0.22151970863342285\n",
      "Epoch 90 | Batch 70 | Loss: 0.6004905700683594\n",
      "Epoch 90 | Batch 71 | Loss: 0.17434564232826233\n",
      "Epoch 90 | Batch 72 | Loss: 0.14752298593521118\n",
      "Epoch 90 | Batch 73 | Loss: 0.15926283597946167\n",
      "Epoch 90 | Batch 74 | Loss: 0.345855712890625\n",
      "Epoch 90 | Batch 75 | Loss: 0.08778531849384308\n",
      "Epoch 90 | Batch 76 | Loss: 0.6476417779922485\n",
      "Epoch 90 | Batch 77 | Loss: 0.10015513002872467\n",
      "Epoch 90 | Batch 78 | Loss: 0.1215893030166626\n",
      "Epoch 90 | Batch 79 | Loss: 0.11092507839202881\n",
      "Epoch 90 | Batch 80 | Loss: 0.7593839764595032\n",
      "Epoch 90 | Batch 81 | Loss: 0.726670503616333\n",
      "Epoch 90 | Batch 82 | Loss: 0.3501134514808655\n",
      "Epoch 90 | Batch 83 | Loss: 0.5258133411407471\n",
      "Epoch 90 | Batch 84 | Loss: 0.5702180862426758\n",
      "Epoch 90 | Batch 85 | Loss: 0.4198395013809204\n",
      "Epoch 90 | Batch 86 | Loss: 0.39732906222343445\n",
      "Epoch 90 | Batch 87 | Loss: 0.15425416827201843\n",
      "Epoch 90 | Batch 88 | Loss: 0.42634817957878113\n",
      "Epoch 90 | Batch 89 | Loss: 0.26512524485588074\n",
      "Epoch 90 | Batch 90 | Loss: 0.14172466099262238\n",
      "Epoch 91 | Batch 1 | Loss: 0.313215047121048\n",
      "Epoch 91 | Batch 2 | Loss: 0.4056265950202942\n",
      "Epoch 91 | Batch 3 | Loss: 0.3741432726383209\n",
      "Epoch 91 | Batch 4 | Loss: 0.33297890424728394\n",
      "Epoch 91 | Batch 5 | Loss: 0.48383164405822754\n",
      "Epoch 91 | Batch 6 | Loss: 0.13731111586093903\n",
      "Epoch 91 | Batch 7 | Loss: 0.15490204095840454\n",
      "Epoch 91 | Batch 8 | Loss: 0.3900143802165985\n",
      "Epoch 91 | Batch 9 | Loss: 0.48003441095352173\n",
      "Epoch 91 | Batch 10 | Loss: 0.5726818442344666\n",
      "Epoch 91 | Batch 11 | Loss: 0.35194841027259827\n",
      "Epoch 91 | Batch 12 | Loss: 0.31917470693588257\n",
      "Epoch 91 | Batch 13 | Loss: 0.28016290068626404\n",
      "Epoch 91 | Batch 14 | Loss: 0.5580741167068481\n",
      "Epoch 91 | Batch 15 | Loss: 0.15819098055362701\n",
      "Epoch 91 | Batch 16 | Loss: 0.34358102083206177\n",
      "Epoch 91 | Batch 17 | Loss: 0.42322444915771484\n",
      "Epoch 91 | Batch 18 | Loss: 0.578487753868103\n",
      "Epoch 91 | Batch 19 | Loss: 0.8029466271400452\n",
      "Epoch 91 | Batch 20 | Loss: 0.5090940594673157\n",
      "Epoch 91 | Batch 21 | Loss: 0.2742688059806824\n",
      "Epoch 91 | Batch 22 | Loss: 0.5993412733078003\n",
      "Epoch 91 | Batch 23 | Loss: 0.3900878131389618\n",
      "Epoch 91 | Batch 24 | Loss: 0.4188227653503418\n",
      "Epoch 91 | Batch 25 | Loss: 0.2502819001674652\n",
      "Epoch 91 | Batch 26 | Loss: 0.16465221345424652\n",
      "Epoch 91 | Batch 27 | Loss: 0.2905786633491516\n",
      "Epoch 91 | Batch 28 | Loss: 0.38961321115493774\n",
      "Epoch 91 | Batch 29 | Loss: 0.36334922909736633\n",
      "Epoch 91 | Batch 30 | Loss: 0.4875975549221039\n",
      "Epoch 91 | Batch 31 | Loss: 0.4112929701805115\n",
      "Epoch 91 | Batch 32 | Loss: 0.34486398100852966\n",
      "Epoch 91 | Batch 33 | Loss: 0.6439417600631714\n",
      "Epoch 91 | Batch 34 | Loss: 0.27281129360198975\n",
      "Epoch 91 | Batch 35 | Loss: 0.19994069635868073\n",
      "Epoch 91 | Batch 36 | Loss: 0.3804197609424591\n",
      "Epoch 91 | Batch 37 | Loss: 0.4863571524620056\n",
      "Epoch 91 | Batch 38 | Loss: 0.3090217411518097\n",
      "Epoch 91 | Batch 39 | Loss: 0.1908176988363266\n",
      "Epoch 91 | Batch 40 | Loss: 0.4481152296066284\n",
      "Epoch 91 | Batch 41 | Loss: 0.25384119153022766\n",
      "Epoch 91 | Batch 42 | Loss: 0.47001442313194275\n",
      "Epoch 91 | Batch 43 | Loss: 0.3693782091140747\n",
      "Epoch 91 | Batch 44 | Loss: 0.5817114114761353\n",
      "Epoch 91 | Batch 45 | Loss: 0.3903181552886963\n",
      "Epoch 91 | Batch 46 | Loss: 0.2165568768978119\n",
      "Epoch 91 | Batch 47 | Loss: 0.1856122612953186\n",
      "Epoch 91 | Batch 48 | Loss: 0.2615751326084137\n",
      "Epoch 91 | Batch 49 | Loss: 0.4393990933895111\n",
      "Epoch 91 | Batch 50 | Loss: 0.4736202359199524\n",
      "Epoch 91 | Batch 51 | Loss: 0.1128508523106575\n",
      "Epoch 91 | Batch 52 | Loss: 0.6782798171043396\n",
      "Epoch 91 | Batch 53 | Loss: 0.41651761531829834\n",
      "Epoch 91 | Batch 54 | Loss: 0.6212546825408936\n",
      "Epoch 91 | Batch 55 | Loss: 0.31053611636161804\n",
      "Epoch 91 | Batch 56 | Loss: 0.3853064179420471\n",
      "Epoch 91 | Batch 57 | Loss: 0.20050789415836334\n",
      "Epoch 91 | Batch 58 | Loss: 0.2068144977092743\n",
      "Epoch 91 | Batch 59 | Loss: 0.6407470703125\n",
      "Epoch 91 | Batch 60 | Loss: 0.5696708559989929\n",
      "Epoch 91 | Batch 61 | Loss: 0.6257251501083374\n",
      "Epoch 91 | Batch 62 | Loss: 0.1675429493188858\n",
      "Epoch 91 | Batch 63 | Loss: 0.14588764309883118\n",
      "Epoch 91 | Batch 64 | Loss: 0.36742299795150757\n",
      "Epoch 91 | Batch 65 | Loss: 0.37261444330215454\n",
      "Epoch 91 | Batch 66 | Loss: 0.24131187796592712\n",
      "Epoch 91 | Batch 67 | Loss: 0.12716823816299438\n",
      "Epoch 91 | Batch 68 | Loss: 0.11525750160217285\n",
      "Epoch 91 | Batch 69 | Loss: 0.151783287525177\n",
      "Epoch 91 | Batch 70 | Loss: 0.12071506679058075\n",
      "Epoch 91 | Batch 71 | Loss: 0.26926156878471375\n",
      "Epoch 91 | Batch 72 | Loss: 0.18369057774543762\n",
      "Epoch 91 | Batch 73 | Loss: 0.2414204180240631\n",
      "Epoch 91 | Batch 74 | Loss: 0.2129252851009369\n",
      "Epoch 91 | Batch 75 | Loss: 0.43737852573394775\n",
      "Epoch 91 | Batch 76 | Loss: 0.4927477240562439\n",
      "Epoch 91 | Batch 77 | Loss: 0.15320758521556854\n",
      "Epoch 91 | Batch 78 | Loss: 0.43204671144485474\n",
      "Epoch 91 | Batch 79 | Loss: 0.11628114432096481\n",
      "Epoch 91 | Batch 80 | Loss: 0.22208085656166077\n",
      "Epoch 91 | Batch 81 | Loss: 0.6877076625823975\n",
      "Epoch 91 | Batch 82 | Loss: 0.1400512158870697\n",
      "Epoch 91 | Batch 83 | Loss: 0.5546814203262329\n",
      "Epoch 91 | Batch 84 | Loss: 0.5886138677597046\n",
      "Epoch 91 | Batch 85 | Loss: 0.19108378887176514\n",
      "Epoch 91 | Batch 86 | Loss: 0.6658008098602295\n",
      "Epoch 91 | Batch 87 | Loss: 0.6995200514793396\n",
      "Epoch 91 | Batch 88 | Loss: 0.415727823972702\n",
      "Epoch 91 | Batch 89 | Loss: 0.24102485179901123\n",
      "Epoch 91 | Batch 90 | Loss: 0.10217227786779404\n",
      "Epoch 92 | Batch 1 | Loss: 0.1162966713309288\n",
      "Epoch 92 | Batch 2 | Loss: 0.8507869243621826\n",
      "Epoch 92 | Batch 3 | Loss: 0.3254951238632202\n",
      "Epoch 92 | Batch 4 | Loss: 0.199405699968338\n",
      "Epoch 92 | Batch 5 | Loss: 0.15454307198524475\n",
      "Epoch 92 | Batch 6 | Loss: 0.3605847954750061\n",
      "Epoch 92 | Batch 7 | Loss: 0.7513933777809143\n",
      "Epoch 92 | Batch 8 | Loss: 0.25661906599998474\n",
      "Epoch 92 | Batch 9 | Loss: 0.592624306678772\n",
      "Epoch 92 | Batch 10 | Loss: 0.7645273208618164\n",
      "Epoch 92 | Batch 11 | Loss: 0.7314713001251221\n",
      "Epoch 92 | Batch 12 | Loss: 0.38553252816200256\n",
      "Epoch 92 | Batch 13 | Loss: 0.4120990037918091\n",
      "Epoch 92 | Batch 14 | Loss: 0.415353000164032\n",
      "Epoch 92 | Batch 15 | Loss: 0.23562867939472198\n",
      "Epoch 92 | Batch 16 | Loss: 0.13704174757003784\n",
      "Epoch 92 | Batch 17 | Loss: 0.5046612620353699\n",
      "Epoch 92 | Batch 18 | Loss: 0.2673373520374298\n",
      "Epoch 92 | Batch 19 | Loss: 0.5702893733978271\n",
      "Epoch 92 | Batch 20 | Loss: 0.2745168209075928\n",
      "Epoch 92 | Batch 21 | Loss: 0.3947914242744446\n",
      "Epoch 92 | Batch 22 | Loss: 0.26428210735321045\n",
      "Epoch 92 | Batch 23 | Loss: 0.848824679851532\n",
      "Epoch 92 | Batch 24 | Loss: 0.310779333114624\n",
      "Epoch 92 | Batch 25 | Loss: 0.12059605121612549\n",
      "Epoch 92 | Batch 26 | Loss: 0.1515086591243744\n",
      "Epoch 92 | Batch 27 | Loss: 0.38233035802841187\n",
      "Epoch 92 | Batch 28 | Loss: 0.23544028401374817\n",
      "Epoch 92 | Batch 29 | Loss: 0.14355182647705078\n",
      "Epoch 92 | Batch 30 | Loss: 0.13551554083824158\n",
      "Epoch 92 | Batch 31 | Loss: 0.4560477137565613\n",
      "Epoch 92 | Batch 32 | Loss: 0.1346392035484314\n",
      "Epoch 92 | Batch 33 | Loss: 0.7174593210220337\n",
      "Epoch 92 | Batch 34 | Loss: 0.3718448579311371\n",
      "Epoch 92 | Batch 35 | Loss: 0.4173595905303955\n",
      "Epoch 92 | Batch 36 | Loss: 0.32728642225265503\n",
      "Epoch 92 | Batch 37 | Loss: 0.44745421409606934\n",
      "Epoch 92 | Batch 38 | Loss: 0.1766289472579956\n",
      "Epoch 92 | Batch 39 | Loss: 0.2815631628036499\n",
      "Epoch 92 | Batch 40 | Loss: 0.0491565465927124\n",
      "Epoch 92 | Batch 41 | Loss: 0.3362792730331421\n",
      "Epoch 92 | Batch 42 | Loss: 0.4283066391944885\n",
      "Epoch 92 | Batch 43 | Loss: 0.37817978858947754\n",
      "Epoch 92 | Batch 44 | Loss: 0.42896169424057007\n",
      "Epoch 92 | Batch 45 | Loss: 0.584749698638916\n",
      "Epoch 92 | Batch 46 | Loss: 0.35422253608703613\n",
      "Epoch 92 | Batch 47 | Loss: 0.6305303573608398\n",
      "Epoch 92 | Batch 48 | Loss: 0.09582096338272095\n",
      "Epoch 92 | Batch 49 | Loss: 0.39715316891670227\n",
      "Epoch 92 | Batch 50 | Loss: 0.42945289611816406\n",
      "Epoch 92 | Batch 51 | Loss: 0.5901076197624207\n",
      "Epoch 92 | Batch 52 | Loss: 0.7274820804595947\n",
      "Epoch 92 | Batch 53 | Loss: 0.3631109595298767\n",
      "Epoch 92 | Batch 54 | Loss: 0.3445506691932678\n",
      "Epoch 92 | Batch 55 | Loss: 0.19650983810424805\n",
      "Epoch 92 | Batch 56 | Loss: 0.4650249481201172\n",
      "Epoch 92 | Batch 57 | Loss: 0.08103451132774353\n",
      "Epoch 92 | Batch 58 | Loss: 0.32115474343299866\n",
      "Epoch 92 | Batch 59 | Loss: 0.4030264914035797\n",
      "Epoch 92 | Batch 60 | Loss: 0.32758867740631104\n",
      "Epoch 92 | Batch 61 | Loss: 0.36476248502731323\n",
      "Epoch 92 | Batch 62 | Loss: 0.24108032882213593\n",
      "Epoch 92 | Batch 63 | Loss: 0.43802085518836975\n",
      "Epoch 92 | Batch 64 | Loss: 0.21011680364608765\n",
      "Epoch 92 | Batch 65 | Loss: 0.4622158706188202\n",
      "Epoch 92 | Batch 66 | Loss: 0.5744000673294067\n",
      "Epoch 92 | Batch 67 | Loss: 0.345102995634079\n",
      "Epoch 92 | Batch 68 | Loss: 0.5946930646896362\n",
      "Epoch 92 | Batch 69 | Loss: 0.4106530547142029\n",
      "Epoch 92 | Batch 70 | Loss: 0.37564146518707275\n",
      "Epoch 92 | Batch 71 | Loss: 0.07202722132205963\n",
      "Epoch 92 | Batch 72 | Loss: 0.24875448644161224\n",
      "Epoch 92 | Batch 73 | Loss: 0.27886414527893066\n",
      "Epoch 92 | Batch 74 | Loss: 0.3262771964073181\n",
      "Epoch 92 | Batch 75 | Loss: 0.6047194004058838\n",
      "Epoch 92 | Batch 76 | Loss: 0.26356565952301025\n",
      "Epoch 92 | Batch 77 | Loss: 0.3520786166191101\n",
      "Epoch 92 | Batch 78 | Loss: 0.39016327261924744\n",
      "Epoch 92 | Batch 79 | Loss: 0.17297902703285217\n",
      "Epoch 92 | Batch 80 | Loss: 0.2970547378063202\n",
      "Epoch 92 | Batch 81 | Loss: 0.2725202441215515\n",
      "Epoch 92 | Batch 82 | Loss: 0.2116285115480423\n",
      "Epoch 92 | Batch 83 | Loss: 0.5481219291687012\n",
      "Epoch 92 | Batch 84 | Loss: 0.3164769411087036\n",
      "Epoch 92 | Batch 85 | Loss: 0.14229218661785126\n",
      "Epoch 92 | Batch 86 | Loss: 0.18430908024311066\n",
      "Epoch 92 | Batch 87 | Loss: 0.15740391612052917\n",
      "Epoch 92 | Batch 88 | Loss: 0.37290775775909424\n",
      "Epoch 92 | Batch 89 | Loss: 0.47112515568733215\n",
      "Epoch 92 | Batch 90 | Loss: 0.15944641828536987\n",
      "Epoch 93 | Batch 1 | Loss: 0.16252818703651428\n",
      "Epoch 93 | Batch 2 | Loss: 0.43195223808288574\n",
      "Epoch 93 | Batch 3 | Loss: 0.562626838684082\n",
      "Epoch 93 | Batch 4 | Loss: 0.7219806909561157\n",
      "Epoch 93 | Batch 5 | Loss: 0.4305594861507416\n",
      "Epoch 93 | Batch 6 | Loss: 0.164714515209198\n",
      "Epoch 93 | Batch 7 | Loss: 0.8735388517379761\n",
      "Epoch 93 | Batch 8 | Loss: 0.37785959243774414\n",
      "Epoch 93 | Batch 9 | Loss: 0.5190036296844482\n",
      "Epoch 93 | Batch 10 | Loss: 0.4247574210166931\n",
      "Epoch 93 | Batch 11 | Loss: 0.7730648517608643\n",
      "Epoch 93 | Batch 12 | Loss: 0.580240786075592\n",
      "Epoch 93 | Batch 13 | Loss: 0.23253947496414185\n",
      "Epoch 93 | Batch 14 | Loss: 0.9356571435928345\n",
      "Epoch 93 | Batch 15 | Loss: 0.45254653692245483\n",
      "Epoch 93 | Batch 16 | Loss: 0.09487118571996689\n",
      "Epoch 93 | Batch 17 | Loss: 0.4552510380744934\n",
      "Epoch 93 | Batch 18 | Loss: 0.6501975655555725\n",
      "Epoch 93 | Batch 19 | Loss: 0.6090418696403503\n",
      "Epoch 93 | Batch 20 | Loss: 0.27248793840408325\n",
      "Epoch 93 | Batch 21 | Loss: 0.5427484512329102\n",
      "Epoch 93 | Batch 22 | Loss: 0.43006113171577454\n",
      "Epoch 93 | Batch 23 | Loss: 0.6612818241119385\n",
      "Epoch 93 | Batch 24 | Loss: 0.3061276078224182\n",
      "Epoch 93 | Batch 25 | Loss: 0.39398419857025146\n",
      "Epoch 93 | Batch 26 | Loss: 0.42335081100463867\n",
      "Epoch 93 | Batch 27 | Loss: 0.18884514272212982\n",
      "Epoch 93 | Batch 28 | Loss: 0.3327482342720032\n",
      "Epoch 93 | Batch 29 | Loss: 0.28556832671165466\n",
      "Epoch 93 | Batch 30 | Loss: 0.1645728051662445\n",
      "Epoch 93 | Batch 31 | Loss: 0.12057866156101227\n",
      "Epoch 93 | Batch 32 | Loss: 0.07959994673728943\n",
      "Epoch 93 | Batch 33 | Loss: 0.2728714346885681\n",
      "Epoch 93 | Batch 34 | Loss: 0.16066384315490723\n",
      "Epoch 93 | Batch 35 | Loss: 0.47284549474716187\n",
      "Epoch 93 | Batch 36 | Loss: 0.39994069933891296\n",
      "Epoch 93 | Batch 37 | Loss: 0.5441624522209167\n",
      "Epoch 93 | Batch 38 | Loss: 0.14774192869663239\n",
      "Epoch 93 | Batch 39 | Loss: 0.1329554319381714\n",
      "Epoch 93 | Batch 40 | Loss: 0.6585950255393982\n",
      "Epoch 93 | Batch 41 | Loss: 0.23705162107944489\n",
      "Epoch 93 | Batch 42 | Loss: 0.5306676626205444\n",
      "Epoch 93 | Batch 43 | Loss: 0.25654345750808716\n",
      "Epoch 93 | Batch 44 | Loss: 0.43372052907943726\n",
      "Epoch 93 | Batch 45 | Loss: 0.3482381999492645\n",
      "Epoch 93 | Batch 46 | Loss: 0.2629943788051605\n",
      "Epoch 93 | Batch 47 | Loss: 0.18982842564582825\n",
      "Epoch 93 | Batch 48 | Loss: 0.11454472690820694\n",
      "Epoch 93 | Batch 49 | Loss: 0.380498468875885\n",
      "Epoch 93 | Batch 50 | Loss: 0.6558371782302856\n",
      "Epoch 93 | Batch 51 | Loss: 0.51865553855896\n",
      "Epoch 93 | Batch 52 | Loss: 0.6028894782066345\n",
      "Epoch 93 | Batch 53 | Loss: 0.6544144749641418\n",
      "Epoch 93 | Batch 54 | Loss: 0.5282264947891235\n",
      "Epoch 93 | Batch 55 | Loss: 0.23442822694778442\n",
      "Epoch 93 | Batch 56 | Loss: 0.8340689539909363\n",
      "Epoch 93 | Batch 57 | Loss: 0.2683209776878357\n",
      "Epoch 93 | Batch 58 | Loss: 0.36402249336242676\n",
      "Epoch 93 | Batch 59 | Loss: 0.5598896741867065\n",
      "Epoch 93 | Batch 60 | Loss: 0.20999863743782043\n",
      "Epoch 93 | Batch 61 | Loss: 0.39908748865127563\n",
      "Epoch 93 | Batch 62 | Loss: 0.10637219995260239\n",
      "Epoch 93 | Batch 63 | Loss: 0.5004816651344299\n",
      "Epoch 93 | Batch 64 | Loss: 0.31624799966812134\n",
      "Epoch 93 | Batch 65 | Loss: 0.1385469138622284\n",
      "Epoch 93 | Batch 66 | Loss: 0.16114024817943573\n",
      "Epoch 93 | Batch 67 | Loss: 0.20354010164737701\n",
      "Epoch 93 | Batch 68 | Loss: 0.12882648408412933\n",
      "Epoch 93 | Batch 69 | Loss: 0.2542714476585388\n",
      "Epoch 93 | Batch 70 | Loss: 0.18221530318260193\n",
      "Epoch 93 | Batch 71 | Loss: 0.31362876296043396\n",
      "Epoch 93 | Batch 72 | Loss: 0.122689388692379\n",
      "Epoch 93 | Batch 73 | Loss: 0.2169089913368225\n",
      "Epoch 93 | Batch 74 | Loss: 0.3953601121902466\n",
      "Epoch 93 | Batch 75 | Loss: 0.25836968421936035\n",
      "Epoch 93 | Batch 76 | Loss: 0.7498849034309387\n",
      "Epoch 93 | Batch 77 | Loss: 0.12446237355470657\n",
      "Epoch 93 | Batch 78 | Loss: 0.16159707307815552\n",
      "Epoch 93 | Batch 79 | Loss: 0.0813826471567154\n",
      "Epoch 93 | Batch 80 | Loss: 0.2839289903640747\n",
      "Epoch 93 | Batch 81 | Loss: 0.37716320157051086\n",
      "Epoch 93 | Batch 82 | Loss: 0.27814167737960815\n",
      "Epoch 93 | Batch 83 | Loss: 0.1610289365053177\n",
      "Epoch 93 | Batch 84 | Loss: 0.09545636922121048\n",
      "Epoch 93 | Batch 85 | Loss: 0.09293006360530853\n",
      "Epoch 93 | Batch 86 | Loss: 0.9896957874298096\n",
      "Epoch 93 | Batch 87 | Loss: 0.1260330080986023\n",
      "Epoch 93 | Batch 88 | Loss: 0.06723009049892426\n",
      "Epoch 93 | Batch 89 | Loss: 0.36617201566696167\n",
      "Epoch 93 | Batch 90 | Loss: 2.253350257873535\n",
      "Epoch 94 | Batch 1 | Loss: 0.394084095954895\n",
      "Epoch 94 | Batch 2 | Loss: 0.24603410065174103\n",
      "Epoch 94 | Batch 3 | Loss: 0.3942890167236328\n",
      "Epoch 94 | Batch 4 | Loss: 0.26239466667175293\n",
      "Epoch 94 | Batch 5 | Loss: 0.14742538332939148\n",
      "Epoch 94 | Batch 6 | Loss: 0.21114489436149597\n",
      "Epoch 94 | Batch 7 | Loss: 0.5180767178535461\n",
      "Epoch 94 | Batch 8 | Loss: 0.4958328902721405\n",
      "Epoch 94 | Batch 9 | Loss: 0.6646891832351685\n",
      "Epoch 94 | Batch 10 | Loss: 0.22803111374378204\n",
      "Epoch 94 | Batch 11 | Loss: 0.5773276686668396\n",
      "Epoch 94 | Batch 12 | Loss: 0.15773379802703857\n",
      "Epoch 94 | Batch 13 | Loss: 0.12839630246162415\n",
      "Epoch 94 | Batch 14 | Loss: 0.5967223644256592\n",
      "Epoch 94 | Batch 15 | Loss: 0.35849806666374207\n",
      "Epoch 94 | Batch 16 | Loss: 0.47739988565444946\n",
      "Epoch 94 | Batch 17 | Loss: 0.20171013474464417\n",
      "Epoch 94 | Batch 18 | Loss: 0.14459091424942017\n",
      "Epoch 94 | Batch 19 | Loss: 0.31772881746292114\n",
      "Epoch 94 | Batch 20 | Loss: 0.22055327892303467\n",
      "Epoch 94 | Batch 21 | Loss: 0.44542595744132996\n",
      "Epoch 94 | Batch 22 | Loss: 0.3371213376522064\n",
      "Epoch 94 | Batch 23 | Loss: 0.19398191571235657\n",
      "Epoch 94 | Batch 24 | Loss: 0.6665849089622498\n",
      "Epoch 94 | Batch 25 | Loss: 0.23406526446342468\n",
      "Epoch 94 | Batch 26 | Loss: 0.3354946970939636\n",
      "Epoch 94 | Batch 27 | Loss: 0.4057185649871826\n",
      "Epoch 94 | Batch 28 | Loss: 0.47457656264305115\n",
      "Epoch 94 | Batch 29 | Loss: 0.4185103476047516\n",
      "Epoch 94 | Batch 30 | Loss: 0.16316227614879608\n",
      "Epoch 94 | Batch 31 | Loss: 0.4651210904121399\n",
      "Epoch 94 | Batch 32 | Loss: 0.42093920707702637\n",
      "Epoch 94 | Batch 33 | Loss: 0.17631712555885315\n",
      "Epoch 94 | Batch 34 | Loss: 0.5313622355461121\n",
      "Epoch 94 | Batch 35 | Loss: 0.18369647860527039\n",
      "Epoch 94 | Batch 36 | Loss: 0.34271666407585144\n",
      "Epoch 94 | Batch 37 | Loss: 0.2985520362854004\n",
      "Epoch 94 | Batch 38 | Loss: 0.4714764654636383\n",
      "Epoch 94 | Batch 39 | Loss: 0.23194874823093414\n",
      "Epoch 94 | Batch 40 | Loss: 0.5378639101982117\n",
      "Epoch 94 | Batch 41 | Loss: 0.49942755699157715\n",
      "Epoch 94 | Batch 42 | Loss: 0.5042814016342163\n",
      "Epoch 94 | Batch 43 | Loss: 0.1667262315750122\n",
      "Epoch 94 | Batch 44 | Loss: 0.4132360816001892\n",
      "Epoch 94 | Batch 45 | Loss: 0.5617793202400208\n",
      "Epoch 94 | Batch 46 | Loss: 0.2809827923774719\n",
      "Epoch 94 | Batch 47 | Loss: 0.2554408609867096\n",
      "Epoch 94 | Batch 48 | Loss: 0.38081520795822144\n",
      "Epoch 94 | Batch 49 | Loss: 0.2043377161026001\n",
      "Epoch 94 | Batch 50 | Loss: 0.43364354968070984\n",
      "Epoch 94 | Batch 51 | Loss: 0.45647603273391724\n",
      "Epoch 94 | Batch 52 | Loss: 0.15474066138267517\n",
      "Epoch 94 | Batch 53 | Loss: 0.3619251847267151\n",
      "Epoch 94 | Batch 54 | Loss: 0.35007143020629883\n",
      "Epoch 94 | Batch 55 | Loss: 0.2683817148208618\n",
      "Epoch 94 | Batch 56 | Loss: 0.27224043011665344\n",
      "Epoch 94 | Batch 57 | Loss: 0.09553521126508713\n",
      "Epoch 94 | Batch 58 | Loss: 0.6505188345909119\n",
      "Epoch 94 | Batch 59 | Loss: 0.38796940445899963\n",
      "Epoch 94 | Batch 60 | Loss: 0.3225524425506592\n",
      "Epoch 94 | Batch 61 | Loss: 0.3070541024208069\n",
      "Epoch 94 | Batch 62 | Loss: 0.48924893140792847\n",
      "Epoch 94 | Batch 63 | Loss: 0.20706802606582642\n",
      "Epoch 94 | Batch 64 | Loss: 0.17514395713806152\n",
      "Epoch 94 | Batch 65 | Loss: 0.26614999771118164\n",
      "Epoch 94 | Batch 66 | Loss: 0.3703146278858185\n",
      "Epoch 94 | Batch 67 | Loss: 0.3021089434623718\n",
      "Epoch 94 | Batch 68 | Loss: 0.5226273536682129\n",
      "Epoch 94 | Batch 69 | Loss: 0.5411911010742188\n",
      "Epoch 94 | Batch 70 | Loss: 0.32675206661224365\n",
      "Epoch 94 | Batch 71 | Loss: 0.4350804090499878\n",
      "Epoch 94 | Batch 72 | Loss: 0.5967394709587097\n",
      "Epoch 94 | Batch 73 | Loss: 0.24510851502418518\n",
      "Epoch 94 | Batch 74 | Loss: 0.5297148823738098\n",
      "Epoch 94 | Batch 75 | Loss: 0.41286200284957886\n",
      "Epoch 94 | Batch 76 | Loss: 0.3745177388191223\n",
      "Epoch 94 | Batch 77 | Loss: 0.21779745817184448\n",
      "Epoch 94 | Batch 78 | Loss: 0.43753620982170105\n",
      "Epoch 94 | Batch 79 | Loss: 0.5181774497032166\n",
      "Epoch 94 | Batch 80 | Loss: 0.47191473841667175\n",
      "Epoch 94 | Batch 81 | Loss: 0.23341120779514313\n",
      "Epoch 94 | Batch 82 | Loss: 0.24604971706867218\n",
      "Epoch 94 | Batch 83 | Loss: 0.5955528020858765\n",
      "Epoch 94 | Batch 84 | Loss: 0.3911861777305603\n",
      "Epoch 94 | Batch 85 | Loss: 0.13287077844142914\n",
      "Epoch 94 | Batch 86 | Loss: 0.5106675028800964\n",
      "Epoch 94 | Batch 87 | Loss: 0.40070104598999023\n",
      "Epoch 94 | Batch 88 | Loss: 0.45406046509742737\n",
      "Epoch 94 | Batch 89 | Loss: 0.16364170610904694\n",
      "Epoch 94 | Batch 90 | Loss: 0.17419837415218353\n",
      "Epoch 95 | Batch 1 | Loss: 0.5706518888473511\n",
      "Epoch 95 | Batch 2 | Loss: 0.7234931588172913\n",
      "Epoch 95 | Batch 3 | Loss: 0.3697575628757477\n",
      "Epoch 95 | Batch 4 | Loss: 0.5892217755317688\n",
      "Epoch 95 | Batch 5 | Loss: 0.5501477122306824\n",
      "Epoch 95 | Batch 6 | Loss: 0.3294293284416199\n",
      "Epoch 95 | Batch 7 | Loss: 0.5502722263336182\n",
      "Epoch 95 | Batch 8 | Loss: 0.16419409215450287\n",
      "Epoch 95 | Batch 9 | Loss: 0.2863287329673767\n",
      "Epoch 95 | Batch 10 | Loss: 0.4330524504184723\n",
      "Epoch 95 | Batch 11 | Loss: 0.31606724858283997\n",
      "Epoch 95 | Batch 12 | Loss: 0.6342207789421082\n",
      "Epoch 95 | Batch 13 | Loss: 0.21535803377628326\n",
      "Epoch 95 | Batch 14 | Loss: 0.4177735149860382\n",
      "Epoch 95 | Batch 15 | Loss: 0.13557168841362\n",
      "Epoch 95 | Batch 16 | Loss: 0.34932273626327515\n",
      "Epoch 95 | Batch 17 | Loss: 0.5409290194511414\n",
      "Epoch 95 | Batch 18 | Loss: 0.29123979806900024\n",
      "Epoch 95 | Batch 19 | Loss: 0.6555275917053223\n",
      "Epoch 95 | Batch 20 | Loss: 0.37027886509895325\n",
      "Epoch 95 | Batch 21 | Loss: 0.5255088806152344\n",
      "Epoch 95 | Batch 22 | Loss: 0.4402492046356201\n",
      "Epoch 95 | Batch 23 | Loss: 0.31405067443847656\n",
      "Epoch 95 | Batch 24 | Loss: 0.24716615676879883\n",
      "Epoch 95 | Batch 25 | Loss: 0.4965166449546814\n",
      "Epoch 95 | Batch 26 | Loss: 0.1729324907064438\n",
      "Epoch 95 | Batch 27 | Loss: 0.9144091010093689\n",
      "Epoch 95 | Batch 28 | Loss: 0.36406177282333374\n",
      "Epoch 95 | Batch 29 | Loss: 0.4021531939506531\n",
      "Epoch 95 | Batch 30 | Loss: 0.384537935256958\n",
      "Epoch 95 | Batch 31 | Loss: 0.2470860481262207\n",
      "Epoch 95 | Batch 32 | Loss: 0.3941895067691803\n",
      "Epoch 95 | Batch 33 | Loss: 0.2333044409751892\n",
      "Epoch 95 | Batch 34 | Loss: 0.23079460859298706\n",
      "Epoch 95 | Batch 35 | Loss: 0.09971527010202408\n",
      "Epoch 95 | Batch 36 | Loss: 0.6449066400527954\n",
      "Epoch 95 | Batch 37 | Loss: 0.6878743767738342\n",
      "Epoch 95 | Batch 38 | Loss: 0.2727983295917511\n",
      "Epoch 95 | Batch 39 | Loss: 0.26127171516418457\n",
      "Epoch 95 | Batch 40 | Loss: 0.7928370833396912\n",
      "Epoch 95 | Batch 41 | Loss: 0.20119324326515198\n",
      "Epoch 95 | Batch 42 | Loss: 0.2945612072944641\n",
      "Epoch 95 | Batch 43 | Loss: 0.5694735050201416\n",
      "Epoch 95 | Batch 44 | Loss: 0.12356492131948471\n",
      "Epoch 95 | Batch 45 | Loss: 0.23121483623981476\n",
      "Epoch 95 | Batch 46 | Loss: 0.3948744535446167\n",
      "Epoch 95 | Batch 47 | Loss: 0.3987208604812622\n",
      "Epoch 95 | Batch 48 | Loss: 0.4170832931995392\n",
      "Epoch 95 | Batch 49 | Loss: 0.12879928946495056\n",
      "Epoch 95 | Batch 50 | Loss: 0.4390038251876831\n",
      "Epoch 95 | Batch 51 | Loss: 0.18926405906677246\n",
      "Epoch 95 | Batch 52 | Loss: 0.3970111310482025\n",
      "Epoch 95 | Batch 53 | Loss: 0.19784358143806458\n",
      "Epoch 95 | Batch 54 | Loss: 0.23127108812332153\n",
      "Epoch 95 | Batch 55 | Loss: 0.3474046289920807\n",
      "Epoch 95 | Batch 56 | Loss: 0.20279522240161896\n",
      "Epoch 95 | Batch 57 | Loss: 0.4883849322795868\n",
      "Epoch 95 | Batch 58 | Loss: 0.11531747132539749\n",
      "Epoch 95 | Batch 59 | Loss: 0.42439383268356323\n",
      "Epoch 95 | Batch 60 | Loss: 0.42024466395378113\n",
      "Epoch 95 | Batch 61 | Loss: 0.9272019863128662\n",
      "Epoch 95 | Batch 62 | Loss: 0.20197758078575134\n",
      "Epoch 95 | Batch 63 | Loss: 0.07748882472515106\n",
      "Epoch 95 | Batch 64 | Loss: 0.3970262408256531\n",
      "Epoch 95 | Batch 65 | Loss: 0.16787809133529663\n",
      "Epoch 95 | Batch 66 | Loss: 0.2998226284980774\n",
      "Epoch 95 | Batch 67 | Loss: 0.2919885516166687\n",
      "Epoch 95 | Batch 68 | Loss: 0.2772200405597687\n",
      "Epoch 95 | Batch 69 | Loss: 0.080562524497509\n",
      "Epoch 95 | Batch 70 | Loss: 0.5538936257362366\n",
      "Epoch 95 | Batch 71 | Loss: 0.4005383253097534\n",
      "Epoch 95 | Batch 72 | Loss: 0.4897167384624481\n",
      "Epoch 95 | Batch 73 | Loss: 0.6816116571426392\n",
      "Epoch 95 | Batch 74 | Loss: 0.3951745927333832\n",
      "Epoch 95 | Batch 75 | Loss: 0.4199659526348114\n",
      "Epoch 95 | Batch 76 | Loss: 0.6002025604248047\n",
      "Epoch 95 | Batch 77 | Loss: 0.2256113886833191\n",
      "Epoch 95 | Batch 78 | Loss: 0.12721574306488037\n",
      "Epoch 95 | Batch 79 | Loss: 0.1296904981136322\n",
      "Epoch 95 | Batch 80 | Loss: 0.14990875124931335\n",
      "Epoch 95 | Batch 81 | Loss: 0.16470424830913544\n",
      "Epoch 95 | Batch 82 | Loss: 0.23817896842956543\n",
      "Epoch 95 | Batch 83 | Loss: 0.13313910365104675\n",
      "Epoch 95 | Batch 84 | Loss: 0.0992288738489151\n",
      "Epoch 95 | Batch 85 | Loss: 0.3003762364387512\n",
      "Epoch 95 | Batch 86 | Loss: 0.5289508104324341\n",
      "Epoch 95 | Batch 87 | Loss: 1.0011613368988037\n",
      "Epoch 95 | Batch 88 | Loss: 0.23107506334781647\n",
      "Epoch 95 | Batch 89 | Loss: 0.6137974262237549\n",
      "Epoch 95 | Batch 90 | Loss: 0.13100038468837738\n",
      "Epoch 96 | Batch 1 | Loss: 0.26143351197242737\n",
      "Epoch 96 | Batch 2 | Loss: 0.6166098713874817\n",
      "Epoch 96 | Batch 3 | Loss: 0.459502249956131\n",
      "Epoch 96 | Batch 4 | Loss: 0.20108768343925476\n",
      "Epoch 96 | Batch 5 | Loss: 0.4068467617034912\n",
      "Epoch 96 | Batch 6 | Loss: 0.36381250619888306\n",
      "Epoch 96 | Batch 7 | Loss: 0.4323596954345703\n",
      "Epoch 96 | Batch 8 | Loss: 0.2901471257209778\n",
      "Epoch 96 | Batch 9 | Loss: 0.24549180269241333\n",
      "Epoch 96 | Batch 10 | Loss: 0.5695880651473999\n",
      "Epoch 96 | Batch 11 | Loss: 0.0722620040178299\n",
      "Epoch 96 | Batch 12 | Loss: 0.5794302821159363\n",
      "Epoch 96 | Batch 13 | Loss: 0.09718737006187439\n",
      "Epoch 96 | Batch 14 | Loss: 0.4385277032852173\n",
      "Epoch 96 | Batch 15 | Loss: 0.34898990392684937\n",
      "Epoch 96 | Batch 16 | Loss: 0.22771109640598297\n",
      "Epoch 96 | Batch 17 | Loss: 0.3318817615509033\n",
      "Epoch 96 | Batch 18 | Loss: 0.4588283896446228\n",
      "Epoch 96 | Batch 19 | Loss: 0.1212828978896141\n",
      "Epoch 96 | Batch 20 | Loss: 0.38251829147338867\n",
      "Epoch 96 | Batch 21 | Loss: 0.49116426706314087\n",
      "Epoch 96 | Batch 22 | Loss: 0.457471638917923\n",
      "Epoch 96 | Batch 23 | Loss: 0.11694221198558807\n",
      "Epoch 96 | Batch 24 | Loss: 0.13148391246795654\n",
      "Epoch 96 | Batch 25 | Loss: 0.5496983528137207\n",
      "Epoch 96 | Batch 26 | Loss: 0.15176047384738922\n",
      "Epoch 96 | Batch 27 | Loss: 0.2142276018857956\n",
      "Epoch 96 | Batch 28 | Loss: 0.4014267325401306\n",
      "Epoch 96 | Batch 29 | Loss: 0.28468450903892517\n",
      "Epoch 96 | Batch 30 | Loss: 0.20863771438598633\n",
      "Epoch 96 | Batch 31 | Loss: 0.3595430254936218\n",
      "Epoch 96 | Batch 32 | Loss: 0.6412230730056763\n",
      "Epoch 96 | Batch 33 | Loss: 0.6192595362663269\n",
      "Epoch 96 | Batch 34 | Loss: 0.20990370213985443\n",
      "Epoch 96 | Batch 35 | Loss: 0.04506191238760948\n",
      "Epoch 96 | Batch 36 | Loss: 0.5962656736373901\n",
      "Epoch 96 | Batch 37 | Loss: 0.48879897594451904\n",
      "Epoch 96 | Batch 38 | Loss: 0.474894642829895\n",
      "Epoch 96 | Batch 39 | Loss: 0.37239769101142883\n",
      "Epoch 96 | Batch 40 | Loss: 0.34932854771614075\n",
      "Epoch 96 | Batch 41 | Loss: 0.33406639099121094\n",
      "Epoch 96 | Batch 42 | Loss: 0.31531476974487305\n",
      "Epoch 96 | Batch 43 | Loss: 0.3819149136543274\n",
      "Epoch 96 | Batch 44 | Loss: 0.12206072360277176\n",
      "Epoch 96 | Batch 45 | Loss: 0.5653401613235474\n",
      "Epoch 96 | Batch 46 | Loss: 0.4692017138004303\n",
      "Epoch 96 | Batch 47 | Loss: 0.15331849455833435\n",
      "Epoch 96 | Batch 48 | Loss: 0.4443131387233734\n",
      "Epoch 96 | Batch 49 | Loss: 0.3975856304168701\n",
      "Epoch 96 | Batch 50 | Loss: 0.3135456442832947\n",
      "Epoch 96 | Batch 51 | Loss: 0.4507761299610138\n",
      "Epoch 96 | Batch 52 | Loss: 0.10371500253677368\n",
      "Epoch 96 | Batch 53 | Loss: 0.3266342878341675\n",
      "Epoch 96 | Batch 54 | Loss: 0.46569713950157166\n",
      "Epoch 96 | Batch 55 | Loss: 0.2266198694705963\n",
      "Epoch 96 | Batch 56 | Loss: 0.26575082540512085\n",
      "Epoch 96 | Batch 57 | Loss: 0.411363810300827\n",
      "Epoch 96 | Batch 58 | Loss: 0.08192502707242966\n",
      "Epoch 96 | Batch 59 | Loss: 0.4586946666240692\n",
      "Epoch 96 | Batch 60 | Loss: 0.2573356628417969\n",
      "Epoch 96 | Batch 61 | Loss: 0.20794005692005157\n",
      "Epoch 96 | Batch 62 | Loss: 0.3543494641780853\n",
      "Epoch 96 | Batch 63 | Loss: 0.1560269594192505\n",
      "Epoch 96 | Batch 64 | Loss: 0.1034913957118988\n",
      "Epoch 96 | Batch 65 | Loss: 0.14133602380752563\n",
      "Epoch 96 | Batch 66 | Loss: 0.4628410339355469\n",
      "Epoch 96 | Batch 67 | Loss: 0.09382098913192749\n",
      "Epoch 96 | Batch 68 | Loss: 0.2513188421726227\n",
      "Epoch 96 | Batch 69 | Loss: 0.5148583054542542\n",
      "Epoch 96 | Batch 70 | Loss: 0.5652348399162292\n",
      "Epoch 96 | Batch 71 | Loss: 0.6285769939422607\n",
      "Epoch 96 | Batch 72 | Loss: 0.783568263053894\n",
      "Epoch 96 | Batch 73 | Loss: 0.21761561930179596\n",
      "Epoch 96 | Batch 74 | Loss: 0.6914026141166687\n",
      "Epoch 96 | Batch 75 | Loss: 0.15968233346939087\n",
      "Epoch 96 | Batch 76 | Loss: 0.3455439805984497\n",
      "Epoch 96 | Batch 77 | Loss: 0.590874195098877\n",
      "Epoch 96 | Batch 78 | Loss: 0.7682929039001465\n",
      "Epoch 96 | Batch 79 | Loss: 0.284478098154068\n",
      "Epoch 96 | Batch 80 | Loss: 0.37677568197250366\n",
      "Epoch 96 | Batch 81 | Loss: 0.0877215564250946\n",
      "Epoch 96 | Batch 82 | Loss: 0.1432119607925415\n",
      "Epoch 96 | Batch 83 | Loss: 0.9955496788024902\n",
      "Epoch 96 | Batch 84 | Loss: 0.2178095132112503\n",
      "Epoch 96 | Batch 85 | Loss: 0.4933016002178192\n",
      "Epoch 96 | Batch 86 | Loss: 0.20858553051948547\n",
      "Epoch 96 | Batch 87 | Loss: 0.1884728968143463\n",
      "Epoch 96 | Batch 88 | Loss: 1.1052181720733643\n",
      "Epoch 96 | Batch 89 | Loss: 0.5441251397132874\n",
      "Epoch 96 | Batch 90 | Loss: 0.05371672660112381\n",
      "Epoch 97 | Batch 1 | Loss: 0.31384822726249695\n",
      "Epoch 97 | Batch 2 | Loss: 0.3825260400772095\n",
      "Epoch 97 | Batch 3 | Loss: 0.42382127046585083\n",
      "Epoch 97 | Batch 4 | Loss: 1.012962818145752\n",
      "Epoch 97 | Batch 5 | Loss: 0.2037094533443451\n",
      "Epoch 97 | Batch 6 | Loss: 0.7624372243881226\n",
      "Epoch 97 | Batch 7 | Loss: 0.7277697324752808\n",
      "Epoch 97 | Batch 8 | Loss: 0.5718058347702026\n",
      "Epoch 97 | Batch 9 | Loss: 0.5670424699783325\n",
      "Epoch 97 | Batch 10 | Loss: 0.6128568649291992\n",
      "Epoch 97 | Batch 11 | Loss: 0.34596630930900574\n",
      "Epoch 97 | Batch 12 | Loss: 0.20269718766212463\n",
      "Epoch 97 | Batch 13 | Loss: 0.4906145930290222\n",
      "Epoch 97 | Batch 14 | Loss: 0.4930424392223358\n",
      "Epoch 97 | Batch 15 | Loss: 0.2000199258327484\n",
      "Epoch 97 | Batch 16 | Loss: 0.3697737455368042\n",
      "Epoch 97 | Batch 17 | Loss: 0.23047199845314026\n",
      "Epoch 97 | Batch 18 | Loss: 0.42659029364585876\n",
      "Epoch 97 | Batch 19 | Loss: 0.4951174259185791\n",
      "Epoch 97 | Batch 20 | Loss: 0.26784712076187134\n",
      "Epoch 97 | Batch 21 | Loss: 0.10978186130523682\n",
      "Epoch 97 | Batch 22 | Loss: 0.10899356752634048\n",
      "Epoch 97 | Batch 23 | Loss: 0.8686466217041016\n",
      "Epoch 97 | Batch 24 | Loss: 0.2824101448059082\n",
      "Epoch 97 | Batch 25 | Loss: 0.7792619466781616\n",
      "Epoch 97 | Batch 26 | Loss: 0.5079710483551025\n",
      "Epoch 97 | Batch 27 | Loss: 0.2379189133644104\n",
      "Epoch 97 | Batch 28 | Loss: 0.25189435482025146\n",
      "Epoch 97 | Batch 29 | Loss: 0.36600059270858765\n",
      "Epoch 97 | Batch 30 | Loss: 0.17537418007850647\n",
      "Epoch 97 | Batch 31 | Loss: 0.20924803614616394\n",
      "Epoch 97 | Batch 32 | Loss: 0.2332690805196762\n",
      "Epoch 97 | Batch 33 | Loss: 0.12649084627628326\n",
      "Epoch 97 | Batch 34 | Loss: 0.22759607434272766\n",
      "Epoch 97 | Batch 35 | Loss: 0.2402363270521164\n",
      "Epoch 97 | Batch 36 | Loss: 0.6340680122375488\n",
      "Epoch 97 | Batch 37 | Loss: 0.17100568115711212\n",
      "Epoch 97 | Batch 38 | Loss: 0.6481561064720154\n",
      "Epoch 97 | Batch 39 | Loss: 0.424946129322052\n",
      "Epoch 97 | Batch 40 | Loss: 0.3256910443305969\n",
      "Epoch 97 | Batch 41 | Loss: 0.9252480268478394\n",
      "Epoch 97 | Batch 42 | Loss: 0.703941285610199\n",
      "Epoch 97 | Batch 43 | Loss: 0.3344036936759949\n",
      "Epoch 97 | Batch 44 | Loss: 0.20861418545246124\n",
      "Epoch 97 | Batch 45 | Loss: 0.34441861510276794\n",
      "Epoch 97 | Batch 46 | Loss: 0.17982642352581024\n",
      "Epoch 97 | Batch 47 | Loss: 0.3037535548210144\n",
      "Epoch 97 | Batch 48 | Loss: 0.34408944845199585\n",
      "Epoch 97 | Batch 49 | Loss: 0.185166135430336\n",
      "Epoch 97 | Batch 50 | Loss: 0.3994528651237488\n",
      "Epoch 97 | Batch 51 | Loss: 0.5606162548065186\n",
      "Epoch 97 | Batch 52 | Loss: 0.20576736330986023\n",
      "Epoch 97 | Batch 53 | Loss: 0.3244330883026123\n",
      "Epoch 97 | Batch 54 | Loss: 0.44393038749694824\n",
      "Epoch 97 | Batch 55 | Loss: 0.19146081805229187\n",
      "Epoch 97 | Batch 56 | Loss: 0.30776768922805786\n",
      "Epoch 97 | Batch 57 | Loss: 0.2661365866661072\n",
      "Epoch 97 | Batch 58 | Loss: 0.43590113520622253\n",
      "Epoch 97 | Batch 59 | Loss: 0.3960121273994446\n",
      "Epoch 97 | Batch 60 | Loss: 0.2089589536190033\n",
      "Epoch 97 | Batch 61 | Loss: 0.4501945674419403\n",
      "Epoch 97 | Batch 62 | Loss: 0.3223893642425537\n",
      "Epoch 97 | Batch 63 | Loss: 0.26073670387268066\n",
      "Epoch 97 | Batch 64 | Loss: 0.7105342149734497\n",
      "Epoch 97 | Batch 65 | Loss: 0.6181027889251709\n",
      "Epoch 97 | Batch 66 | Loss: 0.4420281946659088\n",
      "Epoch 97 | Batch 67 | Loss: 0.29106879234313965\n",
      "Epoch 97 | Batch 68 | Loss: 0.687774658203125\n",
      "Epoch 97 | Batch 69 | Loss: 0.3264436721801758\n",
      "Epoch 97 | Batch 70 | Loss: 0.31798991560935974\n",
      "Epoch 97 | Batch 71 | Loss: 0.14899230003356934\n",
      "Epoch 97 | Batch 72 | Loss: 0.21460801362991333\n",
      "Epoch 97 | Batch 73 | Loss: 0.1592014878988266\n",
      "Epoch 97 | Batch 74 | Loss: 0.4174572825431824\n",
      "Epoch 97 | Batch 75 | Loss: 0.4219909906387329\n",
      "Epoch 97 | Batch 76 | Loss: 0.6498318314552307\n",
      "Epoch 97 | Batch 77 | Loss: 0.17827481031417847\n",
      "Epoch 97 | Batch 78 | Loss: 0.21822617948055267\n",
      "Epoch 97 | Batch 79 | Loss: 0.10925085842609406\n",
      "Epoch 97 | Batch 80 | Loss: 0.23786504566669464\n",
      "Epoch 97 | Batch 81 | Loss: 0.34755098819732666\n",
      "Epoch 97 | Batch 82 | Loss: 0.2561366558074951\n",
      "Epoch 97 | Batch 83 | Loss: 0.19583290815353394\n",
      "Epoch 97 | Batch 84 | Loss: 0.2561420798301697\n",
      "Epoch 97 | Batch 85 | Loss: 0.3382095694541931\n",
      "Epoch 97 | Batch 86 | Loss: 0.09450510144233704\n",
      "Epoch 97 | Batch 87 | Loss: 0.2095891833305359\n",
      "Epoch 97 | Batch 88 | Loss: 0.29375359416007996\n",
      "Epoch 97 | Batch 89 | Loss: 0.1881127804517746\n",
      "Epoch 97 | Batch 90 | Loss: 0.46963930130004883\n",
      "Epoch 98 | Batch 1 | Loss: 0.23215459287166595\n",
      "Epoch 98 | Batch 2 | Loss: 0.18386372923851013\n",
      "Epoch 98 | Batch 3 | Loss: 1.086124062538147\n",
      "Epoch 98 | Batch 4 | Loss: 0.11343976855278015\n",
      "Epoch 98 | Batch 5 | Loss: 0.07021176815032959\n",
      "Epoch 98 | Batch 6 | Loss: 0.22584155201911926\n",
      "Epoch 98 | Batch 7 | Loss: 0.10041175782680511\n",
      "Epoch 98 | Batch 8 | Loss: 0.18821273744106293\n",
      "Epoch 98 | Batch 9 | Loss: 0.38290005922317505\n",
      "Epoch 98 | Batch 10 | Loss: 0.3130813241004944\n",
      "Epoch 98 | Batch 11 | Loss: 0.6098442077636719\n",
      "Epoch 98 | Batch 12 | Loss: 0.3320837616920471\n",
      "Epoch 98 | Batch 13 | Loss: 0.686325192451477\n",
      "Epoch 98 | Batch 14 | Loss: 0.19249027967453003\n",
      "Epoch 98 | Batch 15 | Loss: 0.5631617307662964\n",
      "Epoch 98 | Batch 16 | Loss: 0.2401932030916214\n",
      "Epoch 98 | Batch 17 | Loss: 0.4645995497703552\n",
      "Epoch 98 | Batch 18 | Loss: 0.28803664445877075\n",
      "Epoch 98 | Batch 19 | Loss: 0.18358924984931946\n",
      "Epoch 98 | Batch 20 | Loss: 0.37052392959594727\n",
      "Epoch 98 | Batch 21 | Loss: 0.40641582012176514\n",
      "Epoch 98 | Batch 22 | Loss: 0.3073679208755493\n",
      "Epoch 98 | Batch 23 | Loss: 0.5870627164840698\n",
      "Epoch 98 | Batch 24 | Loss: 0.09768915921449661\n",
      "Epoch 98 | Batch 25 | Loss: 0.14447176456451416\n",
      "Epoch 98 | Batch 26 | Loss: 0.6519463658332825\n",
      "Epoch 98 | Batch 27 | Loss: 0.4651985168457031\n",
      "Epoch 98 | Batch 28 | Loss: 0.10131488740444183\n",
      "Epoch 98 | Batch 29 | Loss: 0.07583105564117432\n",
      "Epoch 98 | Batch 30 | Loss: 0.11890599876642227\n",
      "Epoch 98 | Batch 31 | Loss: 0.5562902092933655\n",
      "Epoch 98 | Batch 32 | Loss: 0.24485750496387482\n",
      "Epoch 98 | Batch 33 | Loss: 0.27788975834846497\n",
      "Epoch 98 | Batch 34 | Loss: 0.586631178855896\n",
      "Epoch 98 | Batch 35 | Loss: 0.3308235704898834\n",
      "Epoch 98 | Batch 36 | Loss: 0.295229971408844\n",
      "Epoch 98 | Batch 37 | Loss: 0.18751481175422668\n",
      "Epoch 98 | Batch 38 | Loss: 0.21057265996932983\n",
      "Epoch 98 | Batch 39 | Loss: 0.360434353351593\n",
      "Epoch 98 | Batch 40 | Loss: 0.19948536157608032\n",
      "Epoch 98 | Batch 41 | Loss: 0.5279128551483154\n",
      "Epoch 98 | Batch 42 | Loss: 0.16841650009155273\n",
      "Epoch 98 | Batch 43 | Loss: 0.4579848051071167\n",
      "Epoch 98 | Batch 44 | Loss: 1.0447145700454712\n",
      "Epoch 98 | Batch 45 | Loss: 0.6753187775611877\n",
      "Epoch 98 | Batch 46 | Loss: 0.2615302503108978\n",
      "Epoch 98 | Batch 47 | Loss: 0.18157394230365753\n",
      "Epoch 98 | Batch 48 | Loss: 0.1265752613544464\n",
      "Epoch 98 | Batch 49 | Loss: 0.2714725732803345\n",
      "Epoch 98 | Batch 50 | Loss: 0.5194673538208008\n",
      "Epoch 98 | Batch 51 | Loss: 0.13206776976585388\n",
      "Epoch 98 | Batch 52 | Loss: 0.873193621635437\n",
      "Epoch 98 | Batch 53 | Loss: 0.5381278991699219\n",
      "Epoch 98 | Batch 54 | Loss: 0.4925421178340912\n",
      "Epoch 98 | Batch 55 | Loss: 0.5738493204116821\n",
      "Epoch 98 | Batch 56 | Loss: 0.361430823802948\n",
      "Epoch 98 | Batch 57 | Loss: 0.7017613053321838\n",
      "Epoch 98 | Batch 58 | Loss: 0.2948991656303406\n",
      "Epoch 98 | Batch 59 | Loss: 0.7423626184463501\n",
      "Epoch 98 | Batch 60 | Loss: 0.4782588481903076\n",
      "Epoch 98 | Batch 61 | Loss: 0.12940295040607452\n",
      "Epoch 98 | Batch 62 | Loss: 0.4344756007194519\n",
      "Epoch 98 | Batch 63 | Loss: 0.22930115461349487\n",
      "Epoch 98 | Batch 64 | Loss: 0.17681337893009186\n",
      "Epoch 98 | Batch 65 | Loss: 0.585289716720581\n",
      "Epoch 98 | Batch 66 | Loss: 0.4830588698387146\n",
      "Epoch 98 | Batch 67 | Loss: 0.28158825635910034\n",
      "Epoch 98 | Batch 68 | Loss: 0.09861703217029572\n",
      "Epoch 98 | Batch 69 | Loss: 0.1765197366476059\n",
      "Epoch 98 | Batch 70 | Loss: 0.2461622953414917\n",
      "Epoch 98 | Batch 71 | Loss: 0.591158390045166\n",
      "Epoch 98 | Batch 72 | Loss: 0.331231951713562\n",
      "Epoch 98 | Batch 73 | Loss: 0.3475014269351959\n",
      "Epoch 98 | Batch 74 | Loss: 0.25134921073913574\n",
      "Epoch 98 | Batch 75 | Loss: 0.624416172504425\n",
      "Epoch 98 | Batch 76 | Loss: 0.21075507998466492\n",
      "Epoch 98 | Batch 77 | Loss: 0.5145573019981384\n",
      "Epoch 98 | Batch 78 | Loss: 0.5084603428840637\n",
      "Epoch 98 | Batch 79 | Loss: 0.20415787398815155\n",
      "Epoch 98 | Batch 80 | Loss: 0.4152596592903137\n",
      "Epoch 98 | Batch 81 | Loss: 0.09957447648048401\n",
      "Epoch 98 | Batch 82 | Loss: 0.28129953145980835\n",
      "Epoch 98 | Batch 83 | Loss: 0.3630576729774475\n",
      "Epoch 98 | Batch 84 | Loss: 0.10223095118999481\n",
      "Epoch 98 | Batch 85 | Loss: 0.4012487530708313\n",
      "Epoch 98 | Batch 86 | Loss: 0.9069157242774963\n",
      "Epoch 98 | Batch 87 | Loss: 0.7037354707717896\n",
      "Epoch 98 | Batch 88 | Loss: 0.08801503479480743\n",
      "Epoch 98 | Batch 89 | Loss: 0.07563099265098572\n",
      "Epoch 98 | Batch 90 | Loss: 0.09069885313510895\n",
      "Epoch 99 | Batch 1 | Loss: 0.20110929012298584\n",
      "Epoch 99 | Batch 2 | Loss: 0.11247299611568451\n",
      "Epoch 99 | Batch 3 | Loss: 0.16919006407260895\n",
      "Epoch 99 | Batch 4 | Loss: 0.2912609577178955\n",
      "Epoch 99 | Batch 5 | Loss: 0.5203846096992493\n",
      "Epoch 99 | Batch 6 | Loss: 0.48592665791511536\n",
      "Epoch 99 | Batch 7 | Loss: 0.6888173818588257\n",
      "Epoch 99 | Batch 8 | Loss: 0.15607397258281708\n",
      "Epoch 99 | Batch 9 | Loss: 0.500867486000061\n",
      "Epoch 99 | Batch 10 | Loss: 0.12444347143173218\n",
      "Epoch 99 | Batch 11 | Loss: 0.7230695486068726\n",
      "Epoch 99 | Batch 12 | Loss: 0.5093017816543579\n",
      "Epoch 99 | Batch 13 | Loss: 0.650675892829895\n",
      "Epoch 99 | Batch 14 | Loss: 0.7009665966033936\n",
      "Epoch 99 | Batch 15 | Loss: 0.23007455468177795\n",
      "Epoch 99 | Batch 16 | Loss: 0.3793068528175354\n",
      "Epoch 99 | Batch 17 | Loss: 0.6781982183456421\n",
      "Epoch 99 | Batch 18 | Loss: 0.39236438274383545\n",
      "Epoch 99 | Batch 19 | Loss: 0.26569080352783203\n",
      "Epoch 99 | Batch 20 | Loss: 0.3392742872238159\n",
      "Epoch 99 | Batch 21 | Loss: 0.24871225655078888\n",
      "Epoch 99 | Batch 22 | Loss: 0.5094462633132935\n",
      "Epoch 99 | Batch 23 | Loss: 0.4014849066734314\n",
      "Epoch 99 | Batch 24 | Loss: 0.5632186532020569\n",
      "Epoch 99 | Batch 25 | Loss: 0.31137141585350037\n",
      "Epoch 99 | Batch 26 | Loss: 0.12272621691226959\n",
      "Epoch 99 | Batch 27 | Loss: 0.17151063680648804\n",
      "Epoch 99 | Batch 28 | Loss: 0.35441645979881287\n",
      "Epoch 99 | Batch 29 | Loss: 0.39215025305747986\n",
      "Epoch 99 | Batch 30 | Loss: 0.41798341274261475\n",
      "Epoch 99 | Batch 31 | Loss: 0.41729646921157837\n",
      "Epoch 99 | Batch 32 | Loss: 0.7493258714675903\n",
      "Epoch 99 | Batch 33 | Loss: 0.16667315363883972\n",
      "Epoch 99 | Batch 34 | Loss: 0.17886358499526978\n",
      "Epoch 99 | Batch 35 | Loss: 0.16134075820446014\n",
      "Epoch 99 | Batch 36 | Loss: 0.08380673080682755\n",
      "Epoch 99 | Batch 37 | Loss: 0.6256489753723145\n",
      "Epoch 99 | Batch 38 | Loss: 0.22732725739479065\n",
      "Epoch 99 | Batch 39 | Loss: 0.1571444272994995\n",
      "Epoch 99 | Batch 40 | Loss: 0.6094081401824951\n",
      "Epoch 99 | Batch 41 | Loss: 0.6393949389457703\n",
      "Epoch 99 | Batch 42 | Loss: 0.5229595303535461\n",
      "Epoch 99 | Batch 43 | Loss: 0.2103511542081833\n",
      "Epoch 99 | Batch 44 | Loss: 0.2797270119190216\n",
      "Epoch 99 | Batch 45 | Loss: 0.10303156077861786\n",
      "Epoch 99 | Batch 46 | Loss: 0.06414933502674103\n",
      "Epoch 99 | Batch 47 | Loss: 0.2009851336479187\n",
      "Epoch 99 | Batch 48 | Loss: 0.1223718598484993\n",
      "Epoch 99 | Batch 49 | Loss: 0.25441715121269226\n",
      "Epoch 99 | Batch 50 | Loss: 0.23903295397758484\n",
      "Epoch 99 | Batch 51 | Loss: 0.4126673936843872\n",
      "Epoch 99 | Batch 52 | Loss: 0.6938437223434448\n",
      "Epoch 99 | Batch 53 | Loss: 0.30000680685043335\n",
      "Epoch 99 | Batch 54 | Loss: 0.40680456161499023\n",
      "Epoch 99 | Batch 55 | Loss: 0.22381646931171417\n",
      "Epoch 99 | Batch 56 | Loss: 0.5448747277259827\n",
      "Epoch 99 | Batch 57 | Loss: 0.21370090544223785\n",
      "Epoch 99 | Batch 58 | Loss: 0.47180065512657166\n",
      "Epoch 99 | Batch 59 | Loss: 0.42949527502059937\n",
      "Epoch 99 | Batch 60 | Loss: 0.0925670713186264\n",
      "Epoch 99 | Batch 61 | Loss: 0.34301841259002686\n",
      "Epoch 99 | Batch 62 | Loss: 0.21873053908348083\n",
      "Epoch 99 | Batch 63 | Loss: 0.3976074457168579\n",
      "Epoch 99 | Batch 64 | Loss: 0.24976462125778198\n",
      "Epoch 99 | Batch 65 | Loss: 0.7417093515396118\n",
      "Epoch 99 | Batch 66 | Loss: 1.1785616874694824\n",
      "Epoch 99 | Batch 67 | Loss: 0.31234949827194214\n",
      "Epoch 99 | Batch 68 | Loss: 0.1334555745124817\n",
      "Epoch 99 | Batch 69 | Loss: 0.36743590235710144\n",
      "Epoch 99 | Batch 70 | Loss: 0.3335190713405609\n",
      "Epoch 99 | Batch 71 | Loss: 0.32134392857551575\n",
      "Epoch 99 | Batch 72 | Loss: 0.17462193965911865\n",
      "Epoch 99 | Batch 73 | Loss: 0.3245488405227661\n",
      "Epoch 99 | Batch 74 | Loss: 0.2629520893096924\n",
      "Epoch 99 | Batch 75 | Loss: 0.4042017161846161\n",
      "Epoch 99 | Batch 76 | Loss: 0.4071524143218994\n",
      "Epoch 99 | Batch 77 | Loss: 0.622122049331665\n",
      "Epoch 99 | Batch 78 | Loss: 0.5283445119857788\n",
      "Epoch 99 | Batch 79 | Loss: 0.15956471860408783\n",
      "Epoch 99 | Batch 80 | Loss: 0.3989289402961731\n",
      "Epoch 99 | Batch 81 | Loss: 0.24240954220294952\n",
      "Epoch 99 | Batch 82 | Loss: 0.3678545355796814\n",
      "Epoch 99 | Batch 83 | Loss: 0.25901466608047485\n",
      "Epoch 99 | Batch 84 | Loss: 0.3035259544849396\n",
      "Epoch 99 | Batch 85 | Loss: 0.3957556486129761\n",
      "Epoch 99 | Batch 86 | Loss: 0.4289461076259613\n",
      "Epoch 99 | Batch 87 | Loss: 0.25924986600875854\n",
      "Epoch 99 | Batch 88 | Loss: 0.35461458563804626\n",
      "Epoch 99 | Batch 89 | Loss: 0.13192737102508545\n",
      "Epoch 99 | Batch 90 | Loss: 0.025245770812034607\n",
      "Epoch 100 | Batch 1 | Loss: 0.21710607409477234\n",
      "Epoch 100 | Batch 2 | Loss: 0.1957002878189087\n",
      "Epoch 100 | Batch 3 | Loss: 0.24088862538337708\n",
      "Epoch 100 | Batch 4 | Loss: 0.4576185345649719\n",
      "Epoch 100 | Batch 5 | Loss: 0.5106735825538635\n",
      "Epoch 100 | Batch 6 | Loss: 0.1021755188703537\n",
      "Epoch 100 | Batch 7 | Loss: 0.06805221736431122\n",
      "Epoch 100 | Batch 8 | Loss: 0.8699719905853271\n",
      "Epoch 100 | Batch 9 | Loss: 0.28541237115859985\n",
      "Epoch 100 | Batch 10 | Loss: 0.25666430592536926\n",
      "Epoch 100 | Batch 11 | Loss: 0.2026381939649582\n",
      "Epoch 100 | Batch 12 | Loss: 0.33148857951164246\n",
      "Epoch 100 | Batch 13 | Loss: 0.12244566529989243\n",
      "Epoch 100 | Batch 14 | Loss: 0.20682762563228607\n",
      "Epoch 100 | Batch 15 | Loss: 0.135761097073555\n",
      "Epoch 100 | Batch 16 | Loss: 0.6211696267127991\n",
      "Epoch 100 | Batch 17 | Loss: 0.46381309628486633\n",
      "Epoch 100 | Batch 18 | Loss: 0.36300161480903625\n",
      "Epoch 100 | Batch 19 | Loss: 0.2218509018421173\n",
      "Epoch 100 | Batch 20 | Loss: 0.2557036578655243\n",
      "Epoch 100 | Batch 21 | Loss: 0.3767646253108978\n",
      "Epoch 100 | Batch 22 | Loss: 0.6107770204544067\n",
      "Epoch 100 | Batch 23 | Loss: 0.18302872776985168\n",
      "Epoch 100 | Batch 24 | Loss: 0.6664146780967712\n",
      "Epoch 100 | Batch 25 | Loss: 0.0914563313126564\n",
      "Epoch 100 | Batch 26 | Loss: 0.2221970409154892\n",
      "Epoch 100 | Batch 27 | Loss: 0.1820049285888672\n",
      "Epoch 100 | Batch 28 | Loss: 0.6752467155456543\n",
      "Epoch 100 | Batch 29 | Loss: 0.20986677706241608\n",
      "Epoch 100 | Batch 30 | Loss: 0.4191480576992035\n",
      "Epoch 100 | Batch 31 | Loss: 0.14460143446922302\n",
      "Epoch 100 | Batch 32 | Loss: 0.5304150581359863\n",
      "Epoch 100 | Batch 33 | Loss: 1.1046545505523682\n",
      "Epoch 100 | Batch 34 | Loss: 0.152535080909729\n",
      "Epoch 100 | Batch 35 | Loss: 0.15877854824066162\n",
      "Epoch 100 | Batch 36 | Loss: 0.5301539301872253\n",
      "Epoch 100 | Batch 37 | Loss: 0.3584176003932953\n",
      "Epoch 100 | Batch 38 | Loss: 0.3596991300582886\n",
      "Epoch 100 | Batch 39 | Loss: 0.5404331088066101\n",
      "Epoch 100 | Batch 40 | Loss: 0.527395486831665\n",
      "Epoch 100 | Batch 41 | Loss: 0.4096897840499878\n",
      "Epoch 100 | Batch 42 | Loss: 0.6310737729072571\n",
      "Epoch 100 | Batch 43 | Loss: 0.34808629751205444\n",
      "Epoch 100 | Batch 44 | Loss: 0.7415680289268494\n",
      "Epoch 100 | Batch 45 | Loss: 0.5975450277328491\n",
      "Epoch 100 | Batch 46 | Loss: 0.7509761452674866\n",
      "Epoch 100 | Batch 47 | Loss: 0.4298824071884155\n",
      "Epoch 100 | Batch 48 | Loss: 0.362559050321579\n",
      "Epoch 100 | Batch 49 | Loss: 0.28378450870513916\n",
      "Epoch 100 | Batch 50 | Loss: 0.23846258223056793\n",
      "Epoch 100 | Batch 51 | Loss: 0.5129266381263733\n",
      "Epoch 100 | Batch 52 | Loss: 0.3402915596961975\n",
      "Epoch 100 | Batch 53 | Loss: 0.226204514503479\n",
      "Epoch 100 | Batch 54 | Loss: 0.6886872053146362\n",
      "Epoch 100 | Batch 55 | Loss: 0.3973396122455597\n",
      "Epoch 100 | Batch 56 | Loss: 0.576233983039856\n",
      "Epoch 100 | Batch 57 | Loss: 0.5133916735649109\n",
      "Epoch 100 | Batch 58 | Loss: 0.2549961805343628\n",
      "Epoch 100 | Batch 59 | Loss: 0.6119880080223083\n",
      "Epoch 100 | Batch 60 | Loss: 0.36590591073036194\n",
      "Epoch 100 | Batch 61 | Loss: 0.45305749773979187\n",
      "Epoch 100 | Batch 62 | Loss: 0.2580994963645935\n",
      "Epoch 100 | Batch 63 | Loss: 0.2023230940103531\n",
      "Epoch 100 | Batch 64 | Loss: 0.2133113443851471\n",
      "Epoch 100 | Batch 65 | Loss: 0.31042158603668213\n",
      "Epoch 100 | Batch 66 | Loss: 0.4506096839904785\n",
      "Epoch 100 | Batch 67 | Loss: 0.09188390523195267\n",
      "Epoch 100 | Batch 68 | Loss: 0.3055553436279297\n",
      "Epoch 100 | Batch 69 | Loss: 0.17368577420711517\n",
      "Epoch 100 | Batch 70 | Loss: 0.2595102787017822\n",
      "Epoch 100 | Batch 71 | Loss: 0.4292246103286743\n",
      "Epoch 100 | Batch 72 | Loss: 0.15833239257335663\n",
      "Epoch 100 | Batch 73 | Loss: 0.19375255703926086\n",
      "Epoch 100 | Batch 74 | Loss: 0.317380428314209\n",
      "Epoch 100 | Batch 75 | Loss: 0.5469775199890137\n",
      "Epoch 100 | Batch 76 | Loss: 0.40845629572868347\n",
      "Epoch 100 | Batch 77 | Loss: 0.21704789996147156\n",
      "Epoch 100 | Batch 78 | Loss: 0.17841190099716187\n",
      "Epoch 100 | Batch 79 | Loss: 0.31964629888534546\n",
      "Epoch 100 | Batch 80 | Loss: 0.4657966196537018\n",
      "Epoch 100 | Batch 81 | Loss: 0.35480618476867676\n",
      "Epoch 100 | Batch 82 | Loss: 0.571471095085144\n",
      "Epoch 100 | Batch 83 | Loss: 0.06657141447067261\n",
      "Epoch 100 | Batch 84 | Loss: 0.2170119285583496\n",
      "Epoch 100 | Batch 85 | Loss: 0.1660601943731308\n",
      "Epoch 100 | Batch 86 | Loss: 0.4041173458099365\n",
      "Epoch 100 | Batch 87 | Loss: 0.5587705373764038\n",
      "Epoch 100 | Batch 88 | Loss: 0.8935363292694092\n",
      "Epoch 100 | Batch 89 | Loss: 0.10873770713806152\n",
      "Epoch 100 | Batch 90 | Loss: 0.17392690479755402\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    i = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = feedforward(x_batch.cuda())\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = loss_fn(y_pred, y_batch.unsqueeze(1).float().cuda())\n",
    "        print(f'Epoch {epoch + 1} | Batch {i + 1} | Loss: {loss.item()}')\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "\n",
    "# Testing the model\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
